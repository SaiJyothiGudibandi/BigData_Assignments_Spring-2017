"C:\Program Files\Java\jdk8\bin\java" -Didea.launcher.port=7535 "-Didea.launcher.bin.path=C:\Program Files (x86)\JetBrains\IntelliJ IDEA 2016.3.3\bin" -Dfile.encoding=UTF-8 -classpath "C:\Program Files\Java\jdk8\jre\lib\charsets.jar;C:\Program Files\Java\jdk8\jre\lib\deploy.jar;C:\Program Files\Java\jdk8\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk8\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk8\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk8\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk8\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk8\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk8\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk8\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk8\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk8\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk8\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk8\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk8\jre\lib\javaws.jar;C:\Program Files\Java\jdk8\jre\lib\jce.jar;C:\Program Files\Java\jdk8\jre\lib\jfr.jar;C:\Program Files\Java\jdk8\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk8\jre\lib\jsse.jar;C:\Program Files\Java\jdk8\jre\lib\management-agent.jar;C:\Program Files\Java\jdk8\jre\lib\plugin.jar;C:\Program Files\Java\jdk8\jre\lib\resources.jar;C:\Program Files\Java\jdk8\jre\lib\rt.jar;C:\Users\saijy\Desktop\BigData\Sample\image_classification_Windows\target\scala-2.11\classes;C:\Users\saijy\.ivy2\cache\aopalliance\aopalliance\jars\aopalliance-1.0.jar;C:\Users\saijy\.ivy2\cache\org.codehaus.jackson\jackson-xc\jars\jackson-xc-1.8.3.jar;C:\Users\saijy\.ivy2\cache\org.codehaus.jackson\jackson-jaxrs\jars\jackson-jaxrs-1.8.3.jar;C:\Users\saijy\.ivy2\cache\commons-net\commons-net\jars\commons-net-2.2.jar;C:\Users\saijy\.ivy2\cache\asm\asm\jars\asm-3.1.jar;C:\Users\saijy\.ivy2\cache\com.clearspring.analytics\stream\jars\stream-2.7.0.jar;C:\Users\saijy\.ivy2\cache\com.esotericsoftware.kryo\kryo\bundles\kryo-2.21.jar;C:\Users\saijy\.ivy2\cache\com.esotericsoftware.minlog\minlog\jars\minlog-1.2.jar;C:\Users\saijy\.ivy2\cache\com.esotericsoftware.reflectasm\reflectasm\jars\reflectasm-1.07-shaded.jar;C:\Users\saijy\.ivy2\cache\com.fasterxml.jackson.core\jackson-annotations\bundles\jackson-annotations-2.4.4.jar;C:\Users\saijy\.ivy2\cache\com.fasterxml.jackson.core\jackson-core\bundles\jackson-core-2.4.4.jar;C:\Users\saijy\.ivy2\cache\com.fasterxml.jackson.core\jackson-databind\bundles\jackson-databind-2.4.4.jar;C:\Users\saijy\.ivy2\cache\com.fasterxml.jackson.module\jackson-module-scala_2.11\bundles\jackson-module-scala_2.11-2.4.4.jar;C:\Users\saijy\.ivy2\cache\com.github.fommil.netlib\core\jars\core-1.1.2.jar;C:\Users\saijy\.ivy2\cache\com.github.rwl\jtransforms\jars\jtransforms-2.4.0.jar;C:\Users\saijy\.ivy2\cache\com.google.code.findbugs\jsr305\jars\jsr305-1.3.9.jar;C:\Users\saijy\.ivy2\cache\com.google.guava\guava\bundles\guava-14.0.1.jar;C:\Users\saijy\.ivy2\cache\com.google.inject\guice\jars\guice-3.0.jar;C:\Users\saijy\.ivy2\cache\com.google.protobuf\protobuf-java\bundles\protobuf-java-2.5.0.jar;C:\Users\saijy\.ivy2\cache\com.ning\compress-lzf\bundles\compress-lzf-1.0.3.jar;C:\Users\saijy\.ivy2\cache\com.sun.istack\istack-commons-runtime\jars\istack-commons-runtime-2.16.jar;C:\Users\saijy\.ivy2\cache\com.sun.jersey\jersey-core\bundles\jersey-core-1.9.jar;C:\Users\saijy\.ivy2\cache\com.sun.jersey\jersey-json\bundles\jersey-json-1.9.jar;C:\Users\saijy\.ivy2\cache\com.sun.jersey\jersey-server\bundles\jersey-server-1.9.jar;C:\Users\saijy\.ivy2\cache\com.sun.jersey.contribs\jersey-guice\jars\jersey-guice-1.9.jar;C:\Users\saijy\.ivy2\cache\com.sun.jersey.jersey-test-framework\jersey-test-framework-grizzly2\jars\jersey-test-framework-grizzly2-1.9.jar;C:\Users\saijy\.ivy2\cache\com.sun.xml.bind\jaxb-core\jars\jaxb-core-2.2.7.jar;C:\Users\saijy\.ivy2\cache\com.sun.xml.bind\jaxb-impl\jars\jaxb-impl-2.2.7.jar;C:\Users\saijy\.ivy2\cache\com.sun.xml.fastinfoset\FastInfoset\jars\FastInfoset-1.2.12.jar;C:\Users\saijy\.ivy2\cache\com.thoughtworks.paranamer\paranamer\jars\paranamer-2.6.jar;C:\Users\saijy\.ivy2\cache\com.twitter\chill-java\jars\chill-java-0.5.0.jar;C:\Users\saijy\.ivy2\cache\com.twitter\chill_2.11\jars\chill_2.11-0.5.0.jar;C:\Users\saijy\.ivy2\cache\com.typesafe\config\bundles\config-1.2.1.jar;C:\Users\saijy\.ivy2\cache\com.typesafe.akka\akka-actor_2.11\jars\akka-actor_2.11-2.3.11.jar;C:\Users\saijy\.ivy2\cache\com.typesafe.akka\akka-remote_2.11\jars\akka-remote_2.11-2.3.11.jar;C:\Users\saijy\.ivy2\cache\com.typesafe.akka\akka-slf4j_2.11\jars\akka-slf4j_2.11-2.3.11.jar;C:\Users\saijy\.ivy2\cache\commons-beanutils\commons-beanutils\jars\commons-beanutils-1.7.0.jar;C:\Users\saijy\.ivy2\cache\commons-beanutils\commons-beanutils-core\jars\commons-beanutils-core-1.8.0.jar;C:\Users\saijy\.ivy2\cache\commons-cli\commons-cli\jars\commons-cli-1.2.jar;C:\Users\saijy\.ivy2\cache\commons-codec\commons-codec\jars\commons-codec-1.5.jar;C:\Users\saijy\.ivy2\cache\commons-collections\commons-collections\jars\commons-collections-3.2.1.jar;C:\Users\saijy\.ivy2\cache\commons-configuration\commons-configuration\jars\commons-configuration-1.6.jar;C:\Users\saijy\.ivy2\cache\commons-digester\commons-digester\jars\commons-digester-1.8.jar;C:\Users\saijy\.ivy2\cache\commons-httpclient\commons-httpclient\jars\commons-httpclient-3.1.jar;C:\Users\saijy\.ivy2\cache\commons-io\commons-io\jars\commons-io-2.4.jar;C:\Users\saijy\.ivy2\cache\commons-lang\commons-lang\jars\commons-lang-2.5.jar;C:\Users\saijy\.ivy2\cache\io.dropwizard.metrics\metrics-core\bundles\metrics-core-3.1.2.jar;C:\Users\saijy\.ivy2\cache\io.dropwizard.metrics\metrics-graphite\bundles\metrics-graphite-3.1.2.jar;C:\Users\saijy\.ivy2\cache\io.dropwizard.metrics\metrics-json\bundles\metrics-json-3.1.2.jar;C:\Users\saijy\.ivy2\cache\io.dropwizard.metrics\metrics-jvm\bundles\metrics-jvm-3.1.2.jar;C:\Users\saijy\.ivy2\cache\io.netty\netty\bundles\netty-3.8.0.Final.jar;C:\Users\saijy\.ivy2\cache\io.netty\netty-all\jars\netty-all-4.0.29.Final.jar;C:\Users\saijy\.ivy2\cache\javax.inject\javax.inject\jars\javax.inject-1.jar;C:\Users\saijy\.ivy2\cache\javax.xml.bind\jaxb-api\jars\jaxb-api-2.2.7.jar;C:\Users\saijy\.ivy2\cache\javax.xml.bind\jsr173_api\jars\jsr173_api-1.0.jar;C:\Users\saijy\.ivy2\cache\jline\jline\jars\jline-0.9.94.jar;C:\Users\saijy\.ivy2\cache\log4j\log4j\bundles\log4j-1.2.17.jar;C:\Users\saijy\.ivy2\cache\net.java.dev.jets3t\jets3t\jars\jets3t-0.7.1.jar;C:\Users\saijy\.ivy2\cache\net.jpountz.lz4\lz4\jars\lz4-1.3.0.jar;C:\Users\saijy\.ivy2\cache\net.razorvine\pyrolite\jars\pyrolite-4.9.jar;C:\Users\saijy\.ivy2\cache\net.sf.opencsv\opencsv\jars\opencsv-2.3.jar;C:\Users\saijy\.ivy2\cache\net.sf.py4j\py4j\jars\py4j-0.9.jar;C:\Users\saijy\.ivy2\cache\net.sourceforge.f2j\arpack_combined_all\jars\arpack_combined_all-0.1-javadoc.jar;C:\Users\saijy\.ivy2\cache\net.sourceforge.f2j\arpack_combined_all\jars\arpack_combined_all-0.1.jar;C:\Users\saijy\.ivy2\cache\org.apache.avro\avro\jars\avro-1.7.7.jar;C:\Users\saijy\.ivy2\cache\org.apache.avro\avro-ipc\jars\avro-ipc-1.7.7.jar;C:\Users\saijy\.ivy2\cache\org.apache.avro\avro-ipc\jars\avro-ipc-1.7.7-tests.jar;C:\Users\saijy\.ivy2\cache\org.apache.avro\avro-mapred\jars\avro-mapred-1.7.7-hadoop2.jar;C:\Users\saijy\.ivy2\cache\org.apache.commons\commons-compress\jars\commons-compress-1.4.1.jar;C:\Users\saijy\.ivy2\cache\org.apache.commons\commons-lang3\jars\commons-lang3-3.3.2.jar;C:\Users\saijy\.ivy2\cache\org.apache.commons\commons-math\jars\commons-math-2.1.jar;C:\Users\saijy\.ivy2\cache\org.apache.commons\commons-math3\jars\commons-math3-3.4.1.jar;C:\Users\saijy\.ivy2\cache\org.apache.curator\curator-client\bundles\curator-client-2.4.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.curator\curator-framework\bundles\curator-framework-2.4.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.curator\curator-recipes\bundles\curator-recipes-2.4.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.hadoop\hadoop-annotations\jars\hadoop-annotations-2.2.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.hadoop\hadoop-auth\jars\hadoop-auth-2.2.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.hadoop\hadoop-client\jars\hadoop-client-2.2.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.hadoop\hadoop-common\jars\hadoop-common-2.2.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.hadoop\hadoop-hdfs\jars\hadoop-hdfs-2.2.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.hadoop\hadoop-mapreduce-client-app\jars\hadoop-mapreduce-client-app-2.2.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.hadoop\hadoop-mapreduce-client-common\jars\hadoop-mapreduce-client-common-2.2.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.hadoop\hadoop-mapreduce-client-core\jars\hadoop-mapreduce-client-core-2.2.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.hadoop\hadoop-mapreduce-client-jobclient\jars\hadoop-mapreduce-client-jobclient-2.2.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.hadoop\hadoop-mapreduce-client-shuffle\jars\hadoop-mapreduce-client-shuffle-2.2.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.hadoop\hadoop-yarn-api\jars\hadoop-yarn-api-2.2.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.hadoop\hadoop-yarn-client\jars\hadoop-yarn-client-2.2.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.hadoop\hadoop-yarn-common\jars\hadoop-yarn-common-2.2.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.hadoop\hadoop-yarn-server-common\jars\hadoop-yarn-server-common-2.2.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.ivy\ivy\jars\ivy-2.4.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.mesos\mesos\jars\mesos-0.21.1-shaded-protobuf.jar;C:\Users\saijy\.ivy2\cache\org.apache.parquet\parquet-column\jars\parquet-column-1.7.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.parquet\parquet-common\jars\parquet-common-1.7.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.parquet\parquet-encoding\jars\parquet-encoding-1.7.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.parquet\parquet-format\jars\parquet-format-2.3.0-incubating.jar;C:\Users\saijy\.ivy2\cache\org.apache.parquet\parquet-generator\jars\parquet-generator-1.7.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.parquet\parquet-hadoop\jars\parquet-hadoop-1.7.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.parquet\parquet-jackson\jars\parquet-jackson-1.7.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.spark\spark-catalyst_2.11\jars\spark-catalyst_2.11-1.6.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.spark\spark-core_2.11\jars\spark-core_2.11-1.6.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.spark\spark-graphx_2.11\jars\spark-graphx_2.11-1.6.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.spark\spark-launcher_2.11\jars\spark-launcher_2.11-1.6.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.spark\spark-mllib_2.11\jars\spark-mllib_2.11-1.6.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.spark\spark-network-common_2.11\jars\spark-network-common_2.11-1.6.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.spark\spark-network-shuffle_2.11\jars\spark-network-shuffle_2.11-1.6.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.spark\spark-sql_2.11\jars\spark-sql_2.11-1.6.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.spark\spark-streaming_2.11\jars\spark-streaming_2.11-1.6.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.spark\spark-unsafe_2.11\jars\spark-unsafe_2.11-1.6.0.jar;C:\Users\saijy\.ivy2\cache\org.apache.xbean\xbean-asm5-shaded\bundles\xbean-asm5-shaded-4.4.jar;C:\Users\saijy\.ivy2\cache\org.apache.zookeeper\zookeeper\jars\zookeeper-3.4.5.jar;C:\Users\saijy\.ivy2\cache\org.bytedeco\javacpp\maven-plugins\javacpp-0.11.jar;C:\Users\saijy\.ivy2\cache\org.bytedeco\javacv\jars\javacv-0.11.jar;C:\Users\saijy\.ivy2\cache\org.bytedeco.javacpp-presets\artoolkitplus\jars\artoolkitplus-2.3.1-0.11.jar;C:\Users\saijy\.ivy2\cache\org.bytedeco.javacpp-presets\ffmpeg\jars\ffmpeg-2.6.1-0.11.jar;C:\Users\saijy\.ivy2\cache\org.bytedeco.javacpp-presets\flandmark\jars\flandmark-1.07-0.11.jar;C:\Users\saijy\.ivy2\cache\org.bytedeco.javacpp-presets\flycapture\jars\flycapture-2.7.3.19-0.11.jar;C:\Users\saijy\.ivy2\cache\org.bytedeco.javacpp-presets\libdc1394\jars\libdc1394-2.2.3-0.11.jar;C:\Users\saijy\.ivy2\cache\org.bytedeco.javacpp-presets\libfreenect\jars\libfreenect-0.5.2-0.11.jar;C:\Users\saijy\.ivy2\cache\org.bytedeco.javacpp-presets\opencv\jars\opencv-2.4.11-0.11.jar;C:\Users\saijy\.ivy2\cache\org.bytedeco.javacpp-presets\opencv\jars\opencv-2.4.11-0.11-linux-x86.jar;C:\Users\saijy\.ivy2\cache\org.bytedeco.javacpp-presets\opencv\jars\opencv-2.4.11-0.11-linux-x86_64.jar;C:\Users\saijy\.ivy2\cache\org.bytedeco.javacpp-presets\opencv\jars\opencv-2.4.11-0.11-macosx-x86_64.jar;C:\Users\saijy\.ivy2\cache\org.bytedeco.javacpp-presets\opencv\jars\opencv-2.4.11-0.11-windows-x86.jar;C:\Users\saijy\.ivy2\cache\org.bytedeco.javacpp-presets\opencv\jars\opencv-2.4.11-0.11-windows-x86_64.jar;C:\Users\saijy\.ivy2\cache\org.bytedeco.javacpp-presets\videoinput\jars\videoinput-0.200-0.11.jar;C:\Users\saijy\.ivy2\cache\org.codehaus.jackson\jackson-core-asl\jars\jackson-core-asl-1.9.13.jar;C:\Users\saijy\.ivy2\cache\org.codehaus.jackson\jackson-mapper-asl\jars\jackson-mapper-asl-1.9.13.jar;C:\Users\saijy\.ivy2\cache\org.codehaus.janino\commons-compiler\jars\commons-compiler-2.7.8.jar;C:\Users\saijy\.ivy2\cache\org.codehaus.janino\janino\jars\janino-2.7.8.jar;C:\Users\saijy\.ivy2\cache\org.codehaus.jettison\jettison\bundles\jettison-1.1.jar;C:\Users\saijy\.ivy2\cache\org.eclipse.jetty.orbit\javax.servlet\orbits\javax.servlet-3.0.0.v201112011016.jar;C:\Users\saijy\.ivy2\cache\org.fusesource.leveldbjni\leveldbjni-all\bundles\leveldbjni-all-1.8.jar;C:\Users\saijy\.ivy2\cache\org.jpmml\pmml-agent\jars\pmml-agent-1.1.15.jar;C:\Users\saijy\.ivy2\cache\org.jpmml\pmml-model\jars\pmml-model-1.1.15.jar;C:\Users\saijy\.ivy2\cache\org.jpmml\pmml-schema\jars\pmml-schema-1.1.15.jar;C:\Users\saijy\.ivy2\cache\org.json4s\json4s-ast_2.11\jars\json4s-ast_2.11-3.2.10.jar;C:\Users\saijy\.ivy2\cache\org.json4s\json4s-core_2.11\jars\json4s-core_2.11-3.2.10.jar;C:\Users\saijy\.ivy2\cache\org.json4s\json4s-jackson_2.11\jars\json4s-jackson_2.11-3.2.10.jar;C:\Users\saijy\.ivy2\cache\org.mortbay.jetty\jetty-util\jars\jetty-util-6.1.26.jar;C:\Users\saijy\.ivy2\cache\org.objenesis\objenesis\jars\objenesis-1.2.jar;C:\Users\saijy\.ivy2\cache\org.roaringbitmap\RoaringBitmap\bundles\RoaringBitmap-0.5.11.jar;C:\Users\saijy\.ivy2\cache\org.scala-lang\scala-compiler\jars\scala-compiler-2.11.0.jar;C:\Users\saijy\.ivy2\cache\org.scala-lang\scala-library\jars\scala-library-2.11.8.jar;C:\Users\saijy\.ivy2\cache\org.scala-lang\scala-reflect\jars\scala-reflect-2.11.7.jar;C:\Users\saijy\.ivy2\cache\org.scala-lang\scalap\jars\scalap-2.11.0.jar;C:\Users\saijy\.ivy2\cache\org.scala-lang.modules\scala-parser-combinators_2.11\bundles\scala-parser-combinators_2.11-1.0.1.jar;C:\Users\saijy\.ivy2\cache\org.scala-lang.modules\scala-xml_2.11\bundles\scala-xml_2.11-1.0.1.jar;C:\Users\saijy\.ivy2\cache\org.scalanlp\breeze-macros_2.11\jars\breeze-macros_2.11-0.11.2.jar;C:\Users\saijy\.ivy2\cache\org.scalanlp\breeze_2.11\jars\breeze_2.11-0.11.2.jar;C:\Users\saijy\.ivy2\cache\org.slf4j\jcl-over-slf4j\jars\jcl-over-slf4j-1.7.10.jar;C:\Users\saijy\.ivy2\cache\org.slf4j\jul-to-slf4j\jars\jul-to-slf4j-1.7.10.jar;C:\Users\saijy\.ivy2\cache\org.slf4j\slf4j-api\jars\slf4j-api-1.7.10.jar;C:\Users\saijy\.ivy2\cache\org.slf4j\slf4j-log4j12\jars\slf4j-log4j12-1.7.10.jar;C:\Users\saijy\.ivy2\cache\org.sonatype.sisu.inject\cglib\jars\cglib-2.2.1-v20090111.jar;C:\Users\saijy\.ivy2\cache\org.spark-project.spark\unused\jars\unused-1.0.0.jar;C:\Users\saijy\.ivy2\cache\org.spire-math\spire-macros_2.11\jars\spire-macros_2.11-0.7.4.jar;C:\Users\saijy\.ivy2\cache\org.spire-math\spire_2.11\jars\spire_2.11-0.7.4.jar;C:\Users\saijy\.ivy2\cache\org.tachyonproject\tachyon-client\jars\tachyon-client-0.8.2.jar;C:\Users\saijy\.ivy2\cache\org.tachyonproject\tachyon-underfs-hdfs\jars\tachyon-underfs-hdfs-0.8.2.jar;C:\Users\saijy\.ivy2\cache\org.tachyonproject\tachyon-underfs-local\jars\tachyon-underfs-local-0.8.2.jar;C:\Users\saijy\.ivy2\cache\org.tachyonproject\tachyon-underfs-s3\jars\tachyon-underfs-s3-0.8.2.jar;C:\Users\saijy\.ivy2\cache\org.tukaani\xz\jars\xz-1.0.jar;C:\Users\saijy\.ivy2\cache\org.uncommons.maths\uncommons-maths\jars\uncommons-maths-1.2.2a.jar;C:\Users\saijy\.ivy2\cache\org.xerial.snappy\snappy-java\bundles\snappy-java-1.1.2.jar;C:\Users\saijy\.ivy2\cache\oro\oro\jars\oro-2.0.8.jar;C:\Users\saijy\.ivy2\cache\stax\stax-api\jars\stax-api-1.0.1.jar;C:\Users\saijy\.ivy2\cache\xmlenc\xmlenc\jars\xmlenc-0.52.jar;C:\Program Files (x86)\JetBrains\IntelliJ IDEA 2016.3.3\lib\idea_rt.jar" com.intellij.rt.execution.application.AppMain IPApp
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/02/16 16:45:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/02/16 16:45:50 INFO Slf4jLogger: Slf4jLogger started
17/02/16 16:45:50 INFO Remoting: Starting remoting
17/02/16 16:45:51 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.99.3.151:51611]
17/02/16 16:45:53 INFO FileInputFormat: Total input paths to process : 47
17/02/16 16:45:53 INFO FileInputFormat: Total input paths to process : 47
17/02/16 16:45:53 INFO CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 180638
[Stage 0:>                                                          (0 + 2) / 2]Key Descriptors 81 x 128
-- 81
Key Descriptors 179 x 128
-- 179
Key Descriptors 60 x 128
-- 60
Key Descriptors 56 x 128
-- 56
Key Descriptors 128 x 128
-- 128
Key Descriptors 214 x 128
-- 214
Key Descriptors 122 x 128
-- 122
Key Descriptors 57 x 128
-- 57
Key Descriptors 75 x 128
-- 75
Key Descriptors 65 x 128
-- 65
Key Descriptors 171 x 128
-- 171
Key Descriptors 44 x 128
-- 44
Key Descriptors 63 x 128
-- 63
Key Descriptors 28 x 128
-- 28
Key Descriptors 203 x 128
-- 203
Key Descriptors 269 x 128
-- 269
Key Descriptors 106 x 128
-- 106
Key Descriptors 152 x 128
-- 152
Key Descriptors 59 x 128
-- 59
Key Descriptors 185 x 128
17/02/16 16:46:03 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/02/16 16:46:03 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/02/16 16:46:03 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
17/02/16 16:46:03 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
17/02/16 16:46:03 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
Key Descriptors 169 x 128
-- 185
-- 169
Key Descriptors 149 x 128
-- 149
Key Descriptors 179 x 128
-- 179
Key Descriptors 198 x 128
Key Descriptors 196 x 128
-- 196
-- 198
Key Descriptors 38 x 128
-- 38
Key Descriptors 180 x 128
-- 180
Key Descriptors 524 x 128
-- 524
Key Descriptors 74 x 128
-- 74
Key Descriptors 389 x 128
-- 389
Key Descriptors 508 x 128
-- 508
Key Descriptors 395 x 128
-- 395
Key Descriptors 393 x 128
-- 393
Key Descriptors 484 x 128
-- 484
Key Descriptors 551 x 128
-- 551
Key Descriptors 590 x 128
-- 590
Key Descriptors 111 x 128
-- 111
Key Descriptors 666 x 128
-- 666
Key Descriptors 115 x 128
-- 115
Key Descriptors 96 x 128
-- 96
Key Descriptors 240 x 128
-- 240
Key Descriptors 331 x 128
-- 331
Key Descriptors 592 x 128
-- 592
Key Descriptors 147 x 128
-- 147
Key Descriptors 918 x 128
-- 918
Key Descriptors 424 x 128
-- 424
Key Descriptors 634 x 128
-- 634
17/02/16 16:46:03 WARN TaskSetManager: Stage 1 contains a task of very large size (1664 KB). The maximum recommended task size is 100 KB.
17/02/16 16:46:03 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
17/02/16 16:46:03 INFO deprecation: mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
17/02/16 16:46:03 INFO deprecation: mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
17/02/16 16:46:03 INFO deprecation: mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir
17/02/16 16:46:03 INFO FileOutputCommitter: Saved output of task 'attempt_201702161646_0001_m_000001_3' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/features/_temporary/0/task_201702161646_0001_m_000001
17/02/16 16:46:03 INFO FileOutputCommitter: Saved output of task 'attempt_201702161646_0001_m_000000_2' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/features/_temporary/0/task_201702161646_0001_m_000000
17/02/16 16:46:03 INFO FileOutputCommitter: Saved output of task 'attempt_201702161646_0001_m_000002_4' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/features/_temporary/0/task_201702161646_0001_m_000002
17/02/16 16:46:03 INFO FileOutputCommitter: Saved output of task 'attempt_201702161646_0001_m_000003_5' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/features/_temporary/0/task_201702161646_0001_m_000003
Total size : 11608
17/02/16 16:46:04 WARN KMeans: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.
17/02/16 16:46:05 INFO FileInputFormat: Total input paths to process : 4
17/02/16 16:46:06 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
17/02/16 16:46:06 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
17/02/16 16:48:06 WARN KMeans: The input data was not directly cached, which may hurt performance if its parent RDDs are also uncached.
Within Set Sum of Squared Errors = 7.722383076334606E8
17/02/16 16:48:06 INFO FileOutputCommitter: Saved output of task 'attempt_201702161648_0057_m_000000_226' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/clusters/metadata/_temporary/0/task_201702161648_0057_m_000000
17/02/16 16:48:09 WARN TaskSetManager: Stage 58 contains a task of very large size (105 KB). The maximum recommended task size is 100 KB.
17/02/16 16:48:09 INFO deprecation: mapreduce.outputformat.class is deprecated. Instead, use mapreduce.job.outputformat.class
17/02/16 16:48:09 INFO CodecConfig: Compression: GZIP
17/02/16 16:48:09 INFO ParquetOutputFormat: Parquet block size to 134217728
17/02/16 16:48:09 INFO ParquetOutputFormat: Parquet page size to 1048576
17/02/16 16:48:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
17/02/16 16:48:09 INFO ParquetOutputFormat: Dictionary is on
17/02/16 16:48:09 INFO ParquetOutputFormat: Validation is off
17/02/16 16:48:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
17/02/16 16:48:09 INFO CodecConfig: Compression: GZIP
17/02/16 16:48:09 INFO CodecConfig: Compression: GZIP
17/02/16 16:48:09 INFO CodecConfig: Compression: GZIP
17/02/16 16:48:09 INFO ParquetOutputFormat: Parquet block size to 134217728
17/02/16 16:48:09 INFO ParquetOutputFormat: Parquet page size to 1048576
17/02/16 16:48:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
17/02/16 16:48:09 INFO ParquetOutputFormat: Dictionary is on
17/02/16 16:48:09 INFO ParquetOutputFormat: Validation is off
17/02/16 16:48:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
17/02/16 16:48:09 INFO ParquetOutputFormat: Parquet block size to 134217728
17/02/16 16:48:09 INFO ParquetOutputFormat: Parquet page size to 1048576
17/02/16 16:48:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
17/02/16 16:48:09 INFO ParquetOutputFormat: Dictionary is on
17/02/16 16:48:09 INFO ParquetOutputFormat: Validation is off
17/02/16 16:48:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
17/02/16 16:48:09 INFO ParquetOutputFormat: Parquet block size to 134217728
17/02/16 16:48:09 INFO ParquetOutputFormat: Parquet page size to 1048576
17/02/16 16:48:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
17/02/16 16:48:09 INFO ParquetOutputFormat: Dictionary is on
17/02/16 16:48:09 INFO ParquetOutputFormat: Validation is off
17/02/16 16:48:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
17/02/16 16:48:09 INFO CodecPool: Got brand-new compressor [.gz]
17/02/16 16:48:10 INFO CodecPool: Got brand-new compressor [.gz]
17/02/16 16:48:10 INFO CodecPool: Got brand-new compressor [.gz]
17/02/16 16:48:10 INFO CodecPool: Got brand-new compressor [.gz]
17/02/16 16:48:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 124,140
17/02/16 16:48:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 122,716
17/02/16 16:48:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 127,588
17/02/16 16:48:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 123,028
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
[Stage 58:>                                                         (0 + 4) / 4]17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 216B for [id] INT32: 100 values, 407B raw, 180B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 218B for [id] INT32: 100 values, 407B raw, 182B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 214B for [id] INT32: 100 values, 407B raw, 178B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 62B for [point, type] INT32: 100 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 4B raw, 1B comp}
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 62B for [point, type] INT32: 100 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 4B raw, 1B comp}
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 62B for [point, type] INT32: 100 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 4B raw, 1B comp}
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 50B for [point, size] INT32: 100 values, 7B raw, 27B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 50B for [point, size] INT32: 100 values, 7B raw, 27B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 53B for [point, indices, list, element] INT32: 100 values, 14B raw, 30B comp, 1 pages, encodings: [RLE, PLAIN]
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 53B for [point, indices, list, element] INT32: 100 values, 14B raw, 30B comp, 1 pages, encodings: [RLE, PLAIN]
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 22,302B for [point, values, list, element] DOUBLE: 12,800 values, 22,919B raw, 22,255B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE], dic { 9,334 entries, 74,672B raw, 9,334B comp}
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 22,373B for [point, values, list, element] DOUBLE: 12,800 values, 22,939B raw, 22,326B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE], dic { 8,903 entries, 71,224B raw, 8,903B comp}
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 50B for [point, size] INT32: 100 values, 7B raw, 27B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 53B for [point, indices, list, element] INT32: 100 values, 14B raw, 30B comp, 1 pages, encodings: [RLE, PLAIN]
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 22,249B for [point, values, list, element] DOUBLE: 12,800 values, 22,939B raw, 22,202B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE], dic { 8,725 entries, 69,800B raw, 8,725B comp}
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 214B for [id] INT32: 100 values, 407B raw, 178B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 62B for [point, type] INT32: 100 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 4B raw, 1B comp}
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 50B for [point, size] INT32: 100 values, 7B raw, 27B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 53B for [point, indices, list, element] INT32: 100 values, 14B raw, 30B comp, 1 pages, encodings: [RLE, PLAIN]
17/02/16 16:48:10 INFO ColumnChunkPageWriteStore: written 22,287B for [point, values, list, element] DOUBLE: 12,800 values, 22,939B raw, 22,240B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE], dic { 8,764 entries, 70,112B raw, 8,764B comp}
17/02/16 16:48:10 INFO FileOutputCommitter: Saved output of task 'attempt_201702161648_0058_m_000000_0' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/clusters/data/_temporary/0/task_201702161648_0058_m_000000
17/02/16 16:48:10 INFO FileOutputCommitter: Saved output of task 'attempt_201702161648_0058_m_000001_0' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/clusters/data/_temporary/0/task_201702161648_0058_m_000001
17/02/16 16:48:10 INFO FileOutputCommitter: Saved output of task 'attempt_201702161648_0058_m_000002_0' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/clusters/data/_temporary/0/task_201702161648_0058_m_000002
17/02/16 16:48:10 INFO FileOutputCommitter: Saved output of task 'attempt_201702161648_0058_m_000003_0' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/clusters/data/_temporary/0/task_201702161648_0058_m_000003
17/02/16 16:48:10 INFO ParquetFileReader: Initiating action with parallelism: 5
Saves Clusters to data/model/clusters
17/02/16 16:48:11 WARN TaskSetManager: Stage 59 contains a task of very large size (199 KB). The maximum recommended task size is 100 KB.
17/02/16 16:48:11 INFO FileOutputCommitter: Saved output of task 'attempt_201702161648_0059_m_000002_233' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/clusterCenters/_temporary/0/task_201702161648_0059_m_000002
17/02/16 16:48:12 INFO FileOutputCommitter: Saved output of task 'attempt_201702161648_0059_m_000003_234' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/clusterCenters/_temporary/0/task_201702161648_0059_m_000003
17/02/16 16:48:12 INFO FileOutputCommitter: Saved output of task 'attempt_201702161648_0059_m_000001_232' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/clusterCenters/_temporary/0/task_201702161648_0059_m_000001
17/02/16 16:48:12 INFO FileOutputCommitter: Saved output of task 'attempt_201702161648_0059_m_000000_231' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/clusterCenters/_temporary/0/task_201702161648_0059_m_000000
17/02/16 16:48:12 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:48:12 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:48:12 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:48:12 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:48:12 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:48:12 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
17/02/16 16:48:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:48:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:48:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:48:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:48:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:48:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:48:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:48:13 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:48:13 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:48:13 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:48:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:48:13 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:48:13 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:48:13 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:48:13 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:48:13 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:48:13 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 100
17/02/16 16:48:13 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 100
17/02/16 16:48:13 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 100
17/02/16 16:48:13 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 100
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024691358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024691358, 0.0, 0.0, 0.0, 0.0, 0.024691358, 0.0, 0.0, 0.037037037, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024691358, 0.0, 0.012345679, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.024691358, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.12345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024691358, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024691358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.012345679, 0.0, 0.0, 0.024691358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.024691358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024691358, 0.012345679, 0.024691358, 0.0, 0.024691358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.024691358, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.024691358, 0.0, 0.0, 0.0, 0.024691358, 0.0 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.05586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.005586592, 0.0, 0.0, 0.011173184, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022346368, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.011173184, 0.005586592, 0.0, 0.0, 0.0, 0.022346368, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.005586592, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.011173184, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.016759776, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.011173184, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.005586592, 0.016759776, 0.0, 0.005586592, 0.05586592, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.005586592, 0.0, 0.0, 0.016759776, 0.005586592, 0.016759776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016759776, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016759776, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.011173184, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016759776, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.0, 0.005586592, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.016759776, 0.0, 0.039106146, 0.0, 0.016759776, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016759776, 0.011173184, 0.016759776, 0.011173184, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.016759776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.011173184 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.050000004, 0.0, 0.0, 0.016666668, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033333335, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050000004, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033333335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.033333335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.116666675, 0.0, 0.0, 0.016666668, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033333335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.033333335, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.033333335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050000004, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08928572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035714287, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035714287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19642858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.035714287, 0.017857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14285715, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.03125, 0.0, 0.0078125, 0.0390625, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.015625, 0.0, 0.0, 0.0390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0390625, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.015625, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.015625, 0.0, 0.015625, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.015625, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0078125, 0.015625, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.015625, 0.0078125 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.004672897, 0.004672897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.0, 0.014018691, 0.0, 0.0, 0.0, 0.018691588, 0.0, 0.004672897, 0.0, 0.004672897, 0.0, 0.0, 0.004672897, 0.0, 0.0, 0.004672897, 0.0, 0.004672897, 0.014018691, 0.004672897, 0.004672897, 0.0, 0.0, 0.0, 0.004672897, 0.009345794, 0.0, 0.009345794, 0.0, 0.004672897, 0.004672897, 0.0, 0.023364484, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.004672897, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.004672897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004672897, 0.004672897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.004672897, 0.0, 0.0, 0.004672897, 0.004672897, 0.004672897, 0.0, 0.004672897, 0.009345794, 0.004672897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014018691, 0.0, 0.0, 0.0, 0.0, 0.014018691, 0.0, 0.004672897, 0.0, 0.0, 0.0, 0.0, 0.018691588, 0.004672897, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.009345794, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009345794, 0.0, 0.009345794, 0.004672897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004672897, 0.009345794, 0.004672897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.009345794, 0.0, 0.0, 0.004672897, 0.014018691, 0.004672897, 0.0, 0.0, 0.004672897, 0.0, 0.0, 0.004672897, 0.004672897, 0.014018691, 0.004672897, 0.0, 0.004672897, 0.0, 0.0, 0.004672897, 0.0, 0.009345794, 0.0, 0.0, 0.004672897, 0.009345794, 0.0, 0.004672897, 0.014018691, 0.0, 0.004672897, 0.0, 0.014018691, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014018691, 0.0, 0.009345794, 0.0, 0.0, 0.0, 0.009345794, 0.0, 0.004672897, 0.004672897, 0.004672897, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.0, 0.004672897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.009345794, 0.004672897, 0.0, 0.0, 0.014018691, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023364484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.0, 0.0, 0.023364484, 0.028037382, 0.009345794, 0.0, 0.0, 0.004672897, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014018691, 0.0, 0.0, 0.0, 0.004672897, 0.004672897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.009345794, 0.004672897, 0.0, 0.0, 0.014018691, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023364484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028037382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009345794, 0.009345794, 0.0, 0.0, 0.004672897, 0.009345794, 0.0, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.0, 0.0, 0.009345794, 0.0, 0.004672897, 0.0, 0.014018691, 0.0, 0.014018691, 0.004672897, 0.009345794, 0.0, 0.004672897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014018691, 0.0, 0.0, 0.018691588, 0.0, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.0, 0.03271028, 0.004672897, 0.0, 0.009345794, 0.004672897, 0.0, 0.0, 0.004672897, 0.004672897, 0.0, 0.0, 0.004672897, 0.0, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.009345794, 0.0, 0.0, 0.0, 0.004672897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.008196721, 0.0, 0.008196721, 0.0, 0.0, 0.024590163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.024590163, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024590163, 0.0, 0.0, 0.008196721, 0.0, 0.008196721, 0.008196721, 0.0, 0.0, 0.016393442, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.024590163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.008196721, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.008196721, 0.016393442, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.008196721, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.024590163, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.016393442, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.008196721, 0.0, 0.0, 0.008196721, 0.0, 0.008196721, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.008196721, 0.008196721, 0.0, 0.008196721, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.016393442, 0.0, 0.032786883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.008196721, 0.0, 0.008196721, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.049180325, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.008196721, 0.0, 0.040983602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008196721, 0.0, 0.0, 0.0, 0.016393442, 0.0 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03508772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.05263158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.03508772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.03508772, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.03508772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03508772, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05263158, 0.0, 0.0, 0.0, 0.03508772, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03508772, 0.0, 0.01754386, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.01754386, 0.01754386, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03508772, 0.0, 0.0, 0.03508772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
-- 1
[Stage 63:>                                                         (0 + 2) / 2]400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.013333334, 0.0, 0.013333334, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.013333334, 0.0, 0.0, 0.0, 0.013333334, 0.013333334, 0.0, 0.026666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.026666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.013333334, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.053333335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026666667, 0.026666667, 0.0, 0.0, 0.026666667, 0.026666667, 0.0, 0.013333334, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.013333334, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.013333334, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.026666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0, 0.0 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046153847, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03076923, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.03076923, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.015384615, 0.0, 0.015384615, 0.015384615, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03076923, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046153847, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.046153847, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.015384615, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046153847, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03076923, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03076923, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015384615, 0.015384615, 0.0, 0.0, 0.015384615, 0.0, 0.015384615, 0.0, 0.0, 0.015384615, 0.015384615, 0.0, 0.0, 0.0, 0.015384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0116959065, 0.0, 0.0058479533, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0116959065, 0.0, 0.01754386, 0.0116959065, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0116959065, 0.0, 0.0, 0.0, 0.0058479533, 0.0116959065, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0058479533, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0116959065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0058479533, 0.0, 0.0058479533, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.023391813, 0.0, 0.0058479533, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0058479533, 0.0058479533, 0.0, 0.0, 0.0, 0.0058479533, 0.0058479533, 0.0, 0.0058479533, 0.0, 0.0058479533, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0116959065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.029239766, 0.0, 0.0, 0.0116959065, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0116959065, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0116959065, 0.0, 0.0, 0.023391813, 0.0, 0.0058479533, 0.0116959065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0058479533, 0.0058479533, 0.01754386, 0.0, 0.0, 0.029239766, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0058479533, 0.01754386, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0116959065, 0.0, 0.0116959065, 0.0, 0.0116959065, 0.0058479533, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0116959065, 0.0, 0.0058479533, 0.0058479533, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0058479533, 0.0058479533, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0116959065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0058479533, 0.0058479533, 0.0, 0.0, 0.0, 0.0116959065, 0.0058479533, 0.0, 0.0, 0.0, 0.0058479533, 0.0058479533, 0.0058479533, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0058479533, 0.023391813, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.01754386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.03508772, 0.0, 0.0116959065, 0.0116959065, 0.0116959065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.01754386, 0.0, 0.0058479533, 0.0, 0.0, 0.01754386, 0.0058479533, 0.0, 0.0, 0.0, 0.0, 0.0058479533 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.06818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.045454547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.045454547, 0.022727273, 0.022727273, 0.0, 0.0, 0.0, 0.0, 0.09090909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.022727273, 0.0, 0.045454547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.045454547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.022727273, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.022727273, 0.0, 0.0, 0.0 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035714287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.107142866, 0.0, 0.0, 0.0, 0.0, 0.035714287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.071428575, 0.0, 0.0, 0.0, 0.0, 0.071428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.071428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.071428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.071428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.071428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.071428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.071428575, 0.107142866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035714287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035714287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035714287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035714287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.015873017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07936508, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07936508, 0.0, 0.015873017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015873017, 0.015873017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015873017, 0.015873017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015873017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015873017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.031746034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015873017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.095238104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015873017, 0.0, 0.0, 0.0, 0.0, 0.015873017, 0.0, 0.0, 0.0, 0.0, 0.015873017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015873017, 0.015873017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.031746034, 0.047619052, 0.0, 0.0, 0.0, 0.0, 0.015873017, 0.0, 0.0, 0.015873017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015873017, 0.0, 0.0, 0.0, 0.0, 0.015873017, 0.0, 0.0, 0.0, 0.031746034, 0.0, 0.0, 0.015873017, 0.0, 0.0, 0.031746034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.031746034, 0.0, 0.0, 0.0, 0.015873017, 0.0, 0.0, 0.0, 0.015873017, 0.015873017, 0.0, 0.015873017, 0.0, 0.0, 0.015873017, 0.031746034, 0.0, 0.015873017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015873017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015873017, 0.0, 0.0, 0.015873017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015873017, 0.0, 0.0, 0.0, 0.047619052, 0.0 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.003717472, 0.007434944, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.003717472, 0.003717472, 0.0, 0.048327137, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0111524165, 0.01858736, 0.003717472, 0.0, 0.0, 0.0, 0.0, 0.026022304, 0.007434944, 0.003717472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.003717472, 0.007434944, 0.003717472, 0.0, 0.0, 0.014869888, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.007434944, 0.003717472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.0, 0.003717472, 0.003717472, 0.0, 0.0, 0.0111524165, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.003717472, 0.007434944, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022304833, 0.0, 0.0, 0.003717472, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014869888, 0.003717472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003717472, 0.0111524165, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.0111524165, 0.0, 0.0, 0.0, 0.0, 0.007434944, 0.0111524165, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.003717472, 0.0, 0.003717472, 0.007434944, 0.007434944, 0.0, 0.01858736, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.007434944, 0.0, 0.0, 0.0, 0.0, 0.014869888, 0.0, 0.0, 0.0, 0.007434944, 0.007434944, 0.0, 0.0, 0.003717472, 0.003717472, 0.0, 0.003717472, 0.0, 0.0, 0.003717472, 0.003717472, 0.0, 0.003717472, 0.0, 0.003717472, 0.014869888, 0.01858736, 0.0, 0.003717472, 0.003717472, 0.007434944, 0.0, 0.03345725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.01858736, 0.007434944, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003717472, 0.003717472, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.007434944, 0.0111524165, 0.0, 0.0, 0.0111524165, 0.007434944, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.0, 0.0111524165, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.003717472, 0.003717472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01858736, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007434944, 0.007434944, 0.0, 0.003717472, 0.007434944, 0.003717472, 0.003717472, 0.003717472, 0.007434944, 0.0, 0.0, 0.007434944, 0.0, 0.007434944, 0.0, 0.003717472, 0.0, 0.007434944, 0.0, 0.0, 0.003717472, 0.007434944, 0.007434944, 0.0, 0.0, 0.0, 0.007434944, 0.0, 0.0, 0.0, 0.014869888, 0.0, 0.0, 0.003717472, 0.0, 0.0111524165, 0.0, 0.0, 0.0, 0.003717472, 0.007434944, 0.0111524165, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.0, 0.003717472, 0.0, 0.0, 0.0111524165, 0.003717472, 0.0, 0.0, 0.0, 0.003717472, 0.003717472, 0.01858736, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003717472, 0.003717472, 0.003717472, 0.0, 0.0, 0.0, 0.0111524165, 0.022304833, 0.007434944, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003717472, 0.003717472, 0.0, 0.007434944, 0.0, 0.026022304, 0.0, 0.0, 0.0, 0.003717472, 0.007434944, 0.0, 0.003717472, 0.0, 0.003717472, 0.003717472 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.014778325, 0.0, 0.0, 0.0, 0.0049261083, 0.014778325, 0.0, 0.0, 0.0049261083, 0.0, 0.0, 0.0049261083, 0.009852217, 0.0049261083, 0.0, 0.0, 0.0049261083, 0.0049261083, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009852217, 0.0, 0.0, 0.009852217, 0.009852217, 0.0, 0.0, 0.0049261083, 0.0, 0.0, 0.0, 0.0049261083, 0.009852217, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019704433, 0.0049261083, 0.0, 0.0, 0.0049261083, 0.0049261083, 0.0, 0.0049261083, 0.0, 0.0049261083, 0.0, 0.0049261083, 0.0, 0.0, 0.0049261083, 0.0, 0.0, 0.0049261083, 0.0049261083, 0.0, 0.009852217, 0.0049261083, 0.0, 0.0, 0.0, 0.0049261083, 0.0049261083, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0049261083, 0.0, 0.0, 0.0049261083, 0.02955665, 0.0, 0.0, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0049261083, 0.0049261083, 0.0, 0.0, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0049261083, 0.0049261083, 0.0, 0.0, 0.009852217, 0.0, 0.0049261083, 0.0049261083, 0.0, 0.0, 0.0049261083, 0.0049261083, 0.0, 0.0, 0.0049261083, 0.0, 0.0049261083, 0.0, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0049261083, 0.0, 0.0049261083, 0.0049261083, 0.0, 0.0, 0.0, 0.0049261083, 0.0, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.009852217, 0.0, 0.0, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.0049261083, 0.0, 0.0049261083, 0.014778325, 0.019704433, 0.0, 0.0, 0.0049261083, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009852217, 0.009852217, 0.0, 0.0, 0.0049261083, 0.0, 0.009852217, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009852217, 0.0, 0.0049261083, 0.019704433, 0.0, 0.0, 0.0, 0.0, 0.0049261083, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0049261083, 0.0049261083, 0.0, 0.0049261083, 0.0049261083, 0.02955665, 0.0, 0.0, 0.009852217, 0.0049261083, 0.0049261083, 0.0049261083, 0.0, 0.0, 0.009852217, 0.0049261083, 0.0, 0.0, 0.014778325, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.0049261083, 0.014778325, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.009852217, 0.0, 0.0, 0.0049261083, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014778325, 0.0049261083, 0.0, 0.009852217, 0.0, 0.0, 0.0, 0.0, 0.009852217, 0.0, 0.0, 0.0, 0.0049261083, 0.0049261083, 0.0, 0.0, 0.0049261083, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.0049261083, 0.0, 0.0, 0.0049261083, 0.0049261083, 0.0049261083, 0.0, 0.009852217, 0.0, 0.0049261083, 0.0, 0.0, 0.0, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009852217, 0.0, 0.0, 0.0049261083, 0.0, 0.0049261083, 0.0, 0.0049261083, 0.0, 0.0049261083, 0.0049261083, 0.009852217, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009852217, 0.0, 0.0049261083, 0.0, 0.0, 0.0049261083, 0.0, 0.0, 0.0, 0.009852217, 0.02955665, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009852217, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0049261083, 0.0049261083, 0.0, 0.0049261083, 0.0, 0.0, 0.0, 0.0049261083, 0.0, 0.0, 0.0049261083, 0.0049261083, 0.0049261083, 0.0, 0.0, 0.0049261083, 0.0, 0.0049261083, 0.0, 0.0, 0.0049261083, 0.0, 0.0049261083, 0.0, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0049261083, 0.014778325, 0.009852217, 0.014778325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0049261083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0049261083, 0.0, 0.0, 0.0049261083, 0.0, 0.0, 0.0049261083 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0065789474, 0.0, 0.013157895, 0.0, 0.0, 0.0, 0.0, 0.019736841, 0.0, 0.02631579, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032894738, 0.0, 0.0, 0.0065789474, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.019736841, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0065789474, 0.013157895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0065789474, 0.0065789474, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0065789474, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.013157895, 0.013157895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019736841, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013157895, 0.0, 0.0, 0.0065789474, 0.039473683, 0.0, 0.0, 0.0065789474, 0.0, 0.0065789474, 0.0, 0.0, 0.013157895, 0.0065789474, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013157895, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.013157895, 0.0, 0.02631579, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.019736841, 0.02631579, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0065789474, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013157895, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013157895, 0.0, 0.0065789474, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0065789474, 0.0, 0.0065789474, 0.0065789474, 0.0, 0.0, 0.019736841, 0.0, 0.0, 0.013157895, 0.0, 0.0, 0.013157895, 0.0065789474, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.013157895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.013157895, 0.02631579, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019736841, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.02631579, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019736841, 0.0, 0.0, 0.0, 0.013157895, 0.0, 0.0, 0.0, 0.0065789474, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013157895, 0.0, 0.0, 0.0065789474, 0.0, 0.0, 0.0, 0.019736841, 0.0065789474, 0.0, 0.0, 0.0065789474, 0.0, 0.0 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.009433962, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.018867925, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.009433962, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.028301887, 0.0, 0.0, 0.018867925, 0.009433962, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.009433962, 0.009433962, 0.009433962, 0.0, 0.009433962, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.047169812, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.009433962, 0.009433962, 0.0, 0.0, 0.0, 0.018867925, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.009433962, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.028301887, 0.0, 0.0, 0.009433962, 0.009433962, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028301887, 0.0, 0.009433962, 0.018867925, 0.0, 0.0, 0.009433962, 0.028301887, 0.0, 0.009433962, 0.009433962, 0.018867925, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028301887, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.009433962, 0.0 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.033898305, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033898305, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.016949153, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033898305, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033898305, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033898305, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.016949153, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033898305, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779661, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033898305, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.033898305, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.016216217, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016216217, 0.016216217, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010810811, 0.0, 0.0, 0.0, 0.0054054055, 0.010810811, 0.0, 0.0054054055, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.010810811, 0.0054054055, 0.0, 0.0, 0.0054054055, 0.016216217, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010810811, 0.0, 0.010810811, 0.0, 0.0, 0.0, 0.0, 0.010810811, 0.0054054055, 0.032432433, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0054054055, 0.010810811, 0.0, 0.010810811, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.010810811, 0.0, 0.027027028, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0054054055, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.016216217, 0.0, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021621622, 0.0054054055, 0.0, 0.0054054055, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0054054055, 0.0, 0.010810811, 0.0, 0.0, 0.0054054055, 0.0, 0.010810811, 0.0, 0.010810811, 0.0, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0054054055, 0.010810811, 0.0, 0.0, 0.0, 0.0, 0.016216217, 0.010810811, 0.0, 0.010810811, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016216217, 0.0, 0.010810811, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010810811, 0.0, 0.0, 0.0, 0.016216217, 0.016216217, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0054054055, 0.0, 0.0054054055, 0.0, 0.0, 0.0054054055, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010810811, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0054054055, 0.0054054055, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0054054055, 0.010810811, 0.0, 0.0054054055, 0.0, 0.0054054055, 0.0, 0.021621622, 0.0054054055, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.032432433, 0.0, 0.0054054055, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010810811, 0.0, 0.0, 0.0, 0.0054054055, 0.010810811, 0.0, 0.0, 0.0054054055, 0.010810811, 0.010810811, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010810811, 0.0, 0.0054054055, 0.010810811, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021621622, 0.0054054055, 0.0, 0.0054054055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.021621622, 0.0, 0.0, 0.0054054055, 0.0, 0.0, 0.0054054055, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0054054055, 0.0, 0.0 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.01775148, 0.0, 0.0, 0.00591716, 0.00591716, 0.0, 0.0, 0.00591716, 0.0, 0.01183432, 0.00591716, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.00591716, 0.0, 0.0, 0.00591716, 0.00591716, 0.0, 0.00591716, 0.00591716, 0.01183432, 0.0, 0.0, 0.01775148, 0.0, 0.01775148, 0.00591716, 0.02366864, 0.0, 0.0, 0.01775148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01775148, 0.0, 0.0, 0.00591716, 0.0, 0.02366864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.00591716, 0.0, 0.00591716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07100592, 0.0, 0.0, 0.0, 0.01775148, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.02366864, 0.00591716, 0.00591716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.00591716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.00591716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.00591716, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.00591716, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.00591716, 0.00591716, 0.0, 0.02366864, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.02366864, 0.0, 0.0, 0.0, 0.01775148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01775148, 0.0, 0.00591716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.01183432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.00591716, 0.00591716, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.00591716, 0.0, 0.00591716, 0.0, 0.00591716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.00591716, 0.00591716, 0.01183432, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.01775148, 0.0, 0.0, 0.0, 0.01775148, 0.01183432, 0.0, 0.02366864, 0.0, 0.00591716, 0.00591716, 0.00591716, 0.01775148, 0.00591716, 0.0, 0.0, 0.00591716, 0.01183432, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.00591716, 0.00591716, 0.0, 0.02366864, 0.0, 0.0, 0.0, 0.00591716, 0.00591716, 0.0, 0.0, 0.00591716, 0.01775148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00591716, 0.00591716, 0.00591716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01183432, 0.01183432, 0.0, 0.0, 0.0, 0.00591716, 0.0, 0.0, 0.0, 0.00591716, 0.00591716, 0.0 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.005586592, 0.0, 0.005586592, 0.0, 0.005586592, 0.005586592, 0.005586592, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.0, 0.022346368, 0.0, 0.005586592, 0.005586592, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.0, 0.005586592, 0.005586592, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.05586592, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.005586592, 0.022346368, 0.011173184, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.016759776, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.005586592, 0.005586592, 0.0, 0.016759776, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.016759776, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.011173184, 0.0, 0.011173184, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.016759776, 0.011173184, 0.005586592, 0.011173184, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.005586592, 0.0, 0.016759776, 0.0, 0.005586592, 0.0, 0.005586592, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.016759776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.005586592, 0.005586592, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.005586592, 0.005586592, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.016759776, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.022346368, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.022346368, 0.005586592, 0.011173184, 0.0, 0.005586592, 0.0, 0.005586592, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.016759776, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.011173184, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.0, 0.0 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0067114094, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013422819, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033557046, 0.0, 0.0, 0.0, 0.0067114094, 0.0067114094, 0.0, 0.0067114094, 0.02013423, 0.033557046, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0067114094, 0.0, 0.013422819, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013422819, 0.0067114094, 0.02013423, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013422819, 0.0, 0.02013423, 0.0, 0.0067114094, 0.0, 0.046979867, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02013423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.013422819, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0067114094, 0.0, 0.0, 0.013422819, 0.0, 0.0, 0.0067114094, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02013423, 0.0, 0.0, 0.0, 0.0, 0.02013423, 0.0067114094, 0.0067114094, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.013422819, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.013422819, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.02013423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.013422819, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02013423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0067114094, 0.02013423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0067114094, 0.02013423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0067114094, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.013422819, 0.0, 0.0, 0.0, 0.0067114094, 0.0067114094, 0.0, 0.0, 0.013422819, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013422819, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.013422819, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.02013423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013422819, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0, 0.033557046, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013422819, 0.0, 0.0, 0.02013423, 0.0067114094, 0.0, 0.0, 0.0, 0.026845638, 0.0, 0.0, 0.0, 0.0067114094, 0.0, 0.0 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010204081, 0.0051020407, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0051020407, 0.0051020407, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0051020407, 0.0, 0.0051020407, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0051020407, 0.0, 0.010204081, 0.0051020407, 0.0, 0.0051020407, 0.0051020407, 0.0, 0.0051020407, 0.0051020407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010204081, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010204081, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0051020407, 0.0051020407, 0.0, 0.0, 0.015306123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.030612245, 0.015306123, 0.0051020407, 0.010204081, 0.0, 0.0, 0.0051020407, 0.0, 0.0051020407, 0.0, 0.0, 0.0051020407, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0051020407, 0.015306123, 0.0, 0.010204081, 0.0, 0.0, 0.0, 0.0, 0.020408163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0051020407, 0.0, 0.0051020407, 0.0, 0.0051020407, 0.0051020407, 0.0, 0.0, 0.0051020407, 0.0, 0.010204081, 0.0, 0.0051020407, 0.0051020407, 0.0, 0.0, 0.030612245, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0051020407, 0.0051020407, 0.0, 0.0, 0.0051020407, 0.0051020407, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0051020407, 0.0051020407, 0.0051020407, 0.0051020407, 0.0, 0.0051020407, 0.0051020407, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0, 0.0051020407, 0.0051020407, 0.0051020407, 0.0, 0.0051020407, 0.0, 0.0, 0.010204081, 0.0, 0.035714284, 0.0, 0.0, 0.015306123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025510203, 0.0, 0.010204081, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0, 0.0051020407, 0.0051020407, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010204081, 0.0, 0.0051020407, 0.0, 0.010204081, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015306123, 0.030612245, 0.0051020407, 0.0, 0.0, 0.0, 0.025510203, 0.010204081, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035714284, 0.010204081, 0.0, 0.0, 0.0, 0.010204081, 0.0051020407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0051020407, 0.0051020407, 0.0, 0.0051020407, 0.0, 0.0051020407, 0.0, 0.0, 0.0, 0.0, 0.015306123, 0.0051020407, 0.0, 0.020408163, 0.0, 0.0, 0.0, 0.025510203, 0.010204081, 0.0051020407, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0051020407, 0.0, 0.0051020407, 0.010204081, 0.0, 0.0, 0.0051020407, 0.010204081, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010204081, 0.0, 0.0, 0.0, 0.0, 0.015306123, 0.0, 0.0, 0.0, 0.0051020407, 0.0, 0.0 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02631579, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02631579, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15789473, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02631579, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02631579, 0.0, 0.0, 0.0, 0.0, 0.05263158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05263158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02631579, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10526316, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02631579, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05263158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02631579, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05263158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10526316, 0.0, 0.0, 0.02631579, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02631579, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02631579, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10526316, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02631579, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02631579, 0.0 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.015151516, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.0, 0.005050505, 0.0, 0.005050505, 0.0, 0.01010101, 0.005050505, 0.0, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.030303031, 0.0, 0.02020202, 0.0, 0.005050505, 0.005050505, 0.0, 0.0, 0.0, 0.02020202, 0.0, 0.025252525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.005050505, 0.0, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13131313, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.01010101, 0.0, 0.0, 0.0, 0.005050505, 0.015151516, 0.0, 0.01010101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.005050505, 0.0, 0.005050505, 0.015151516, 0.0, 0.01010101, 0.0, 0.005050505, 0.0, 0.0, 0.01010101, 0.005050505, 0.0, 0.025252525, 0.0, 0.0, 0.015151516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01010101, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.0, 0.005050505, 0.01010101, 0.0, 0.0, 0.01010101, 0.035353534, 0.0, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.01010101, 0.01010101, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01010101, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.015151516, 0.0, 0.02020202, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.0, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.005050505, 0.0, 0.0, 0.01010101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015151516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01010101, 0.0, 0.0, 0.005050505, 0.005050505, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.02020202, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.02020202, 0.0, 0.0, 0.0, 0.015151516, 0.0, 0.0, 0.02020202, 0.005050505, 0.0, 0.0, 0.005050505, 0.0, 0.01010101, 0.015151516, 0.0, 0.005050505, 0.045454547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.01010101, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.0, 0.0, 0.01010101, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005050505, 0.0, 0.0, 0.0, 0.01010101, 0.0, 0.0, 0.0, 0.01010101, 0.015151516, 0.005050505 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0055555557, 0.0, 0.0, 0.0, 0.011111111, 0.0055555557, 0.0, 0.0055555557, 0.0, 0.0, 0.0055555557, 0.0055555557, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0055555557, 0.0, 0.011111111, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.011111111, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.0055555557, 0.011111111, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03888889, 0.0055555557, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0055555557, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.022222223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0055555557, 0.011111111, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.011111111, 0.0, 0.0, 0.0, 0.011111111, 0.0055555557, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.016666668, 0.0055555557, 0.0055555557, 0.0055555557, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.033333335, 0.0, 0.0, 0.0, 0.016666668, 0.0055555557, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.011111111, 0.0, 0.0055555557, 0.011111111, 0.0055555557, 0.0055555557, 0.0055555557, 0.0055555557, 0.0055555557, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.0055555557, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0055555557, 0.0, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.016666668, 0.0055555557, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.022222223, 0.0055555557, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.011111111, 0.0055555557, 0.0, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.0055555557, 0.0055555557, 0.0, 0.011111111, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.011111111, 0.0055555557, 0.0, 0.0, 0.0, 0.0055555557, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.011111111, 0.0055555557, 0.011111111, 0.0, 0.0055555557, 0.0, 0.0, 0.0055555557, 0.0055555557, 0.0, 0.011111111, 0.011111111, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0055555557, 0.0055555557, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0055555557 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.005725191, 0.0, 0.001908397, 0.0, 0.0, 0.001908397, 0.0, 0.0, 0.0, 0.030534351, 0.0, 0.001908397, 0.0, 0.0, 0.001908397, 0.0, 0.003816794, 0.0, 0.0, 0.0, 0.0, 0.011450382, 0.0, 0.001908397, 0.0, 0.01908397, 0.007633588, 0.001908397, 0.034351144, 0.003816794, 0.026717557, 0.0, 0.007633588, 0.0, 0.001908397, 0.011450382, 0.003816794, 0.0, 0.001908397, 0.001908397, 0.0, 0.024809161, 0.0, 0.001908397, 0.003816794, 0.001908397, 0.0, 0.001908397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001908397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01908397, 0.005725191, 0.0, 0.0, 0.0, 0.003816794, 0.001908397, 0.001908397, 0.0, 0.0, 0.0, 0.003816794, 0.0, 0.001908397, 0.003816794, 0.0, 0.0, 0.003816794, 0.0, 0.036259543, 0.007633588, 0.001908397, 0.003816794, 0.005725191, 0.022900764, 0.003816794, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001908397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003816794, 0.0, 0.003816794, 0.0, 0.001908397, 0.0, 0.0, 0.0, 0.001908397, 0.003816794, 0.001908397, 0.0, 0.0, 0.0, 0.003816794, 0.0, 0.007633588, 0.0, 0.0, 0.003816794, 0.0, 0.005725191, 0.0, 0.017175572, 0.005725191, 0.007633588, 0.0, 0.005725191, 0.001908397, 0.0, 0.001908397, 0.0, 0.0, 0.0, 0.001908397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001908397, 0.001908397, 0.0, 0.005725191, 0.001908397, 0.0, 0.001908397, 0.001908397, 0.0, 0.005725191, 0.0, 0.003816794, 0.0, 0.001908397, 0.001908397, 0.001908397, 0.001908397, 0.0, 0.001908397, 0.0, 0.0, 0.001908397, 0.0, 0.015267176, 0.0, 0.0, 0.0, 0.0, 0.005725191, 0.001908397, 0.0, 0.001908397, 0.001908397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005725191, 0.001908397, 0.0, 0.001908397, 0.0, 0.005725191, 0.0, 0.0, 0.009541985, 0.0, 0.0, 0.001908397, 0.001908397, 0.0, 0.0, 0.0, 0.0, 0.001908397, 0.0, 0.0, 0.001908397, 0.0, 0.007633588, 0.0, 0.001908397, 0.0, 0.0, 0.0, 0.005725191, 0.003816794, 0.0, 0.0, 0.0, 0.0, 0.005725191, 0.0, 0.003816794, 0.007633588, 0.0, 0.001908397, 0.001908397, 0.0, 0.0, 0.0, 0.005725191, 0.0, 0.001908397, 0.0, 0.001908397, 0.001908397, 0.007633588, 0.0, 0.011450382, 0.001908397, 0.007633588, 0.024809161, 0.001908397, 0.003816794, 0.0, 0.0, 0.0, 0.0, 0.001908397, 0.0, 0.0, 0.003816794, 0.001908397, 0.001908397, 0.0, 0.001908397, 0.0, 0.003816794, 0.011450382, 0.0, 0.001908397, 0.0, 0.003816794, 0.003816794, 0.0, 0.0, 0.001908397, 0.0, 0.0, 0.005725191, 0.0, 0.0, 0.001908397, 0.003816794, 0.0, 0.001908397, 0.0, 0.009541985, 0.0, 0.0, 0.0, 0.0, 0.001908397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001908397, 0.001908397, 0.0, 0.003816794, 0.0, 0.0, 0.001908397, 0.0, 0.0, 0.0, 0.0, 0.001908397, 0.003816794, 0.0, 0.001908397, 0.0, 0.001908397, 0.03244275, 0.001908397, 0.001908397, 0.001908397, 0.005725191, 0.0, 0.001908397, 0.007633588, 0.0, 0.001908397, 0.001908397, 0.0, 0.001908397, 0.0, 0.0, 0.0, 0.0, 0.015267176, 0.001908397, 0.0, 0.0, 0.0, 0.001908397, 0.013358778, 0.0, 0.001908397, 0.013358778, 0.0, 0.0, 0.0, 0.001908397, 0.020992367, 0.001908397, 0.0, 0.0, 0.0, 0.003816794, 0.0, 0.007633588, 0.003816794, 0.001908397, 0.001908397, 0.003816794, 0.0, 0.0, 0.001908397, 0.011450382, 0.009541985, 0.001908397, 0.0, 0.0, 0.001908397, 0.003816794, 0.0, 0.0, 0.0, 0.0, 0.001908397, 0.0, 0.003816794, 0.001908397, 0.001908397, 0.001908397, 0.005725191, 0.0, 0.003816794, 0.0, 0.0, 0.0, 0.001908397, 0.0, 0.0, 0.0, 0.005725191, 0.003816794, 0.0, 0.015267176, 0.001908397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003816794, 0.003816794, 0.003816794, 0.0, 0.0, 0.007633588, 0.001908397, 0.0, 0.0, 0.0, 0.007633588, 0.0, 0.003816794, 0.001908397, 0.003816794, 0.007633588, 0.001908397 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.013513514, 0.040540542, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0945946, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06756757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.054054055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.040540542, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.027027028, 0.0, 0.013513514, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.007712082, 0.0, 0.007712082, 0.005141388, 0.002570694, 0.0, 0.005141388, 0.007712082, 0.005141388, 0.005141388, 0.002570694, 0.005141388, 0.002570694, 0.0, 0.002570694, 0.007712082, 0.005141388, 0.0, 0.005141388, 0.002570694, 0.005141388, 0.0, 0.0, 0.0, 0.005141388, 0.0, 0.002570694, 0.0, 0.005141388, 0.0, 0.0, 0.002570694, 0.0, 0.0, 0.005141388, 0.002570694, 0.007712082, 0.0, 0.0, 0.002570694, 0.0, 0.0, 0.0, 0.0, 0.005141388, 0.005141388, 0.002570694, 0.0, 0.002570694, 0.005141388, 0.0, 0.002570694, 0.002570694, 0.0, 0.0, 0.002570694, 0.002570694, 0.005141388, 0.007712082, 0.0, 0.007712082, 0.002570694, 0.007712082, 0.005141388, 0.0, 0.0, 0.005141388, 0.002570694, 0.002570694, 0.0, 0.002570694, 0.005141388, 0.0, 0.0, 0.0, 0.002570694, 0.0, 0.012853471, 0.005141388, 0.0, 0.002570694, 0.002570694, 0.002570694, 0.002570694, 0.005141388, 0.0, 0.010282776, 0.002570694, 0.0, 0.010282776, 0.0, 0.002570694, 0.005141388, 0.002570694, 0.0, 0.0, 0.002570694, 0.005141388, 0.0, 0.0, 0.0, 0.002570694, 0.002570694, 0.002570694, 0.0, 0.0, 0.002570694, 0.002570694, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002570694, 0.005141388, 0.0, 0.002570694, 0.002570694, 0.0, 0.0, 0.0, 0.012853471, 0.0, 0.007712082, 0.005141388, 0.0, 0.002570694, 0.0, 0.0, 0.002570694, 0.002570694, 0.002570694, 0.0, 0.0, 0.0, 0.005141388, 0.002570694, 0.0, 0.0, 0.002570694, 0.0, 0.0, 0.0, 0.002570694, 0.002570694, 0.0, 0.007712082, 0.0, 0.002570694, 0.0, 0.0, 0.002570694, 0.002570694, 0.002570694, 0.005141388, 0.002570694, 0.010282776, 0.0, 0.0, 0.002570694, 0.002570694, 0.007712082, 0.007712082, 0.005141388, 0.0, 0.002570694, 0.002570694, 0.005141388, 0.0, 0.002570694, 0.0, 0.0, 0.0, 0.002570694, 0.0, 0.0, 0.0, 0.002570694, 0.005141388, 0.0, 0.0, 0.0, 0.005141388, 0.002570694, 0.002570694, 0.0, 0.0, 0.010282776, 0.005141388, 0.0, 0.002570694, 0.0, 0.0, 0.0, 0.002570694, 0.005141388, 0.0, 0.002570694, 0.0, 0.0, 0.0, 0.005141388, 0.005141388, 0.012853471, 0.0, 0.002570694, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002570694, 0.0, 0.0, 0.002570694, 0.002570694, 0.007712082, 0.002570694, 0.002570694, 0.0, 0.002570694, 0.005141388, 0.0, 0.007712082, 0.0, 0.0, 0.0, 0.0, 0.002570694, 0.007712082, 0.0, 0.0, 0.0, 0.007712082, 0.0, 0.0, 0.007712082, 0.002570694, 0.0, 0.0, 0.0, 0.0, 0.002570694, 0.0, 0.002570694, 0.0, 0.0, 0.0, 0.007712082, 0.002570694, 0.0, 0.0, 0.002570694, 0.002570694, 0.010282776, 0.0, 0.012853471, 0.005141388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002570694, 0.0, 0.0, 0.002570694, 0.0, 0.002570694, 0.0, 0.0, 0.005141388, 0.002570694, 0.002570694, 0.010282776, 0.002570694, 0.0, 0.010282776, 0.0, 0.012853471, 0.0, 0.002570694, 0.0, 0.0, 0.0, 0.005141388, 0.010282776, 0.0, 0.002570694, 0.0, 0.012853471, 0.0, 0.0, 0.0, 0.002570694, 0.005141388, 0.0, 0.010282776, 0.012853471, 0.002570694, 0.002570694, 0.0, 0.005141388, 0.005141388, 0.002570694, 0.002570694, 0.0, 0.0, 0.005141388, 0.005141388, 0.005141388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002570694, 0.0, 0.002570694, 0.0, 0.002570694, 0.002570694, 0.0, 0.002570694, 0.0, 0.002570694, 0.002570694, 0.0, 0.0, 0.012853471, 0.002570694, 0.0, 0.0, 0.0, 0.0, 0.002570694, 0.0, 0.0, 0.010282776, 0.0, 0.0, 0.012853471, 0.002570694, 0.005141388, 0.005141388, 0.002570694, 0.0, 0.015424164, 0.0, 0.0, 0.002570694, 0.0, 0.002570694, 0.005141388, 0.002570694, 0.005141388, 0.002570694, 0.0, 0.002570694, 0.0, 0.002570694, 0.0, 0.002570694, 0.002570694, 0.007712082, 0.005141388, 0.002570694, 0.002570694, 0.002570694, 0.005141388, 0.005141388, 0.007712082, 0.010282776, 0.0, 0.002570694, 0.0, 0.002570694, 0.0, 0.0, 0.002570694, 0.002570694, 0.005141388, 0.0, 0.0, 0.0, 0.002570694, 0.0, 0.0, 0.0, 0.010282776, 0.002570694, 0.0, 0.005141388, 0.0, 0.0, 0.0, 0.0, 0.007712082, 0.012853471 ]
400 5
-- 1
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.011811024, 0.003937008, 0.001968504, 0.003937008, 0.0, 0.0, 0.0, 0.001968504, 0.0, 0.0, 0.0, 0.0, 0.001968504, 0.001968504, 0.001968504, 0.003937008, 0.001968504, 0.0, 0.0, 0.001968504, 0.007874016, 0.0, 0.005905512, 0.003937008, 0.001968504, 0.0, 0.001968504, 0.001968504, 0.001968504, 0.015748031, 0.001968504, 0.003937008, 0.001968504, 0.0137795275, 0.003937008, 0.011811024, 0.00984252, 0.003937008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001968504, 0.001968504, 0.003937008, 0.003937008, 0.005905512, 0.003937008, 0.003937008, 0.005905512, 0.001968504, 0.001968504, 0.007874016, 0.0, 0.0, 0.005905512, 0.0, 0.0, 0.0, 0.0137795275, 0.005905512, 0.003937008, 0.001968504, 0.001968504, 0.001968504, 0.001968504, 0.007874016, 0.001968504, 0.005905512, 0.003937008, 0.0, 0.001968504, 0.0, 0.0, 0.003937008, 0.0, 0.001968504, 0.003937008, 0.00984252, 0.0, 0.003937008, 0.0, 0.0, 0.001968504, 0.001968504, 0.0, 0.007874016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001968504, 0.0, 0.003937008, 0.001968504, 0.0, 0.0, 0.003937008, 0.003937008, 0.0, 0.003937008, 0.0, 0.011811024, 0.0, 0.0, 0.0, 0.003937008, 0.0, 0.0, 0.007874016, 0.0, 0.005905512, 0.001968504, 0.0, 0.003937008, 0.0, 0.0, 0.005905512, 0.003937008, 0.0, 0.0, 0.001968504, 0.007874016, 0.0, 0.003937008, 0.001968504, 0.003937008, 0.003937008, 0.003937008, 0.0, 0.0, 0.001968504, 0.007874016, 0.0, 0.0, 0.0, 0.00984252, 0.0, 0.0, 0.0, 0.001968504, 0.001968504, 0.0, 0.001968504, 0.0, 0.001968504, 0.0, 0.001968504, 0.0, 0.003937008, 0.003937008, 0.001968504, 0.003937008, 0.001968504, 0.003937008, 0.001968504, 0.001968504, 0.0, 0.007874016, 0.00984252, 0.003937008, 0.0, 0.0, 0.0, 0.003937008, 0.0, 0.0, 0.003937008, 0.0, 0.001968504, 0.011811024, 0.001968504, 0.001968504, 0.0, 0.005905512, 0.003937008, 0.001968504, 0.007874016, 0.005905512, 0.003937008, 0.0, 0.001968504, 0.0, 0.001968504, 0.0, 0.003937008, 0.001968504, 0.001968504, 0.001968504, 0.001968504, 0.001968504, 0.0, 0.001968504, 0.0, 0.003937008, 0.001968504, 0.0, 0.003937008, 0.0, 0.0, 0.0, 0.003937008, 0.005905512, 0.003937008, 0.015748031, 0.001968504, 0.003937008, 0.001968504, 0.005905512, 0.003937008, 0.0, 0.003937008, 0.0, 0.001968504, 0.001968504, 0.0, 0.0, 0.0, 0.003937008, 0.003937008, 0.007874016, 0.003937008, 0.0, 0.001968504, 0.0, 0.003937008, 0.001968504, 0.005905512, 0.005905512, 0.0, 0.00984252, 0.001968504, 0.001968504, 0.001968504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001968504, 0.001968504, 0.0, 0.0, 0.0, 0.005905512, 0.003937008, 0.003937008, 0.0, 0.005905512, 0.005905512, 0.011811024, 0.001968504, 0.001968504, 0.003937008, 0.003937008, 0.003937008, 0.0, 0.0, 0.0, 0.005905512, 0.0, 0.0, 0.001968504, 0.007874016, 0.001968504, 0.0, 0.0, 0.0, 0.001968504, 0.003937008, 0.001968504, 0.003937008, 0.001968504, 0.0, 0.0, 0.003937008, 0.003937008, 0.0, 0.0, 0.003937008, 0.0, 0.0, 0.007874016, 0.001968504, 0.0, 0.001968504, 0.003937008, 0.001968504, 0.0, 0.003937008, 0.0, 0.0, 0.003937008, 0.0, 0.023622047, 0.0, 0.003937008, 0.0, 0.005905512, 0.001968504, 0.005905512, 0.001968504, 0.001968504, 0.0, 0.0, 0.005905512, 0.0, 0.0, 0.003937008, 0.0, 0.0, 0.003937008, 0.0, 0.001968504, 0.001968504, 0.003937008, 0.0, 0.0, 0.005905512, 0.0, 0.003937008, 0.00984252, 0.001968504, 0.0, 0.0, 0.0, 0.003937008, 0.0, 0.003937008, 0.003937008, 0.0, 0.005905512, 0.0, 0.0, 0.0, 0.005905512, 0.001968504, 0.003937008, 0.001968504, 0.001968504, 0.003937008, 0.005905512, 0.0, 0.001968504, 0.0, 0.001968504, 0.001968504, 0.003937008, 0.003937008, 0.001968504, 0.001968504, 0.001968504, 0.007874016, 0.0, 0.0, 0.011811024, 0.0, 0.005905512, 0.0, 0.0, 0.003937008, 0.0, 0.0, 0.003937008, 0.005905512, 0.001968504, 0.0, 0.0, 0.00984252, 0.0, 0.001968504, 0.0, 0.001968504, 0.001968504, 0.0, 0.0, 0.001968504, 0.0, 0.001968504, 0.0, 0.0, 0.007874016, 0.005905512, 0.001968504, 0.001968504, 0.003937008, 0.0, 0.001968504, 0.0, 0.0, 0.001968504, 0.0, 0.0, 0.001968504, 0.003937008, 0.0 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.010126582, 0.0025316456, 0.0, 0.0075949365, 0.0025316456, 0.005063291, 0.005063291, 0.0025316456, 0.010126582, 0.005063291, 0.0, 0.0, 0.0, 0.010126582, 0.0, 0.0025316456, 0.0025316456, 0.0, 0.0025316456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005063291, 0.0, 0.0075949365, 0.0, 0.0025316456, 0.0, 0.0, 0.0, 0.0, 0.010126582, 0.0025316456, 0.0, 0.0, 0.0025316456, 0.0, 0.0075949365, 0.005063291, 0.0025316456, 0.0, 0.015189873, 0.0075949365, 0.0, 0.0, 0.015189873, 0.0, 0.0025316456, 0.0, 0.005063291, 0.0, 0.0, 0.017721519, 0.0, 0.0, 0.005063291, 0.005063291, 0.005063291, 0.0, 0.0, 0.017721519, 0.005063291, 0.0025316456, 0.005063291, 0.0025316456, 0.0025316456, 0.0, 0.0, 0.0025316456, 0.0075949365, 0.0025316456, 0.012658228, 0.0, 0.0, 0.0, 0.0, 0.005063291, 0.0, 0.0, 0.0025316456, 0.0, 0.0075949365, 0.0075949365, 0.005063291, 0.0025316456, 0.0025316456, 0.0025316456, 0.0, 0.0025316456, 0.010126582, 0.0, 0.0075949365, 0.0, 0.0025316456, 0.0, 0.0, 0.0025316456, 0.0025316456, 0.0, 0.0, 0.005063291, 0.0025316456, 0.0, 0.0075949365, 0.0, 0.0, 0.0, 0.0025316456, 0.0025316456, 0.0025316456, 0.005063291, 0.0, 0.0, 0.0, 0.0, 0.0025316456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0025316456, 0.0, 0.0, 0.0025316456, 0.0, 0.0, 0.0, 0.0025316456, 0.0, 0.0025316456, 0.0, 0.0025316456, 0.0025316456, 0.010126582, 0.0, 0.0025316456, 0.010126582, 0.0, 0.0, 0.0075949365, 0.0025316456, 0.0025316456, 0.0075949365, 0.0, 0.0025316456, 0.0025316456, 0.0, 0.0025316456, 0.0, 0.005063291, 0.0, 0.0075949365, 0.012658228, 0.0, 0.005063291, 0.0, 0.0025316456, 0.0025316456, 0.0025316456, 0.0, 0.0025316456, 0.0, 0.0025316456, 0.0025316456, 0.0, 0.0, 0.005063291, 0.0, 0.005063291, 0.0025316456, 0.0025316456, 0.010126582, 0.0, 0.0025316456, 0.0, 0.0, 0.010126582, 0.0, 0.0075949365, 0.005063291, 0.0, 0.0, 0.0025316456, 0.005063291, 0.0, 0.0025316456, 0.0, 0.0, 0.010126582, 0.0075949365, 0.0, 0.0, 0.0, 0.0, 0.020253165, 0.0025316456, 0.0025316456, 0.0, 0.0025316456, 0.0025316456, 0.010126582, 0.0025316456, 0.0025316456, 0.0, 0.0, 0.0, 0.010126582, 0.0, 0.0, 0.0025316456, 0.0025316456, 0.0075949365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005063291, 0.0025316456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0025316456, 0.005063291, 0.0025316456, 0.0075949365, 0.0, 0.0, 0.0, 0.005063291, 0.0, 0.0025316456, 0.0, 0.010126582, 0.0, 0.0025316456, 0.005063291, 0.0, 0.0, 0.0, 0.005063291, 0.0025316456, 0.005063291, 0.0, 0.0, 0.005063291, 0.0, 0.0025316456, 0.0, 0.0, 0.0, 0.0025316456, 0.005063291, 0.0, 0.0025316456, 0.0, 0.0025316456, 0.0, 0.005063291, 0.0, 0.0025316456, 0.0075949365, 0.0, 0.0, 0.0, 0.0025316456, 0.012658228, 0.0, 0.0025316456, 0.005063291, 0.0, 0.0025316456, 0.012658228, 0.0025316456, 0.0075949365, 0.0, 0.0, 0.0, 0.0, 0.0025316456, 0.0, 0.017721519, 0.005063291, 0.0025316456, 0.012658228, 0.0, 0.0025316456, 0.0025316456, 0.005063291, 0.0025316456, 0.010126582, 0.0, 0.0, 0.0, 0.0025316456, 0.0025316456, 0.005063291, 0.005063291, 0.005063291, 0.0, 0.0075949365, 0.0, 0.0, 0.0025316456, 0.0025316456, 0.0025316456, 0.0025316456, 0.0025316456, 0.0, 0.0, 0.005063291, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0025316456, 0.005063291, 0.0, 0.0, 0.0, 0.0, 0.0025316456, 0.0, 0.0025316456, 0.0, 0.012658228, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0025316456, 0.005063291, 0.0025316456, 0.005063291, 0.0025316456, 0.0075949365, 0.0025316456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0025316456, 0.0, 0.0075949365, 0.0025316456, 0.0, 0.0075949365, 0.0, 0.005063291, 0.005063291, 0.0025316456, 0.005063291, 0.0, 0.0075949365, 0.0025316456, 0.005063291, 0.0, 0.0025316456, 0.0025316456, 0.0, 0.0075949365, 0.0, 0.0, 0.0, 0.0025316456, 0.0, 0.0025316456, 0.0025316456, 0.0025316456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0025316456, 0.0075949365 ]
400 5
-- 1
400 5
Histogram size : (400, 1)
Histogram : [ 0.0025445293, 0.0025445293, 0.0025445293, 0.0050890585, 0.0, 0.0, 0.0025445293, 0.0025445293, 0.0050890585, 0.0050890585, 0.0, 0.0, 0.0, 0.0, 0.0025445293, 0.0025445293, 0.0, 0.010178117, 0.0, 0.0050890585, 0.0025445293, 0.0025445293, 0.0, 0.0025445293, 0.0, 0.0, 0.0, 0.0, 0.010178117, 0.0, 0.0025445293, 0.012722647, 0.0, 0.007633588, 0.0025445293, 0.007633588, 0.0025445293, 0.0025445293, 0.0050890585, 0.0, 0.0, 0.0025445293, 0.0, 0.0050890585, 0.007633588, 0.0025445293, 0.0, 0.0025445293, 0.0050890585, 0.0, 0.0, 0.0, 0.0, 0.0050890585, 0.0, 0.0025445293, 0.0025445293, 0.0, 0.007633588, 0.0050890585, 0.0025445293, 0.0, 0.0050890585, 0.0025445293, 0.0050890585, 0.0025445293, 0.0050890585, 0.0025445293, 0.0, 0.0025445293, 0.0050890585, 0.0025445293, 0.0, 0.0, 0.0050890585, 0.0050890585, 0.0, 0.0025445293, 0.0, 0.0025445293, 0.0025445293, 0.0, 0.0050890585, 0.010178117, 0.0025445293, 0.007633588, 0.0, 0.0025445293, 0.0, 0.0050890585, 0.0, 0.0050890585, 0.0025445293, 0.0025445293, 0.0025445293, 0.0025445293, 0.0, 0.0025445293, 0.0, 0.0025445293, 0.0, 0.0, 0.0, 0.0025445293, 0.0, 0.0, 0.0, 0.0025445293, 0.0, 0.0, 0.0, 0.0, 0.0025445293, 0.0050890585, 0.0, 0.0, 0.0025445293, 0.0, 0.0, 0.0050890585, 0.0, 0.0, 0.0025445293, 0.0, 0.0, 0.007633588, 0.0025445293, 0.0, 0.0, 0.0, 0.0, 0.0025445293, 0.0050890585, 0.0025445293, 0.0025445293, 0.0, 0.007633588, 0.0, 0.0, 0.0, 0.007633588, 0.0, 0.0050890585, 0.0, 0.0025445293, 0.007633588, 0.0, 0.0050890585, 0.0050890585, 0.0025445293, 0.0, 0.0, 0.0, 0.0025445293, 0.0050890585, 0.0025445293, 0.0050890585, 0.0050890585, 0.0, 0.007633588, 0.0, 0.0, 0.0, 0.012722647, 0.0050890585, 0.0025445293, 0.0, 0.0, 0.0025445293, 0.0, 0.0, 0.0, 0.0025445293, 0.0050890585, 0.0050890585, 0.0025445293, 0.0, 0.0050890585, 0.0, 0.0025445293, 0.0025445293, 0.007633588, 0.0025445293, 0.0025445293, 0.0050890585, 0.0025445293, 0.0, 0.0025445293, 0.007633588, 0.0050890585, 0.0, 0.0, 0.0, 0.0025445293, 0.0025445293, 0.0, 0.0025445293, 0.007633588, 0.0, 0.0, 0.0, 0.0025445293, 0.0025445293, 0.0, 0.0025445293, 0.0025445293, 0.0025445293, 0.0050890585, 0.025445294, 0.0025445293, 0.0050890585, 0.0, 0.010178117, 0.0025445293, 0.0025445293, 0.0, 0.0, 0.007633588, 0.0025445293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0025445293, 0.0025445293, 0.0, 0.0, 0.007633588, 0.010178117, 0.010178117, 0.0, 0.0, 0.0, 0.0025445293, 0.0, 0.0, 0.0050890585, 0.0050890585, 0.0, 0.0050890585, 0.0, 0.0, 0.0050890585, 0.0, 0.0025445293, 0.0, 0.0050890585, 0.0, 0.007633588, 0.0050890585, 0.0, 0.0050890585, 0.0, 0.0025445293, 0.0025445293, 0.0025445293, 0.0025445293, 0.0025445293, 0.007633588, 0.0, 0.0, 0.0, 0.007633588, 0.0, 0.0050890585, 0.0, 0.0, 0.0025445293, 0.0025445293, 0.0, 0.0, 0.0, 0.0, 0.0050890585, 0.007633588, 0.0, 0.0, 0.0025445293, 0.0, 0.0050890585, 0.0, 0.0, 0.0025445293, 0.0, 0.0025445293, 0.0050890585, 0.010178117, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0025445293, 0.0025445293, 0.0, 0.0, 0.0, 0.0025445293, 0.0050890585, 0.0, 0.0025445293, 0.0050890585, 0.0025445293, 0.0025445293, 0.0, 0.0, 0.0025445293, 0.0, 0.0050890585, 0.0025445293, 0.0050890585, 0.012722647, 0.0025445293, 0.007633588, 0.0, 0.0025445293, 0.0050890585, 0.0025445293, 0.0, 0.0025445293, 0.0, 0.0050890585, 0.0, 0.0, 0.007633588, 0.0, 0.0, 0.0, 0.0050890585, 0.010178117, 0.0, 0.0050890585, 0.0, 0.0050890585, 0.0, 0.0, 0.0050890585, 0.0, 0.0025445293, 0.007633588, 0.0, 0.0, 0.0025445293, 0.010178117, 0.0, 0.0050890585, 0.0, 0.0, 0.0050890585, 0.0, 0.0, 0.0025445293, 0.0, 0.0050890585, 0.0, 0.0025445293, 0.0, 0.0, 0.0025445293, 0.0, 0.0, 0.0025445293, 0.0025445293, 0.012722647, 0.0025445293, 0.007633588, 0.0, 0.0050890585, 0.0050890585, 0.0, 0.0025445293, 0.0, 0.0025445293, 0.0050890585, 0.0050890585, 0.0025445293, 0.0, 0.0025445293, 0.0050890585, 0.0050890585, 0.0050890585, 0.0025445293, 0.0, 0.0, 0.0025445293, 0.007633588, 0.0, 0.0050890585, 0.0050890585, 0.0050890585, 0.0, 0.0, 0.010178117, 0.0050890585, 0.0, 0.0, 0.0, 0.0, 0.0 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.0020661156, 0.0, 0.0, 0.008264462, 0.0020661156, 0.0061983466, 0.0, 0.0020661156, 0.0061983466, 0.0020661156, 0.0061983466, 0.0020661156, 0.0, 0.0020661156, 0.004132231, 0.004132231, 0.0, 0.0020661156, 0.0020661156, 0.0061983466, 0.0, 0.0020661156, 0.0, 0.0020661156, 0.0, 0.0020661156, 0.0020661156, 0.0, 0.0, 0.0020661156, 0.004132231, 0.0020661156, 0.0020661156, 0.0061983466, 0.0020661156, 0.0, 0.004132231, 0.0020661156, 0.0020661156, 0.0, 0.0, 0.004132231, 0.008264462, 0.0, 0.0020661156, 0.008264462, 0.0, 0.004132231, 0.004132231, 0.0, 0.0, 0.008264462, 0.0020661156, 0.0, 0.0020661156, 0.0, 0.004132231, 0.0061983466, 0.0061983466, 0.0, 0.008264462, 0.0020661156, 0.0, 0.0, 0.004132231, 0.0, 0.0, 0.0, 0.0020661156, 0.0061983466, 0.0061983466, 0.004132231, 0.0020661156, 0.0020661156, 0.004132231, 0.0, 0.0, 0.0020661156, 0.0020661156, 0.0061983466, 0.0, 0.004132231, 0.008264462, 0.004132231, 0.0020661156, 0.004132231, 0.0061983466, 0.0020661156, 0.0020661156, 0.0, 0.0, 0.0061983466, 0.010330578, 0.0, 0.0020661156, 0.0061983466, 0.0, 0.0020661156, 0.004132231, 0.0061983466, 0.0, 0.008264462, 0.004132231, 0.0020661156, 0.0, 0.0, 0.0, 0.0020661156, 0.0, 0.0, 0.0020661156, 0.0020661156, 0.0, 0.004132231, 0.0, 0.004132231, 0.0, 0.0, 0.004132231, 0.0061983466, 0.0020661156, 0.0020661156, 0.0020661156, 0.0, 0.0, 0.004132231, 0.004132231, 0.0, 0.0020661156, 0.0, 0.0061983466, 0.0, 0.0061983466, 0.0, 0.0020661156, 0.008264462, 0.0020661156, 0.0020661156, 0.0, 0.0020661156, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0020661156, 0.0, 0.004132231, 0.0, 0.0, 0.004132231, 0.0, 0.0061983466, 0.0061983466, 0.0, 0.004132231, 0.0020661156, 0.004132231, 0.0020661156, 0.0, 0.0, 0.0020661156, 0.0020661156, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0061983466, 0.0061983466, 0.0020661156, 0.004132231, 0.0061983466, 0.0, 0.0020661156, 0.004132231, 0.0, 0.0, 0.0, 0.0061983466, 0.0, 0.0061983466, 0.0, 0.0020661156, 0.0, 0.0020661156, 0.0, 0.004132231, 0.0, 0.004132231, 0.0020661156, 0.0, 0.0, 0.0, 0.0020661156, 0.004132231, 0.0020661156, 0.0020661156, 0.0020661156, 0.0061983466, 0.0020661156, 0.0020661156, 0.0, 0.0020661156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0061983466, 0.0020661156, 0.0, 0.004132231, 0.0061983466, 0.004132231, 0.0020661156, 0.0, 0.0020661156, 0.0, 0.0, 0.0061983466, 0.0020661156, 0.0020661156, 0.0061983466, 0.0, 0.0, 0.0061983466, 0.0020661156, 0.0061983466, 0.0, 0.0, 0.0, 0.0020661156, 0.004132231, 0.004132231, 0.0020661156, 0.0020661156, 0.0, 0.0020661156, 0.004132231, 0.0, 0.0020661156, 0.004132231, 0.0, 0.0020661156, 0.0, 0.0061983466, 0.0, 0.0020661156, 0.004132231, 0.0020661156, 0.0, 0.0, 0.0061983466, 0.0061983466, 0.0, 0.008264462, 0.0, 0.0061983466, 0.0, 0.0, 0.0, 0.0, 0.004132231, 0.004132231, 0.0020661156, 0.0, 0.0020661156, 0.0020661156, 0.0, 0.0020661156, 0.004132231, 0.0, 0.0020661156, 0.0, 0.008264462, 0.0, 0.0020661156, 0.0020661156, 0.0061983466, 0.004132231, 0.004132231, 0.004132231, 0.0, 0.0020661156, 0.012396693, 0.010330578, 0.004132231, 0.0020661156, 0.0, 0.008264462, 0.0, 0.0020661156, 0.0061983466, 0.0, 0.0020661156, 0.0, 0.0061983466, 0.0, 0.004132231, 0.0020661156, 0.004132231, 0.0020661156, 0.0061983466, 0.0, 0.004132231, 0.0020661156, 0.0, 0.0061983466, 0.010330578, 0.008264462, 0.0, 0.0, 0.008264462, 0.0020661156, 0.0, 0.004132231, 0.004132231, 0.0061983466, 0.0020661156, 0.0020661156, 0.0020661156, 0.0, 0.0, 0.0020661156, 0.0020661156, 0.0020661156, 0.0020661156, 0.0020661156, 0.0061983466, 0.0, 0.0020661156, 0.0020661156, 0.0, 0.004132231, 0.0, 0.0, 0.0, 0.0020661156, 0.0020661156, 0.0020661156, 0.0020661156, 0.0020661156, 0.0020661156, 0.0020661156, 0.0020661156, 0.0, 0.0061983466, 0.014462809, 0.0, 0.008264462, 0.014462809, 0.0, 0.008264462, 0.0, 0.0, 0.0020661156, 0.0020661156, 0.004132231, 0.0, 0.0, 0.0, 0.0, 0.0020661156, 0.0061983466, 0.014462809, 0.0020661156, 0.0, 0.0, 0.0020661156, 0.004132231, 0.0, 0.0, 0.004132231, 0.0, 0.0020661156, 0.004132231, 0.004132231, 0.0020661156, 0.004132231, 0.0, 0.0061983466, 0.0, 0.0, 0.004132231, 0.0020661156, 0.0, 0.0, 0.0, 0.0020661156, 0.0020661156, 0.0, 0.0020661156, 0.0, 0.0, 0.0, 0.0020661156, 0.0020661156, 0.008264462 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.001814882, 0.003629764, 0.001814882, 0.003629764, 0.001814882, 0.001814882, 0.005444646, 0.003629764, 0.005444646, 0.012704174, 0.001814882, 0.003629764, 0.0, 0.0, 0.00907441, 0.001814882, 0.0, 0.001814882, 0.003629764, 0.005444646, 0.001814882, 0.001814882, 0.003629764, 0.001814882, 0.0, 0.003629764, 0.0, 0.0, 0.003629764, 0.0, 0.00907441, 0.001814882, 0.003629764, 0.0, 0.001814882, 0.003629764, 0.007259528, 0.0, 0.0, 0.003629764, 0.0, 0.00907441, 0.012704174, 0.001814882, 0.0, 0.003629764, 0.0, 0.001814882, 0.0, 0.0, 0.001814882, 0.001814882, 0.005444646, 0.00907441, 0.005444646, 0.001814882, 0.001814882, 0.0, 0.0, 0.0, 0.007259528, 0.0, 0.007259528, 0.001814882, 0.001814882, 0.005444646, 0.005444646, 0.0, 0.001814882, 0.007259528, 0.001814882, 0.007259528, 0.0, 0.001814882, 0.0, 0.0, 0.0, 0.003629764, 0.005444646, 0.003629764, 0.003629764, 0.007259528, 0.003629764, 0.005444646, 0.00907441, 0.003629764, 0.003629764, 0.007259528, 0.0, 0.001814882, 0.0, 0.003629764, 0.001814882, 0.007259528, 0.001814882, 0.005444646, 0.0, 0.003629764, 0.003629764, 0.005444646, 0.0, 0.001814882, 0.0, 0.0, 0.001814882, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001814882, 0.003629764, 0.003629764, 0.001814882, 0.0, 0.0, 0.0, 0.003629764, 0.0, 0.007259528, 0.003629764, 0.001814882, 0.003629764, 0.0, 0.0, 0.005444646, 0.0, 0.005444646, 0.001814882, 0.0, 0.007259528, 0.0, 0.0, 0.0, 0.0, 0.001814882, 0.005444646, 0.0, 0.0, 0.001814882, 0.005444646, 0.001814882, 0.0, 0.003629764, 0.003629764, 0.003629764, 0.001814882, 0.0, 0.0, 0.001814882, 0.001814882, 0.0, 0.0, 0.007259528, 0.003629764, 0.003629764, 0.001814882, 0.0, 0.003629764, 0.0, 0.0, 0.001814882, 0.003629764, 0.001814882, 0.003629764, 0.005444646, 0.0, 0.0, 0.012704174, 0.003629764, 0.012704174, 0.0, 0.007259528, 0.0, 0.0, 0.00907441, 0.0, 0.0, 0.005444646, 0.001814882, 0.001814882, 0.0, 0.001814882, 0.0, 0.001814882, 0.0, 0.0, 0.0, 0.0, 0.014519056, 0.0, 0.0, 0.003629764, 0.0, 0.0, 0.003629764, 0.001814882, 0.0, 0.005444646, 0.0, 0.0, 0.003629764, 0.0, 0.003629764, 0.0, 0.0, 0.001814882, 0.001814882, 0.001814882, 0.0, 0.001814882, 0.0, 0.0, 0.001814882, 0.0, 0.010889292, 0.001814882, 0.0, 0.001814882, 0.003629764, 0.003629764, 0.0, 0.0, 0.003629764, 0.003629764, 0.003629764, 0.0, 0.010889292, 0.0, 0.005444646, 0.0, 0.007259528, 0.0, 0.005444646, 0.005444646, 0.0, 0.003629764, 0.001814882, 0.005444646, 0.005444646, 0.0, 0.001814882, 0.0, 0.003629764, 0.0, 0.0, 0.001814882, 0.005444646, 0.005444646, 0.001814882, 0.001814882, 0.0, 0.007259528, 0.001814882, 0.003629764, 0.007259528, 0.0, 0.0, 0.003629764, 0.003629764, 0.007259528, 0.001814882, 0.0, 0.001814882, 0.0, 0.0, 0.005444646, 0.007259528, 0.003629764, 0.003629764, 0.0, 0.0, 0.003629764, 0.0, 0.0, 0.003629764, 0.012704174, 0.0, 0.001814882, 0.003629764, 0.001814882, 0.0, 0.005444646, 0.005444646, 0.0, 0.0, 0.001814882, 0.003629764, 0.001814882, 0.0, 0.0, 0.001814882, 0.0, 0.007259528, 0.003629764, 0.007259528, 0.005444646, 0.001814882, 0.001814882, 0.0, 0.003629764, 0.001814882, 0.005444646, 0.003629764, 0.001814882, 0.005444646, 0.003629764, 0.0, 0.0, 0.001814882, 0.005444646, 0.001814882, 0.005444646, 0.0, 0.007259528, 0.0, 0.0, 0.0, 0.0, 0.003629764, 0.0, 0.0, 0.0, 0.0, 0.003629764, 0.0, 0.0, 0.003629764, 0.0, 0.001814882, 0.003629764, 0.0, 0.007259528, 0.001814882, 0.001814882, 0.003629764, 0.0, 0.0, 0.0, 0.005444646, 0.003629764, 0.001814882, 0.003629764, 0.0, 0.005444646, 0.0, 0.0, 0.001814882, 0.0, 0.001814882, 0.0, 0.003629764, 0.005444646, 0.001814882, 0.005444646, 0.001814882, 0.005444646, 0.001814882, 0.005444646, 0.003629764, 0.0, 0.003629764, 0.0, 0.001814882, 0.001814882, 0.005444646, 0.005444646, 0.001814882, 0.0, 0.001814882, 0.005444646, 0.003629764, 0.005444646, 0.0, 0.0, 0.0, 0.003629764, 0.0, 0.0, 0.001814882, 0.001814882, 0.001814882, 0.003629764, 0.0, 0.001814882, 0.001814882, 0.0, 0.007259528, 0.001814882, 0.001814882, 0.001814882, 0.0, 0.003629764, 0.001814882, 0.003629764, 0.0, 0.003629764, 0.0, 0.005444646, 0.003629764 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0033898305, 0.0016949152, 0.0, 0.0016949152, 0.006779661, 0.0016949152, 0.0050847456, 0.016949153, 0.0016949152, 0.0033898305, 0.0033898305, 0.0, 0.0, 0.0033898305, 0.008474576, 0.0016949152, 0.0, 0.0, 0.0050847456, 0.0050847456, 0.010169491, 0.0, 0.0016949152, 0.0033898305, 0.0050847456, 0.0016949152, 0.0, 0.0, 0.0, 0.0016949152, 0.0, 0.0, 0.0016949152, 0.0016949152, 0.0, 0.0016949152, 0.006779661, 0.0016949152, 0.0050847456, 0.0, 0.0033898305, 0.010169491, 0.0, 0.0, 0.0033898305, 0.0, 0.0, 0.0050847456, 0.0033898305, 0.0, 0.0033898305, 0.0, 0.0, 0.0033898305, 0.0016949152, 0.0050847456, 0.010169491, 0.0016949152, 0.0016949152, 0.006779661, 0.0, 0.013559322, 0.0016949152, 0.0016949152, 0.0050847456, 0.0, 0.0, 0.0016949152, 0.0, 0.0016949152, 0.0016949152, 0.0, 0.0, 0.013559322, 0.0, 0.0, 0.0, 0.0033898305, 0.0, 0.0, 0.006779661, 0.0033898305, 0.0, 0.0, 0.0016949152, 0.0033898305, 0.0, 0.0016949152, 0.0, 0.0, 0.023728814, 0.011864407, 0.0, 0.0, 0.0016949152, 0.0016949152, 0.0, 0.0, 0.006779661, 0.0, 0.013559322, 0.0033898305, 0.0016949152, 0.0016949152, 0.0016949152, 0.0, 0.0033898305, 0.0016949152, 0.0, 0.0016949152, 0.0033898305, 0.0016949152, 0.0050847456, 0.0, 0.0, 0.0016949152, 0.0033898305, 0.0016949152, 0.0033898305, 0.0033898305, 0.0, 0.0016949152, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0033898305, 0.0033898305, 0.0, 0.0, 0.0, 0.0050847456, 0.0016949152, 0.0, 0.0, 0.0033898305, 0.0, 0.0, 0.0050847456, 0.0016949152, 0.008474576, 0.018644068, 0.0, 0.0, 0.0, 0.0, 0.0016949152, 0.0016949152, 0.011864407, 0.0033898305, 0.0016949152, 0.0033898305, 0.0016949152, 0.0, 0.0033898305, 0.0033898305, 0.0, 0.0016949152, 0.0016949152, 0.0, 0.0050847456, 0.0, 0.0016949152, 0.0, 0.0, 0.0, 0.0016949152, 0.0016949152, 0.020338982, 0.0016949152, 0.013559322, 0.0, 0.0050847456, 0.0016949152, 0.011864407, 0.0, 0.0, 0.0016949152, 0.0, 0.0033898305, 0.0033898305, 0.006779661, 0.0, 0.0, 0.0033898305, 0.008474576, 0.0033898305, 0.0016949152, 0.0, 0.0016949152, 0.0, 0.0, 0.008474576, 0.0033898305, 0.013559322, 0.0, 0.0016949152, 0.0033898305, 0.0, 0.0033898305, 0.0, 0.0, 0.0033898305, 0.0016949152, 0.0016949152, 0.0, 0.0, 0.0016949152, 0.0, 0.0016949152, 0.0, 0.0050847456, 0.0050847456, 0.0, 0.0033898305, 0.0, 0.0033898305, 0.0016949152, 0.0033898305, 0.0, 0.0016949152, 0.0033898305, 0.0016949152, 0.0033898305, 0.0, 0.008474576, 0.0016949152, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015254237, 0.0033898305, 0.0050847456, 0.006779661, 0.006779661, 0.0016949152, 0.0016949152, 0.0050847456, 0.0, 0.0, 0.0, 0.0016949152, 0.0016949152, 0.0016949152, 0.0, 0.0, 0.0, 0.0, 0.013559322, 0.0016949152, 0.0, 0.0016949152, 0.0016949152, 0.0, 0.0050847456, 0.0, 0.0, 0.0, 0.0, 0.0016949152, 0.0050847456, 0.0016949152, 0.0016949152, 0.0, 0.0, 0.0016949152, 0.0033898305, 0.0, 0.0, 0.0016949152, 0.0016949152, 0.0, 0.0, 0.0, 0.011864407, 0.0, 0.010169491, 0.0016949152, 0.0, 0.0016949152, 0.008474576, 0.0033898305, 0.0, 0.0, 0.0, 0.0033898305, 0.0, 0.0033898305, 0.0050847456, 0.0050847456, 0.0033898305, 0.0033898305, 0.008474576, 0.0, 0.0033898305, 0.0033898305, 0.0050847456, 0.0050847456, 0.0016949152, 0.0, 0.0033898305, 0.0, 0.0016949152, 0.006779661, 0.0, 0.0033898305, 0.0, 0.0, 0.0, 0.0016949152, 0.0, 0.0016949152, 0.0016949152, 0.0033898305, 0.0, 0.0016949152, 0.0, 0.0, 0.006779661, 0.0, 0.0033898305, 0.0033898305, 0.0, 0.0, 0.0, 0.0, 0.006779661, 0.0016949152, 0.0050847456, 0.0033898305, 0.0016949152, 0.0, 0.0, 0.0, 0.0, 0.0033898305, 0.0033898305, 0.0033898305, 0.0033898305, 0.0, 0.0016949152, 0.0, 0.0, 0.0016949152, 0.0, 0.006779661, 0.0, 0.0, 0.0033898305, 0.0, 0.0, 0.0, 0.0033898305, 0.0, 0.0, 0.0, 0.0016949152, 0.0016949152, 0.006779661, 0.0033898305, 0.018644068, 0.006779661, 0.0, 0.0050847456, 0.0050847456, 0.0, 0.006779661, 0.0016949152, 0.0050847456, 0.0, 0.0016949152, 0.0, 0.0033898305, 0.0016949152, 0.008474576, 0.0, 0.010169491, 0.0016949152, 0.0016949152, 0.0016949152, 0.0050847456, 0.0, 0.0, 0.0016949152, 0.011864407, 0.0016949152, 0.0, 0.0, 0.0016949152, 0.0, 0.0, 0.0016949152, 0.0, 0.010169491 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0045045046, 0.003003003, 0.003003003, 0.003003003, 0.0045045046, 0.003003003, 0.0015015015, 0.0, 0.0045045046, 0.0015015015, 0.003003003, 0.006006006, 0.0, 0.0, 0.0045045046, 0.0045045046, 0.003003003, 0.0015015015, 0.003003003, 0.0015015015, 0.012012012, 0.0, 0.0, 0.003003003, 0.003003003, 0.006006006, 0.009009009, 0.013513514, 0.003003003, 0.0, 0.003003003, 0.0045045046, 0.0045045046, 0.0015015015, 0.0015015015, 0.0015015015, 0.0, 0.0045045046, 0.0, 0.0, 0.0, 0.0015015015, 0.0015015015, 0.0, 0.006006006, 0.0, 0.003003003, 0.0, 0.006006006, 0.0015015015, 0.0015015015, 0.003003003, 0.0, 0.0, 0.0, 0.003003003, 0.018018018, 0.0015015015, 0.0, 0.0015015015, 0.006006006, 0.003003003, 0.0015015015, 0.003003003, 0.0, 0.003003003, 0.006006006, 0.0, 0.0015015015, 0.0015015015, 0.0015015015, 0.010510511, 0.0, 0.003003003, 0.0015015015, 0.0045045046, 0.003003003, 0.0015015015, 0.0015015015, 0.010510511, 0.0, 0.0045045046, 0.0, 0.0, 0.0, 0.003003003, 0.0, 0.003003003, 0.006006006, 0.0045045046, 0.0, 0.0015015015, 0.006006006, 0.0, 0.0015015015, 0.0015015015, 0.0015015015, 0.0045045046, 0.003003003, 0.0015015015, 0.0, 0.0, 0.003003003, 0.0, 0.0015015015, 0.0, 0.0, 0.0045045046, 0.0, 0.003003003, 0.0015015015, 0.0, 0.0, 0.0075075077, 0.003003003, 0.0, 0.006006006, 0.0015015015, 0.0015015015, 0.003003003, 0.0, 0.0015015015, 0.0075075077, 0.0, 0.0075075077, 0.0, 0.003003003, 0.0015015015, 0.0, 0.0, 0.0, 0.0015015015, 0.0045045046, 0.0, 0.0, 0.006006006, 0.003003003, 0.0015015015, 0.003003003, 0.003003003, 0.003003003, 0.003003003, 0.0, 0.0075075077, 0.003003003, 0.003003003, 0.0, 0.0015015015, 0.0015015015, 0.0, 0.0015015015, 0.0, 0.0015015015, 0.0045045046, 0.006006006, 0.0015015015, 0.0015015015, 0.0015015015, 0.003003003, 0.003003003, 0.0015015015, 0.0015015015, 0.0, 0.009009009, 0.003003003, 0.0045045046, 0.0, 0.003003003, 0.0, 0.0, 0.0045045046, 0.003003003, 0.0015015015, 0.006006006, 0.0, 0.0045045046, 0.0015015015, 0.0, 0.006006006, 0.0015015015, 0.0015015015, 0.009009009, 0.0, 0.0015015015, 0.0, 0.0045045046, 0.003003003, 0.0, 0.0015015015, 0.0015015015, 0.0045045046, 0.0015015015, 0.0045045046, 0.006006006, 0.0, 0.006006006, 0.006006006, 0.0015015015, 0.0, 0.0045045046, 0.0015015015, 0.0015015015, 0.0045045046, 0.0045045046, 0.0, 0.0015015015, 0.0015015015, 0.0015015015, 0.0075075077, 0.0015015015, 0.009009009, 0.006006006, 0.006006006, 0.006006006, 0.006006006, 0.0, 0.0045045046, 0.003003003, 0.0, 0.0, 0.010510511, 0.0015015015, 0.0, 0.0045045046, 0.0015015015, 0.0, 0.006006006, 0.0, 0.0015015015, 0.003003003, 0.0015015015, 0.006006006, 0.0, 0.0, 0.0015015015, 0.0015015015, 0.0015015015, 0.0, 0.0015015015, 0.0045045046, 0.0, 0.0045045046, 0.0, 0.003003003, 0.003003003, 0.003003003, 0.0045045046, 0.003003003, 0.0015015015, 0.006006006, 0.0015015015, 0.0015015015, 0.003003003, 0.006006006, 0.0, 0.0015015015, 0.0015015015, 0.0015015015, 0.0, 0.0045045046, 0.0015015015, 0.006006006, 0.0, 0.0045045046, 0.0, 0.003003003, 0.0, 0.003003003, 0.0015015015, 0.0, 0.006006006, 0.013513514, 0.0015015015, 0.0075075077, 0.003003003, 0.0, 0.0015015015, 0.003003003, 0.0015015015, 0.0015015015, 0.0, 0.003003003, 0.0015015015, 0.0015015015, 0.0015015015, 0.0, 0.0, 0.0015015015, 0.003003003, 0.003003003, 0.0015015015, 0.0, 0.0015015015, 0.003003003, 0.003003003, 0.0, 0.003003003, 0.0075075077, 0.0045045046, 0.0, 0.0, 0.003003003, 0.003003003, 0.0015015015, 0.0015015015, 0.003003003, 0.0015015015, 0.006006006, 0.0015015015, 0.0, 0.0, 0.0, 0.0015015015, 0.006006006, 0.0015015015, 0.0015015015, 0.003003003, 0.0, 0.0015015015, 0.0015015015, 0.0015015015, 0.0075075077, 0.0015015015, 0.0015015015, 0.0, 0.0, 0.013513514, 0.0015015015, 0.0015015015, 0.003003003, 0.0045045046, 0.003003003, 0.0015015015, 0.003003003, 0.0, 0.003003003, 0.0045045046, 0.0, 0.0, 0.0015015015, 0.0015015015, 0.006006006, 0.0, 0.0015015015, 0.0, 0.003003003, 0.0, 0.0015015015, 0.0015015015, 0.0045045046, 0.003003003, 0.003003003, 0.0, 0.003003003, 0.0015015015, 0.0, 0.0015015015, 0.0045045046, 0.0, 0.0, 0.0075075077, 0.0, 0.003003003, 0.003003003, 0.0045045046, 0.0, 0.0, 0.0, 0.0045045046, 0.0015015015, 0.0015015015, 0.0, 0.0015015015, 0.0015015015, 0.0, 0.0015015015, 0.009009009, 0.003003003, 0.0015015015, 0.0015015015, 0.003003003, 0.0015015015, 0.010510511, 0.003003003, 0.0015015015, 0.003003003, 0.0045045046, 0.0015015015, 0.0, 0.012012012, 0.003003003, 0.0, 0.0015015015, 0.003003003, 0.0015015015, 0.006006006 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.009009009, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.009009009, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.018018018, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.009009009, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.027027028, 0.0, 0.009009009, 0.0, 0.0, 0.018018018, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.054054055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.009009009, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.018018018, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.009009009, 0.0, 0.0, 0.009009009, 0.0, 0.009009009, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.009009009, 0.009009009, 0.009009009, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018018018, 0.0, 0.0, 0.0, 0.018018018, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.009009009, 0.018018018, 0.0, 0.009009009, 0.0, 0.009009009, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.027027028, 0.009009009, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.018018018, 0.0, 0.0, 0.009009009, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.009009009, 0.009009009, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.018018018, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.018018018, 0.0, 0.036036037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.027027028, 0.009009009, 0.009009009, 0.0, 0.018018018, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018018018, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.009009009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.008333334, 0.0, 0.004166667, 0.0, 0.0, 0.0, 0.004166667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012500001, 0.004166667, 0.0, 0.0, 0.0, 0.004166667, 0.004166667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004166667, 0.0, 0.004166667, 0.0, 0.0, 0.008333334, 0.004166667, 0.0, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008333334, 0.0, 0.012500001, 0.0, 0.008333334, 0.0, 0.020833334, 0.0, 0.004166667, 0.004166667, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.004166667, 0.004166667, 0.0, 0.012500001, 0.0, 0.004166667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012500001, 0.008333334, 0.0, 0.008333334, 0.0, 0.0, 0.0, 0.004166667, 0.0, 0.008333334, 0.0, 0.0, 0.0, 0.004166667, 0.008333334, 0.0, 0.0, 0.0, 0.0, 0.004166667, 0.0, 0.0, 0.0, 0.012500001, 0.004166667, 0.004166667, 0.004166667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004166667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004166667, 0.0, 0.008333334, 0.0, 0.004166667, 0.0, 0.0, 0.0, 0.020833334, 0.008333334, 0.0, 0.0, 0.0, 0.0, 0.008333334, 0.0, 0.0, 0.0, 0.020833334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012500001, 0.0, 0.0, 0.0, 0.012500001, 0.0, 0.008333334, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008333334, 0.0, 0.0, 0.004166667, 0.045833334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004166667, 0.0, 0.008333334, 0.0, 0.0, 0.0, 0.012500001, 0.0, 0.0, 0.0, 0.004166667, 0.0, 0.0, 0.004166667, 0.004166667, 0.0, 0.012500001, 0.004166667, 0.0, 0.012500001, 0.008333334, 0.0, 0.020833334, 0.0, 0.0, 0.0, 0.0, 0.004166667, 0.0, 0.0, 0.0, 0.0, 0.004166667, 0.0, 0.0, 0.0, 0.004166667, 0.020833334, 0.0, 0.008333334, 0.0, 0.0, 0.0, 0.0, 0.008333334, 0.0, 0.0, 0.004166667, 0.0, 0.004166667, 0.0, 0.0, 0.004166667, 0.025000002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025000002, 0.0, 0.004166667, 0.004166667, 0.012500001, 0.0, 0.0, 0.0, 0.004166667, 0.004166667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004166667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004166667, 0.008333334, 0.0, 0.0, 0.0, 0.004166667, 0.0, 0.004166667, 0.016666668, 0.0, 0.020833334, 0.0, 0.0, 0.0, 0.008333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004166667, 0.004166667, 0.0, 0.004166667, 0.0, 0.0, 0.004166667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008333334, 0.004166667, 0.004166667, 0.0, 0.004166667, 0.0, 0.004166667, 0.0, 0.0, 0.0, 0.0, 0.008333334, 0.004166667, 0.0, 0.004166667, 0.004166667, 0.0, 0.0, 0.004166667, 0.0, 0.004166667, 0.008333334, 0.0, 0.004166667, 0.0, 0.012500001, 0.0, 0.0, 0.0, 0.0, 0.008333334, 0.0, 0.004166667, 0.0, 0.004166667, 0.004166667, 0.004166667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004166667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004166667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020833334, 0.004166667, 0.004166667, 0.004166667, 0.0, 0.0, 0.0, 0.012500001, 0.0, 0.004166667, 0.0, 0.004166667, 0.0, 0.004166667, 0.0, 0.0, 0.012500001, 0.0, 0.0, 0.0, 0.0, 0.004166667, 0.004166667, 0.0, 0.0, 0.008333334, 0.0, 0.0 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026086956, 0.034782607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.008695652, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.026086956, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.0, 0.017391304, 0.008695652, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.008695652, 0.0, 0.008695652, 0.0, 0.026086956, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034782607, 0.008695652, 0.008695652, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.008695652, 0.0, 0.026086956, 0.0, 0.0, 0.0, 0.008695652, 0.008695652, 0.008695652, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.008695652, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.008695652, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.017391304, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.008695652, 0.008695652, 0.026086956, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.008695652, 0.0, 0.008695652, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.008695652, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.008695652 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020833334, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020833334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020833334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.010416667, 0.0, 0.010416667, 0.020833334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.020833334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.010416667, 0.0, 0.020833334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.010416667, 0.0, 0.0, 0.020833334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.010416667, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.020833334, 0.0, 0.0, 0.010416667, 0.0, 0.010416667, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.020833334, 0.0, 0.0, 0.010416667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.006042296, 0.0, 0.003021148, 0.0, 0.003021148, 0.0, 0.006042296, 0.0, 0.0, 0.009063444, 0.0, 0.0, 0.003021148, 0.003021148, 0.0, 0.003021148, 0.0, 0.003021148, 0.0, 0.003021148, 0.0, 0.009063444, 0.003021148, 0.0, 0.0, 0.003021148, 0.0, 0.003021148, 0.003021148, 0.003021148, 0.0, 0.006042296, 0.0, 0.0, 0.003021148, 0.006042296, 0.0, 0.009063444, 0.003021148, 0.003021148, 0.009063444, 0.0, 0.0, 0.003021148, 0.01510574, 0.0, 0.0, 0.01510574, 0.0, 0.0, 0.0, 0.006042296, 0.003021148, 0.0, 0.0, 0.009063444, 0.003021148, 0.0, 0.009063444, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006042296, 0.003021148, 0.003021148, 0.006042296, 0.0, 0.009063444, 0.003021148, 0.0, 0.0, 0.003021148, 0.0, 0.012084592, 0.0, 0.003021148, 0.003021148, 0.006042296, 0.0, 0.003021148, 0.0, 0.003021148, 0.006042296, 0.006042296, 0.0, 0.006042296, 0.003021148, 0.01510574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.0, 0.0, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006042296, 0.0, 0.003021148, 0.003021148, 0.003021148, 0.0, 0.003021148, 0.0, 0.003021148, 0.003021148, 0.003021148, 0.003021148, 0.0, 0.006042296, 0.0, 0.003021148, 0.003021148, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.006042296, 0.01510574, 0.0, 0.0, 0.006042296, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.0, 0.0, 0.01510574, 0.006042296, 0.0, 0.0, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.0, 0.003021148, 0.003021148, 0.0, 0.003021148, 0.003021148, 0.0, 0.003021148, 0.0, 0.003021148, 0.01510574, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012084592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.003021148, 0.006042296, 0.0, 0.0, 0.003021148, 0.003021148, 0.0, 0.006042296, 0.003021148, 0.003021148, 0.006042296, 0.003021148, 0.003021148, 0.009063444, 0.0, 0.0, 0.0, 0.006042296, 0.0, 0.0, 0.0, 0.012084592, 0.0, 0.0, 0.003021148, 0.0, 0.003021148, 0.003021148, 0.003021148, 0.003021148, 0.003021148, 0.003021148, 0.0, 0.0, 0.006042296, 0.0, 0.0, 0.003021148, 0.009063444, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.0, 0.0, 0.0, 0.003021148, 0.009063444, 0.0, 0.0, 0.003021148, 0.003021148, 0.0, 0.0, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.0, 0.006042296, 0.003021148, 0.012084592, 0.009063444, 0.003021148, 0.003021148, 0.003021148, 0.003021148, 0.006042296, 0.003021148, 0.006042296, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006042296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.003021148, 0.003021148, 0.0, 0.0, 0.0, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.006042296, 0.0, 0.006042296, 0.0, 0.0, 0.0, 0.0, 0.012084592, 0.006042296, 0.009063444, 0.003021148, 0.009063444, 0.0, 0.006042296, 0.009063444, 0.009063444, 0.003021148, 0.003021148, 0.0, 0.003021148, 0.0, 0.003021148, 0.0, 0.0, 0.003021148, 0.0, 0.009063444, 0.009063444, 0.0, 0.006042296, 0.0, 0.01510574, 0.003021148, 0.0, 0.0, 0.003021148, 0.006042296, 0.009063444, 0.0, 0.0, 0.021148037, 0.0, 0.0, 0.003021148, 0.003021148, 0.009063444, 0.0, 0.0, 0.006042296, 0.0, 0.009063444, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.0, 0.006042296, 0.006042296, 0.012084592, 0.003021148, 0.0, 0.009063444, 0.0, 0.0, 0.009063444, 0.0, 0.0, 0.003021148, 0.009063444, 0.009063444, 0.003021148, 0.003021148, 0.0, 0.0, 0.006042296, 0.003021148, 0.0, 0.003021148, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.009063444, 0.006042296, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.003021148, 0.0, 0.003021148, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.006042296, 0.009063444, 0.003021148, 0.003021148, 0.0, 0.0, 0.0, 0.003021148, 0.003021148, 0.0, 0.0, 0.003021148, 0.0 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0016891892, 0.0, 0.0033783785, 0.0033783785, 0.0, 0.0033783785, 0.0016891892, 0.0033783785, 0.0016891892, 0.0, 0.0016891892, 0.005067568, 0.0, 0.0, 0.0033783785, 0.0, 0.0016891892, 0.0016891892, 0.0, 0.0033783785, 0.0, 0.0016891892, 0.006756757, 0.0, 0.0033783785, 0.0016891892, 0.0016891892, 0.04222973, 0.011824325, 0.0016891892, 0.006756757, 0.006756757, 0.0016891892, 0.013513514, 0.0016891892, 0.0016891892, 0.0016891892, 0.0, 0.0016891892, 0.0, 0.0, 0.0, 0.015202703, 0.0016891892, 0.0016891892, 0.0, 0.0033783785, 0.0, 0.0, 0.0033783785, 0.0033783785, 0.0, 0.0016891892, 0.0084459465, 0.0016891892, 0.0, 0.0033783785, 0.005067568, 0.0, 0.0, 0.0033783785, 0.0, 0.0, 0.0016891892, 0.0, 0.006756757, 0.0016891892, 0.0, 0.0, 0.006756757, 0.0, 0.005067568, 0.0016891892, 0.0033783785, 0.0, 0.0016891892, 0.0033783785, 0.0, 0.0, 0.0016891892, 0.0016891892, 0.006756757, 0.006756757, 0.0016891892, 0.0033783785, 0.0016891892, 0.010135136, 0.0, 0.0, 0.0033783785, 0.0, 0.0016891892, 0.0016891892, 0.0, 0.0, 0.0, 0.0016891892, 0.0, 0.0, 0.005067568, 0.0016891892, 0.0, 0.0, 0.0033783785, 0.0, 0.0016891892, 0.005067568, 0.0, 0.0016891892, 0.0016891892, 0.0016891892, 0.0016891892, 0.0, 0.0, 0.0016891892, 0.0, 0.0016891892, 0.0, 0.0033783785, 0.0033783785, 0.0016891892, 0.0016891892, 0.0016891892, 0.005067568, 0.0016891892, 0.0, 0.006756757, 0.0016891892, 0.0, 0.0033783785, 0.0, 0.0, 0.0033783785, 0.0, 0.0, 0.0016891892, 0.0, 0.0016891892, 0.0016891892, 0.0, 0.005067568, 0.005067568, 0.0016891892, 0.0, 0.0016891892, 0.0016891892, 0.006756757, 0.0033783785, 0.0, 0.0, 0.0016891892, 0.0016891892, 0.006756757, 0.0, 0.0, 0.0, 0.0033783785, 0.0, 0.0, 0.0033783785, 0.0, 0.0033783785, 0.0016891892, 0.040540542, 0.011824325, 0.006756757, 0.0084459465, 0.0, 0.0033783785, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0016891892, 0.0, 0.0, 0.010135136, 0.0084459465, 0.0033783785, 0.0033783785, 0.0, 0.0016891892, 0.0, 0.0, 0.0, 0.0016891892, 0.0, 0.0033783785, 0.0016891892, 0.027027028, 0.0016891892, 0.0, 0.0016891892, 0.0, 0.0, 0.0, 0.0, 0.0016891892, 0.0016891892, 0.0, 0.0016891892, 0.0, 0.0, 0.0033783785, 0.0033783785, 0.0033783785, 0.0016891892, 0.0, 0.0033783785, 0.0016891892, 0.0016891892, 0.0033783785, 0.0016891892, 0.0016891892, 0.0, 0.006756757, 0.0, 0.0, 0.0033783785, 0.0016891892, 0.0033783785, 0.011824325, 0.0084459465, 0.0, 0.0, 0.0033783785, 0.0033783785, 0.010135136, 0.0016891892, 0.0, 0.0, 0.0084459465, 0.011824325, 0.0, 0.0084459465, 0.0, 0.0, 0.006756757, 0.0033783785, 0.011824325, 0.0016891892, 0.0, 0.005067568, 0.005067568, 0.0033783785, 0.0033783785, 0.0, 0.005067568, 0.005067568, 0.013513514, 0.0, 0.0, 0.0016891892, 0.0, 0.0, 0.0016891892, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0033783785, 0.0016891892, 0.0016891892, 0.0016891892, 0.0016891892, 0.0, 0.0, 0.0016891892, 0.006756757, 0.0, 0.015202703, 0.0, 0.0, 0.0033783785, 0.0016891892, 0.0016891892, 0.0033783785, 0.0033783785, 0.0, 0.0, 0.0, 0.0033783785, 0.0016891892, 0.0, 0.0, 0.0016891892, 0.0033783785, 0.0, 0.0, 0.0016891892, 0.0, 0.006756757, 0.0, 0.0, 0.0033783785, 0.0, 0.0016891892, 0.0084459465, 0.0016891892, 0.0, 0.0033783785, 0.0033783785, 0.0, 0.0, 0.0016891892, 0.0, 0.006756757, 0.0033783785, 0.0033783785, 0.0, 0.0016891892, 0.0016891892, 0.006756757, 0.0016891892, 0.0033783785, 0.0, 0.0016891892, 0.0016891892, 0.011824325, 0.0, 0.0, 0.0033783785, 0.0, 0.0084459465, 0.0016891892, 0.0, 0.0016891892, 0.0033783785, 0.0, 0.0016891892, 0.0033783785, 0.0016891892, 0.010135136, 0.018581081, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0016891892, 0.005067568, 0.0, 0.0016891892, 0.0, 0.0016891892, 0.0016891892, 0.0033783785, 0.0, 0.0084459465, 0.0, 0.0016891892, 0.0016891892, 0.0033783785, 0.0, 0.0033783785, 0.005067568, 0.0033783785, 0.0016891892, 0.0033783785, 0.0016891892, 0.0016891892, 0.0, 0.0016891892, 0.0, 0.0016891892, 0.0, 0.0033783785, 0.0016891892, 0.005067568, 0.0, 0.0016891892, 0.015202703, 0.0, 0.0016891892, 0.0, 0.0016891892, 0.0, 0.0, 0.0, 0.0, 0.0016891892, 0.0, 0.0033783785, 0.0, 0.0016891892, 0.0, 0.0016891892, 0.0, 0.0, 0.0033783785, 0.0016891892, 0.005067568, 0.005067568, 0.0016891892, 0.0033783785, 0.0 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0010893246, 0.0021786492, 0.0021786492, 0.0010893246, 0.0021786492, 0.005446623, 0.007625272, 0.0032679737, 0.0010893246, 0.0, 0.0032679737, 0.0, 0.0065359473, 0.0010893246, 0.0032679737, 0.005446623, 0.0021786492, 0.005446623, 0.008714597, 0.005446623, 0.0032679737, 0.0021786492, 0.007625272, 0.0, 0.0021786492, 0.0, 0.0010893246, 0.0010893246, 0.0021786492, 0.005446623, 0.0010893246, 0.0, 0.0010893246, 0.0010893246, 0.0010893246, 0.0021786492, 0.0021786492, 0.0010893246, 0.0010893246, 0.0, 0.0043572984, 0.0032679737, 0.0, 0.0, 0.0043572984, 0.0010893246, 0.0, 0.0032679737, 0.0, 0.010893246, 0.007625272, 0.0032679737, 0.0, 0.007625272, 0.0, 0.0, 0.0032679737, 0.0065359473, 0.0010893246, 0.0021786492, 0.0, 0.0043572984, 0.009803921, 0.0, 0.0032679737, 0.0010893246, 0.0010893246, 0.0065359473, 0.0021786492, 0.010893246, 0.008714597, 0.0043572984, 0.007625272, 0.0021786492, 0.0032679737, 0.0010893246, 0.0021786492, 0.0032679737, 0.0043572984, 0.0032679737, 0.0010893246, 0.005446623, 0.0, 0.007625272, 0.0, 0.0032679737, 0.0021786492, 0.0, 0.0032679737, 0.0, 0.0032679737, 0.0010893246, 0.005446623, 0.0010893246, 0.007625272, 0.0032679737, 0.0010893246, 0.0010893246, 0.0010893246, 0.0, 0.0021786492, 0.0043572984, 0.0, 0.0010893246, 0.0010893246, 0.0, 0.0010893246, 0.0021786492, 0.0010893246, 0.008714597, 0.0, 0.0, 0.0021786492, 0.0010893246, 0.0021786492, 0.0, 0.0032679737, 0.0021786492, 0.0010893246, 0.0010893246, 0.0043572984, 0.005446623, 0.0, 0.0010893246, 0.0043572984, 0.0010893246, 0.0021786492, 0.0021786492, 0.0, 0.005446623, 0.0043572984, 0.0043572984, 0.0, 0.0032679737, 0.0021786492, 0.0065359473, 0.0010893246, 0.0010893246, 0.0, 0.0021786492, 0.0010893246, 0.005446623, 0.0010893246, 0.0010893246, 0.0, 0.0, 0.0032679737, 0.0043572984, 0.0, 0.0021786492, 0.0, 0.005446623, 0.0021786492, 0.0010893246, 0.005446623, 0.005446623, 0.0010893246, 0.007625272, 0.0010893246, 0.0021786492, 0.0, 0.009803921, 0.0032679737, 0.0, 0.0, 0.0065359473, 0.0043572984, 0.0021786492, 0.0021786492, 0.0032679737, 0.0010893246, 0.0010893246, 0.0032679737, 0.0010893246, 0.0032679737, 0.0043572984, 0.0021786492, 0.0010893246, 0.0021786492, 0.005446623, 0.0032679737, 0.0043572984, 0.0021786492, 0.01198257, 0.0021786492, 0.0, 0.0010893246, 0.0, 0.005446623, 0.0, 0.0, 0.0, 0.005446623, 0.005446623, 0.0032679737, 0.009803921, 0.0043572984, 0.0032679737, 0.0021786492, 0.0043572984, 0.0043572984, 0.0, 0.008714597, 0.005446623, 0.0021786492, 0.0010893246, 0.0, 0.0, 0.0065359473, 0.0, 0.0021786492, 0.0021786492, 0.0032679737, 0.0, 0.0043572984, 0.0, 0.0, 0.0043572984, 0.0032679737, 0.0043572984, 0.0043572984, 0.0032679737, 0.0, 0.0010893246, 0.0, 0.0, 0.009803921, 0.009803921, 0.0, 0.0, 0.0010893246, 0.0010893246, 0.0021786492, 0.0, 0.0, 0.0, 0.0, 0.005446623, 0.0010893246, 0.0021786492, 0.0, 0.0021786492, 0.0010893246, 0.0010893246, 0.0043572984, 0.0, 0.0010893246, 0.0021786492, 0.0010893246, 0.0, 0.0, 0.0021786492, 0.0, 0.0010893246, 0.0010893246, 0.0, 0.005446623, 0.0032679737, 0.0043572984, 0.007625272, 0.005446623, 0.0, 0.0, 0.005446623, 0.0032679737, 0.0, 0.0010893246, 0.005446623, 0.0010893246, 0.0010893246, 0.0, 0.0010893246, 0.0, 0.0043572984, 0.0043572984, 0.0010893246, 0.0, 0.0021786492, 0.0021786492, 0.0010893246, 0.0, 0.0021786492, 0.0, 0.0, 0.0, 0.0032679737, 0.0065359473, 0.0, 0.007625272, 0.007625272, 0.005446623, 0.0043572984, 0.0010893246, 0.0010893246, 0.0, 0.0, 0.0021786492, 0.0010893246, 0.0021786492, 0.0043572984, 0.0, 0.0, 0.0021786492, 0.0021786492, 0.0043572984, 0.0010893246, 0.007625272, 0.0010893246, 0.0010893246, 0.007625272, 0.0043572984, 0.0021786492, 0.0, 0.007625272, 0.007625272, 0.0, 0.0021786492, 0.0, 0.0032679737, 0.0010893246, 0.0043572984, 0.0, 0.0, 0.0, 0.0021786492, 0.0021786492, 0.0021786492, 0.0021786492, 0.0032679737, 0.0021786492, 0.0010893246, 0.007625272, 0.0021786492, 0.0065359473, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0032679737, 0.0010893246, 0.0043572984, 0.0010893246, 0.0, 0.0, 0.0, 0.0010893246, 0.007625272, 0.005446623, 0.0021786492, 0.0010893246, 0.008714597, 0.0032679737, 0.0032679737, 0.0010893246, 0.0032679737, 0.0032679737, 0.0021786492, 0.0032679737, 0.0, 0.0043572984, 0.0, 0.005446623, 0.0, 0.0010893246, 0.009803921, 0.0032679737, 0.0010893246, 0.0010893246, 0.0032679737, 0.005446623, 0.0010893246, 0.0, 0.0, 0.0021786492, 0.0010893246, 0.0043572984, 0.0065359473, 0.0, 0.0021786492, 0.0032679737, 0.0032679737, 0.0010893246, 0.0032679737, 0.0021786492, 0.0032679737, 0.0010893246, 0.0, 0.0010893246, 0.0, 0.0043572984, 0.0, 0.0010893246, 0.0, 0.0, 0.0010893246, 0.0021786492, 0.0010893246, 0.0010893246 ]
-- 1
400 5
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013605442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027210884, 0.0, 0.0, 0.020408163, 0.006802721, 0.0, 0.040816326, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020408163, 0.013605442, 0.0, 0.0, 0.0, 0.0, 0.013605442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.013605442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013605442, 0.006802721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06122449, 0.0, 0.006802721, 0.006802721, 0.006802721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.006802721, 0.0, 0.0, 0.020408163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013605442, 0.006802721, 0.0, 0.006802721, 0.0, 0.006802721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013605442, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.006802721, 0.006802721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.020408163, 0.006802721, 0.006802721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.0, 0.006802721, 0.006802721, 0.013605442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10204081, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013605442, 0.013605442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.0, 0.027210884, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.0, 0.006802721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.034013607, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020408163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.013605442, 0.0, 0.0, 0.0, 0.0, 0.013605442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.0, 0.006802721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.013605442, 0.0, 0.0, 0.0, 0.006802721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020408163, 0.0, 0.0, 0.020408163, 0.020408163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006802721, 0.006802721, 0.0, 0.0, 0.0, 0.020408163, 0.013605442, 0.0, 0.0, 0.0, 0.013605442, 0.006802721, 0.0, 0.0, 0.0, 0.020408163, 0.0 ]
-- 1
400 5
Histogram size : (400, 1)
Histogram : [ 0.0031545742, 0.0, 0.0015772871, 0.0, 0.0031545742, 0.012618297, 0.0031545742, 0.0015772871, 0.0031545742, 0.0, 0.0, 0.0031545742, 0.0, 0.0, 0.0, 0.0015772871, 0.0, 0.014195584, 0.0015772871, 0.0, 0.0047318614, 0.0015772871, 0.0, 0.0, 0.0015772871, 0.0, 0.0, 0.0063091484, 0.0047318614, 0.0031545742, 0.0047318614, 0.0031545742, 0.0015772871, 0.0, 0.0015772871, 0.0, 0.0063091484, 0.0031545742, 0.0, 0.0015772871, 0.0, 0.0, 0.0015772871, 0.0031545742, 0.0047318614, 0.0, 0.0, 0.0, 0.0031545742, 0.0015772871, 0.0047318614, 0.009463723, 0.0, 0.0031545742, 0.0, 0.0, 0.0015772871, 0.0063091484, 0.0015772871, 0.0, 0.0015772871, 0.0, 0.007886436, 0.0, 0.0031545742, 0.0, 0.007886436, 0.0015772871, 0.0063091484, 0.0015772871, 0.0031545742, 0.0047318614, 0.0015772871, 0.007886436, 0.0, 0.0031545742, 0.0, 0.007886436, 0.0, 0.0031545742, 0.0, 0.0031545742, 0.0, 0.0, 0.0031545742, 0.0015772871, 0.0047318614, 0.0, 0.0, 0.0031545742, 0.0, 0.0, 0.0015772871, 0.0015772871, 0.0, 0.0015772871, 0.0015772871, 0.0031545742, 0.0015772871, 0.0031545742, 0.0, 0.0063091484, 0.0031545742, 0.0, 0.0015772871, 0.0015772871, 0.009463723, 0.0047318614, 0.0, 0.0, 0.0031545742, 0.0015772871, 0.0047318614, 0.028391168, 0.0, 0.0, 0.0, 0.0015772871, 0.0, 0.0, 0.0015772871, 0.0, 0.0, 0.0015772871, 0.0, 0.0047318614, 0.01104101, 0.0015772871, 0.0, 0.0, 0.0015772871, 0.0, 0.0031545742, 0.0, 0.0015772871, 0.0, 0.0031545742, 0.0063091484, 0.0015772871, 0.0047318614, 0.0015772871, 0.007886436, 0.0, 0.0015772871, 0.0015772871, 0.0, 0.0015772871, 0.0047318614, 0.0015772871, 0.0, 0.0015772871, 0.0, 0.0047318614, 0.0015772871, 0.01104101, 0.0063091484, 0.007886436, 0.012618297, 0.0063091484, 0.0015772871, 0.0, 0.0015772871, 0.0031545742, 0.0, 0.0015772871, 0.0, 0.01104101, 0.0031545742, 0.0, 0.0015772871, 0.0015772871, 0.0031545742, 0.0, 0.0031545742, 0.0, 0.0031545742, 0.0015772871, 0.0063091484, 0.0, 0.0031545742, 0.0, 0.0047318614, 0.0015772871, 0.0063091484, 0.0047318614, 0.0, 0.0, 0.0031545742, 0.0, 0.0, 0.007886436, 0.0031545742, 0.0, 0.007886436, 0.0, 0.0015772871, 0.0015772871, 0.0, 0.0, 0.0, 0.0063091484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0031545742, 0.0, 0.0031545742, 0.0015772871, 0.0031545742, 0.0031545742, 0.0, 0.0015772871, 0.0, 0.0031545742, 0.0, 0.0015772871, 0.007886436, 0.0, 0.0015772871, 0.0015772871, 0.0015772871, 0.0047318614, 0.0015772871, 0.0015772871, 0.0, 0.0031545742, 0.0, 0.0, 0.0031545742, 0.0, 0.0, 0.0063091484, 0.0063091484, 0.0015772871, 0.0, 0.0015772871, 0.0, 0.0, 0.0015772871, 0.0, 0.0015772871, 0.0015772871, 0.0, 0.0063091484, 0.0, 0.009463723, 0.014195584, 0.0, 0.0015772871, 0.0015772871, 0.0015772871, 0.0, 0.0, 0.007886436, 0.0, 0.01104101, 0.0, 0.014195584, 0.0015772871, 0.0031545742, 0.0, 0.0015772871, 0.0, 0.0015772871, 0.0015772871, 0.0, 0.0063091484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007886436, 0.0047318614, 0.0015772871, 0.0031545742, 0.0015772871, 0.0031545742, 0.0, 0.0, 0.0031545742, 0.0047318614, 0.0, 0.0031545742, 0.0015772871, 0.0015772871, 0.0, 0.0063091484, 0.0, 0.0063091484, 0.0015772871, 0.0, 0.0015772871, 0.0, 0.0, 0.0031545742, 0.009463723, 0.0015772871, 0.0063091484, 0.0031545742, 0.0, 0.0015772871, 0.0, 0.0015772871, 0.0047318614, 0.0047318614, 0.0, 0.0, 0.0, 0.007886436, 0.0015772871, 0.0, 0.0063091484, 0.0, 0.0, 0.0015772871, 0.0, 0.0015772871, 0.0015772871, 0.0, 0.0047318614, 0.0, 0.0, 0.0, 0.0015772871, 0.0015772871, 0.0015772871, 0.0031545742, 0.0047318614, 0.0063091484, 0.0047318614, 0.0015772871, 0.018927446, 0.0, 0.0, 0.0, 0.0, 0.0031545742, 0.0047318614, 0.0031545742, 0.009463723, 0.0, 0.0, 0.0047318614, 0.0, 0.0, 0.0031545742, 0.0031545742, 0.0, 0.0, 0.009463723, 0.01104101, 0.0031545742, 0.0047318614, 0.0015772871, 0.0031545742, 0.0, 0.0, 0.012618297, 0.0015772871, 0.0, 0.0031545742, 0.0015772871, 0.0031545742, 0.0031545742, 0.0015772871, 0.0015772871, 0.0047318614, 0.0, 0.0031545742, 0.0015772871, 0.0031545742, 0.009463723, 0.0031545742, 0.0015772871, 0.0031545742, 0.0, 0.0047318614, 0.0, 0.0015772871, 0.0, 0.0, 0.0015772871, 0.01104101, 0.009463723, 0.0031545742, 0.0015772871, 0.009463723, 0.0, 0.007886436, 0.0047318614, 0.0031545742, 0.0, 0.0, 0.0015772871, 0.0015772871, 0.0031545742, 0.0015772871 ]
-- 1
Histogram size : (400, 1)
Histogram : [ 0.007075472, 0.0, 0.0023584906, 0.0, 0.0, 0.0023584906, 0.004716981, 0.0023584906, 0.011792453, 0.0, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.0023584906, 0.0023584906, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.0023584906, 0.004716981, 0.0, 0.0, 0.0754717, 0.0023584906, 0.0, 0.0, 0.0023584906, 0.004716981, 0.0, 0.009433962, 0.0, 0.0023584906, 0.0023584906, 0.004716981, 0.0, 0.0, 0.0, 0.0023584906, 0.0, 0.0, 0.0, 0.0, 0.025943397, 0.0, 0.0, 0.0023584906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004716981, 0.0, 0.0, 0.0023584906, 0.0, 0.011792453, 0.0023584906, 0.0, 0.0, 0.0, 0.0, 0.011792453, 0.009433962, 0.0023584906, 0.004716981, 0.0, 0.0, 0.0023584906, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.004716981, 0.0, 0.028301887, 0.0023584906, 0.0023584906, 0.0, 0.004716981, 0.0, 0.011792453, 0.0, 0.004716981, 0.0023584906, 0.0023584906, 0.0, 0.007075472, 0.0, 0.004716981, 0.0023584906, 0.0, 0.0, 0.0, 0.004716981, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.0, 0.0, 0.0023584906, 0.0, 0.004716981, 0.0023584906, 0.0023584906, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.0, 0.0023584906, 0.0023584906, 0.007075472, 0.0, 0.0, 0.004716981, 0.0, 0.007075472, 0.0, 0.068396226, 0.0023584906, 0.0, 0.004716981, 0.0, 0.007075472, 0.0, 0.0, 0.007075472, 0.0, 0.004716981, 0.0, 0.0023584906, 0.0, 0.0, 0.0, 0.0023584906, 0.0, 0.004716981, 0.0, 0.0023584906, 0.0, 0.0, 0.011792453, 0.004716981, 0.007075472, 0.0, 0.0, 0.0, 0.007075472, 0.0023584906, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.0, 0.004716981, 0.004716981, 0.0, 0.0, 0.0, 0.0023584906, 0.0, 0.0, 0.0, 0.0023584906, 0.0, 0.011792453, 0.0, 0.0, 0.007075472, 0.0, 0.0, 0.004716981, 0.009433962, 0.004716981, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007075472, 0.0, 0.0, 0.007075472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0023584906, 0.0, 0.0, 0.0, 0.0, 0.004716981, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.0, 0.0, 0.0, 0.004716981, 0.0495283, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.0, 0.014150944, 0.0023584906, 0.0, 0.0, 0.0, 0.0023584906, 0.0, 0.0, 0.0023584906, 0.0, 0.007075472, 0.0023584906, 0.0023584906, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.004716981, 0.0, 0.0, 0.009433962, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.0023584906, 0.007075472, 0.0023584906, 0.0023584906, 0.0, 0.0023584906, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.0, 0.009433962, 0.004716981, 0.0, 0.0023584906, 0.0, 0.0, 0.0, 0.004716981, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.007075472, 0.0, 0.004716981, 0.0023584906, 0.004716981, 0.0, 0.0, 0.0, 0.0, 0.0023584906, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.0023584906, 0.0, 0.014150944, 0.0, 0.0023584906, 0.004716981, 0.0, 0.0023584906, 0.0, 0.007075472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.0023584906, 0.004716981, 0.0, 0.004716981, 0.0, 0.0, 0.0023584906, 0.004716981, 0.0, 0.004716981, 0.014150944, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.004716981, 0.004716981, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004716981, 0.0023584906, 0.0, 0.0, 0.0, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.004716981, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.004716981, 0.0, 0.007075472, 0.0023584906, 0.0, 0.0, 0.0, 0.0023584906, 0.007075472, 0.0023584906, 0.0, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.004716981, 0.0, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.0023584906, 0.004716981, 0.0, 0.0, 0.0, 0.0023584906, 0.0, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.004716981, 0.0, 0.0, 0.011792453, 0.0, 0.0, 0.004716981, 0.004716981, 0.0023584906, 0.004716981 ]
-- 1
17/02/16 16:48:20 INFO FileOutputCommitter: Saved output of task 'attempt_201702161648_0064_m_000003_249' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/histograms/_temporary/0/task_201702161648_0064_m_000003
17/02/16 16:48:20 INFO FileOutputCommitter: Saved output of task 'attempt_201702161648_0064_m_000000_246' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/histograms/_temporary/0/task_201702161648_0064_m_000000
17/02/16 16:48:20 INFO FileOutputCommitter: Saved output of task 'attempt_201702161648_0064_m_000002_248' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/histograms/_temporary/0/task_201702161648_0064_m_000002
17/02/16 16:48:20 INFO FileOutputCommitter: Saved output of task 'attempt_201702161648_0064_m_000001_247' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/histograms/_temporary/0/task_201702161648_0064_m_000001
Total size : 47
numTrees 4 featureSubsetStrategy all impurity gini maxDepth 3
17/02/16 16:48:21 INFO FileInputFormat: Total input paths to process : 4
17/02/16 16:48:21 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.75
 |=================== Confusion matrix ==========================
2.0  1.0  0.0  0.0  
0.0  1.0  0.0  0.0  
0.0  0.0  1.0  0.0  
0.0  0.0  1.0  0.0  
17/02/16 16:48:28 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
0.25
numTrees 4 featureSubsetStrategy all impurity gini maxDepth 4
17/02/16 16:48:29 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.4375
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  2.0  0.0  
0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  
0.5625
numTrees 4 featureSubsetStrategy all impurity gini maxDepth 5
Test Error = 0.1875
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.8125
numTrees 4 featureSubsetStrategy all impurity gini maxDepth 6
17/02/16 16:48:31 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.3125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  2.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.6875
numTrees 4 featureSubsetStrategy all impurity entropy maxDepth 3
17/02/16 16:48:32 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.3125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.6875
numTrees 4 featureSubsetStrategy all impurity entropy maxDepth 4
17/02/16 16:48:33 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.3125
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  3.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.6875
numTrees 4 featureSubsetStrategy all impurity entropy maxDepth 5
17/02/16 16:48:35 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
17/02/16 16:48:36 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
0.875
numTrees 4 featureSubsetStrategy all impurity entropy maxDepth 6
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 4 featureSubsetStrategy sqrt impurity gini maxDepth 3
17/02/16 16:48:37 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.4375
[Stage 247:==============>                                          (1 + 3) / 4] |=================== Confusion matrix ==========================
2.0  1.0  1.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  1.0  
0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  1.0  1.0  
0.5625
numTrees 4 featureSubsetStrategy sqrt impurity gini maxDepth 4
Test Error = 0.4375
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  1.0  0.0  
0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  
1.0  0.0  0.0  0.0  2.0  
0.5625
numTrees 4 featureSubsetStrategy sqrt impurity gini maxDepth 5
17/02/16 16:48:44 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
17/02/16 16:48:45 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.3125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  
0.0  0.0  2.0  1.0  1.0  0.0  0.0  1.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  
17/02/16 16:48:49 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
0.6875
numTrees 4 featureSubsetStrategy sqrt impurity gini maxDepth 6
Test Error = 0.25
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  1.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.75
numTrees 4 featureSubsetStrategy sqrt impurity entropy maxDepth 3
17/02/16 16:48:51 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.375
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  2.0  1.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  1.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  
0.625
numTrees 4 featureSubsetStrategy sqrt impurity entropy maxDepth 4
17/02/16 16:48:52 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.875
numTrees 4 featureSubsetStrategy sqrt impurity entropy maxDepth 5
17/02/16 16:48:53 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  2.0  
0.875
numTrees 4 featureSubsetStrategy sqrt impurity entropy maxDepth 6
17/02/16 16:48:54 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.8125
numTrees 4 featureSubsetStrategy log2 impurity gini maxDepth 3
17/02/16 16:48:55 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.3125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  1.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  1.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  
0.6875
numTrees 4 featureSubsetStrategy log2 impurity gini maxDepth 4
17/02/16 16:48:55 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.4375
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  3.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.5625
numTrees 4 featureSubsetStrategy log2 impurity gini maxDepth 5
17/02/16 16:48:56 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.4375
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  
0.0  3.0  0.0  0.0  0.0  
0.0  0.0  2.0  1.0  0.0  
0.0  0.0  0.0  1.0  0.0  
1.0  0.0  0.0  0.0  2.0  
0.5625
numTrees 4 featureSubsetStrategy log2 impurity gini maxDepth 6
17/02/16 16:48:57 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 4 featureSubsetStrategy log2 impurity entropy maxDepth 3
17/02/16 16:48:58 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.3125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  1.0  1.0  0.0  2.0  
0.6875
numTrees 4 featureSubsetStrategy log2 impurity entropy maxDepth 4
17/02/16 16:48:59 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  
17/02/16 16:49:00 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
0.75
numTrees 4 featureSubsetStrategy log2 impurity entropy maxDepth 5
Test Error = 0.1875
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
1.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.8125
numTrees 4 featureSubsetStrategy log2 impurity entropy maxDepth 6
17/02/16 16:49:00 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
17/02/16 16:49:01 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
0.9375
numTrees 4 featureSubsetStrategy onethird impurity gini maxDepth 3
Test Error = 0.5
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  
0.0  0.0  1.0  2.0  0.0  
0.0  0.0  0.0  0.0  2.0  
0.5
numTrees 4 featureSubsetStrategy onethird impurity gini maxDepth 4
17/02/16 16:49:03 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  1.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  
0.75
numTrees 4 featureSubsetStrategy onethird impurity gini maxDepth 5
17/02/16 16:49:04 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.375
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  3.0  0.0  0.0  0.0  0.0  
1.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.625
numTrees 4 featureSubsetStrategy onethird impurity gini maxDepth 6
17/02/16 16:49:05 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  
17/02/16 16:49:06 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
0.8125
numTrees 4 featureSubsetStrategy onethird impurity entropy maxDepth 3
Test Error = 0.375
 |=================== Confusion matrix ==========================
2.0  1.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  1.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  1.0  
0.0  0.0  0.0  0.0  0.0  1.0  
0.625
numTrees 4 featureSubsetStrategy onethird impurity entropy maxDepth 4
17/02/16 16:49:07 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  0.0  
1.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.8125
numTrees 4 featureSubsetStrategy onethird impurity entropy maxDepth 5
17/02/16 16:49:08 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 4 featureSubsetStrategy onethird impurity entropy maxDepth 6
17/02/16 16:49:13 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
1.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.8125
numTrees 5 featureSubsetStrategy all impurity gini maxDepth 3
17/02/16 16:49:31 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.375
                                                                                 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  2.0  
0.625
numTrees 5 featureSubsetStrategy all impurity gini maxDepth 4
17/02/16 16:49:35 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.5625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  1.0  1.0  2.0  0.0  0.0  
0.4375
numTrees 5 featureSubsetStrategy all impurity gini maxDepth 5
17/02/16 16:49:36 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  2.0  
0.8125
numTrees 5 featureSubsetStrategy all impurity gini maxDepth 6
17/02/16 16:49:39 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 5 featureSubsetStrategy all impurity entropy maxDepth 3
17/02/16 16:49:42 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.8125
numTrees 5 featureSubsetStrategy all impurity entropy maxDepth 4
17/02/16 16:49:43 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 5 featureSubsetStrategy all impurity entropy maxDepth 5
17/02/16 16:49:45 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 5 featureSubsetStrategy all impurity entropy maxDepth 6
17/02/16 16:49:46 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 5 featureSubsetStrategy sqrt impurity gini maxDepth 3
17/02/16 16:49:47 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.375
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  1.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  
0.0  1.0  0.0  1.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  
0.625
numTrees 5 featureSubsetStrategy sqrt impurity gini maxDepth 4
17/02/16 16:49:48 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.3125
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  
1.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  
0.6875
numTrees 5 featureSubsetStrategy sqrt impurity gini maxDepth 5
17/02/16 16:49:49 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  1.0  1.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  2.0  
0.75
numTrees 5 featureSubsetStrategy sqrt impurity gini maxDepth 6
17/02/16 16:49:49 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  1.0  1.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  
1.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.75
numTrees 5 featureSubsetStrategy sqrt impurity entropy maxDepth 3
17/02/16 16:49:50 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.375
 |=================== Confusion matrix ==========================
2.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  
0.625
numTrees 5 featureSubsetStrategy sqrt impurity entropy maxDepth 4
17/02/16 16:49:51 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 5 featureSubsetStrategy sqrt impurity entropy maxDepth 5
17/02/16 16:49:52 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 5 featureSubsetStrategy sqrt impurity entropy maxDepth 6
17/02/16 16:49:53 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 5 featureSubsetStrategy log2 impurity gini maxDepth 3
17/02/16 16:49:53 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  1.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  
0.75
numTrees 5 featureSubsetStrategy log2 impurity gini maxDepth 4
17/02/16 16:49:54 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  
0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  
0.8125
numTrees 5 featureSubsetStrategy log2 impurity gini maxDepth 5
17/02/16 16:49:55 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  1.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  
1.0  0.0  0.0  0.0  0.0  1.0  1.0  
0.75
numTrees 5 featureSubsetStrategy log2 impurity gini maxDepth 6
17/02/16 16:49:55 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 5 featureSubsetStrategy log2 impurity entropy maxDepth 3
17/02/16 16:49:56 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.4375
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  
1.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.5625
numTrees 5 featureSubsetStrategy log2 impurity entropy maxDepth 4
17/02/16 16:49:57 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  1.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  
0.8125
numTrees 5 featureSubsetStrategy log2 impurity entropy maxDepth 5
17/02/16 16:49:57 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  2.0  
0.9375
numTrees 5 featureSubsetStrategy log2 impurity entropy maxDepth 6
17/02/16 16:49:58 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.875
numTrees 5 featureSubsetStrategy onethird impurity gini maxDepth 3
17/02/16 16:49:59 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.5625
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  
1.0  1.0  0.0  1.0  1.0  1.0  
0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  1.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  
0.4375
numTrees 5 featureSubsetStrategy onethird impurity gini maxDepth 4
17/02/16 16:50:00 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.875
numTrees 5 featureSubsetStrategy onethird impurity gini maxDepth 5
17/02/16 16:50:01 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 5 featureSubsetStrategy onethird impurity gini maxDepth 6
17/02/16 16:50:02 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  2.0  
0.75
numTrees 5 featureSubsetStrategy onethird impurity entropy maxDepth 3
17/02/16 16:50:03 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  
1.0  0.0  0.0  0.0  0.0  2.0  1.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  
0.75
numTrees 5 featureSubsetStrategy onethird impurity entropy maxDepth 4
17/02/16 16:50:03 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
1.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.875
numTrees 5 featureSubsetStrategy onethird impurity entropy maxDepth 5
17/02/16 16:50:04 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 5 featureSubsetStrategy onethird impurity entropy maxDepth 6
17/02/16 16:50:05 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  
0.75
numTrees 6 featureSubsetStrategy all impurity gini maxDepth 3
17/02/16 16:50:07 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  3.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.8125
numTrees 6 featureSubsetStrategy all impurity gini maxDepth 4
17/02/16 16:50:07 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  2.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  2.0  
0.75
numTrees 6 featureSubsetStrategy all impurity gini maxDepth 5
17/02/16 16:50:08 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 6 featureSubsetStrategy all impurity gini maxDepth 6
17/02/16 16:50:10 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 6 featureSubsetStrategy all impurity entropy maxDepth 3
17/02/16 16:50:11 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  
0.875
numTrees 6 featureSubsetStrategy all impurity entropy maxDepth 4
17/02/16 16:50:12 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 6 featureSubsetStrategy all impurity entropy maxDepth 5
17/02/16 16:50:13 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 6 featureSubsetStrategy all impurity entropy maxDepth 6
17/02/16 16:50:14 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 6 featureSubsetStrategy sqrt impurity gini maxDepth 3
17/02/16 16:50:16 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
2.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.75
numTrees 6 featureSubsetStrategy sqrt impurity gini maxDepth 4
17/02/16 16:50:16 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.3125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  1.0  
0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  
0.6875
numTrees 6 featureSubsetStrategy sqrt impurity gini maxDepth 5
17/02/16 16:50:17 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.875
numTrees 6 featureSubsetStrategy sqrt impurity gini maxDepth 6
17/02/16 16:50:18 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.875
numTrees 6 featureSubsetStrategy sqrt impurity entropy maxDepth 3
17/02/16 16:50:18 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  1.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  
0.75
numTrees 6 featureSubsetStrategy sqrt impurity entropy maxDepth 4
17/02/16 16:50:19 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 6 featureSubsetStrategy sqrt impurity entropy maxDepth 5
17/02/16 16:50:20 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 6 featureSubsetStrategy sqrt impurity entropy maxDepth 6
17/02/16 16:50:20 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 6 featureSubsetStrategy log2 impurity gini maxDepth 3
17/02/16 16:50:21 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
1.0  0.0  0.0  0.0  0.0  2.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  2.0  
0.8125
numTrees 6 featureSubsetStrategy log2 impurity gini maxDepth 4
17/02/16 16:50:22 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.875
numTrees 6 featureSubsetStrategy log2 impurity gini maxDepth 5
17/02/16 16:50:22 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
1.0  0.0  0.0  0.0  3.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.8125
numTrees 6 featureSubsetStrategy log2 impurity gini maxDepth 6
17/02/16 16:50:24 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.875
numTrees 6 featureSubsetStrategy log2 impurity entropy maxDepth 3
17/02/16 16:50:25 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.4375
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  
0.0  0.0  3.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  
0.5625
numTrees 6 featureSubsetStrategy log2 impurity entropy maxDepth 4
17/02/16 16:50:25 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.875
numTrees 6 featureSubsetStrategy log2 impurity entropy maxDepth 5
17/02/16 16:50:26 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 6 featureSubsetStrategy log2 impurity entropy maxDepth 6
17/02/16 16:50:27 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  2.0  
0.9375
numTrees 6 featureSubsetStrategy onethird impurity gini maxDepth 3
17/02/16 16:50:29 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.375
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  1.0  0.0  1.0  2.0  
0.625
numTrees 6 featureSubsetStrategy onethird impurity gini maxDepth 4
17/02/16 16:50:30 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.3125
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
1.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.6875
numTrees 6 featureSubsetStrategy onethird impurity gini maxDepth 5
17/02/16 16:50:31 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  1.0  2.0  
0.8125
numTrees 6 featureSubsetStrategy onethird impurity gini maxDepth 6
17/02/16 16:50:31 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  2.0  
0.8125
numTrees 6 featureSubsetStrategy onethird impurity entropy maxDepth 3
17/02/16 16:50:32 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 6 featureSubsetStrategy onethird impurity entropy maxDepth 4
17/02/16 16:50:33 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 6 featureSubsetStrategy onethird impurity entropy maxDepth 5
17/02/16 16:50:34 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 6 featureSubsetStrategy onethird impurity entropy maxDepth 6
17/02/16 16:50:35 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 7 featureSubsetStrategy all impurity gini maxDepth 3
17/02/16 16:50:36 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.375
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  1.0  3.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  
0.625
numTrees 7 featureSubsetStrategy all impurity gini maxDepth 4
17/02/16 16:50:37 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.8125
numTrees 7 featureSubsetStrategy all impurity gini maxDepth 5
17/02/16 16:50:38 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  2.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
17/02/16 16:50:39 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
0.75
numTrees 7 featureSubsetStrategy all impurity gini maxDepth 6
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.875
numTrees 7 featureSubsetStrategy all impurity entropy maxDepth 3
17/02/16 16:50:41 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
1.0  0.0  0.0  0.0  1.0  0.0  0.0  2.0  
0.875
numTrees 7 featureSubsetStrategy all impurity entropy maxDepth 4
17/02/16 16:50:42 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 7 featureSubsetStrategy all impurity entropy maxDepth 5
17/02/16 16:50:43 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 7 featureSubsetStrategy all impurity entropy maxDepth 6
17/02/16 16:50:44 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 7 featureSubsetStrategy sqrt impurity gini maxDepth 3
17/02/16 16:50:47 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.3125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  
0.6875
numTrees 7 featureSubsetStrategy sqrt impurity gini maxDepth 4
17/02/16 16:50:47 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.3125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  1.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  
0.0  0.0  1.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  
0.6875
numTrees 7 featureSubsetStrategy sqrt impurity gini maxDepth 5
17/02/16 16:50:48 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.9375
numTrees 7 featureSubsetStrategy sqrt impurity gini maxDepth 6
17/02/16 16:50:48 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
1.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 7 featureSubsetStrategy sqrt impurity entropy maxDepth 3
17/02/16 16:50:49 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.3125
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  1.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  
0.6875
numTrees 7 featureSubsetStrategy sqrt impurity entropy maxDepth 4
17/02/16 16:50:50 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.875
numTrees 7 featureSubsetStrategy sqrt impurity entropy maxDepth 5
17/02/16 16:50:50 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 7 featureSubsetStrategy sqrt impurity entropy maxDepth 6
17/02/16 16:50:51 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 7 featureSubsetStrategy log2 impurity gini maxDepth 3
17/02/16 16:50:52 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  
1.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  
0.75
numTrees 7 featureSubsetStrategy log2 impurity gini maxDepth 4
17/02/16 16:50:53 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 7 featureSubsetStrategy log2 impurity gini maxDepth 5
17/02/16 16:50:53 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  3.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
1.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.75
numTrees 7 featureSubsetStrategy log2 impurity gini maxDepth 6
17/02/16 16:50:54 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.9375
numTrees 7 featureSubsetStrategy log2 impurity entropy maxDepth 3
17/02/16 16:50:55 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  1.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  
0.75
numTrees 7 featureSubsetStrategy log2 impurity entropy maxDepth 4
17/02/16 16:50:56 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  1.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  
0.8125
numTrees 7 featureSubsetStrategy log2 impurity entropy maxDepth 5
17/02/16 16:50:56 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  2.0  
0.875
numTrees 7 featureSubsetStrategy log2 impurity entropy maxDepth 6
17/02/16 16:50:57 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 7 featureSubsetStrategy onethird impurity gini maxDepth 3
17/02/16 16:50:58 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.75
numTrees 7 featureSubsetStrategy onethird impurity gini maxDepth 4
17/02/16 16:50:58 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.375
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  
1.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  
0.0  0.0  1.0  0.0  1.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.625
numTrees 7 featureSubsetStrategy onethird impurity gini maxDepth 5
17/02/16 16:50:59 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 7 featureSubsetStrategy onethird impurity gini maxDepth 6
17/02/16 16:51:00 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 7 featureSubsetStrategy onethird impurity entropy maxDepth 3
17/02/16 16:51:01 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.875
numTrees 7 featureSubsetStrategy onethird impurity entropy maxDepth 4
17/02/16 16:51:02 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  
0.9375
numTrees 7 featureSubsetStrategy onethird impurity entropy maxDepth 5
17/02/16 16:51:03 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 7 featureSubsetStrategy onethird impurity entropy maxDepth 6
17/02/16 16:51:04 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 8 featureSubsetStrategy all impurity gini maxDepth 3
17/02/16 16:51:05 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.4375
 |=================== Confusion matrix ==========================
0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  
1.0  0.0  3.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  
1.0  0.0  0.0  0.0  2.0  
0.5625
numTrees 8 featureSubsetStrategy all impurity gini maxDepth 4
17/02/16 16:51:06 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.3125
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  3.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
1.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.6875
numTrees 8 featureSubsetStrategy all impurity gini maxDepth 5
17/02/16 16:51:07 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 8 featureSubsetStrategy all impurity gini maxDepth 6
17/02/16 16:51:08 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  1.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  
0.9375
numTrees 8 featureSubsetStrategy all impurity entropy maxDepth 3
17/02/16 16:51:10 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 8 featureSubsetStrategy all impurity entropy maxDepth 4
17/02/16 16:51:11 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.9375
numTrees 8 featureSubsetStrategy all impurity entropy maxDepth 5
17/02/16 16:51:13 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 8 featureSubsetStrategy all impurity entropy maxDepth 6
17/02/16 16:51:14 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 8 featureSubsetStrategy sqrt impurity gini maxDepth 3
17/02/16 16:51:16 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.4375
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  1.0  3.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  
1.0  0.0  0.0  0.0  0.0  0.0  
0.5625
numTrees 8 featureSubsetStrategy sqrt impurity gini maxDepth 4
17/02/16 16:51:17 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  1.0  0.0  1.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  
0.8125
numTrees 8 featureSubsetStrategy sqrt impurity gini maxDepth 5
17/02/16 16:51:18 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 8 featureSubsetStrategy sqrt impurity gini maxDepth 6
17/02/16 16:51:20 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 8 featureSubsetStrategy sqrt impurity entropy maxDepth 3
17/02/16 16:51:21 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.375
 |=================== Confusion matrix ==========================
1.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  2.0  0.0  
1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.625
numTrees 8 featureSubsetStrategy sqrt impurity entropy maxDepth 4
17/02/16 16:51:22 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 8 featureSubsetStrategy sqrt impurity entropy maxDepth 5
17/02/16 16:51:22 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.875
numTrees 8 featureSubsetStrategy sqrt impurity entropy maxDepth 6
17/02/16 16:51:23 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 8 featureSubsetStrategy log2 impurity gini maxDepth 3
17/02/16 16:51:24 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.5
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  1.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  
1.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  
0.5
numTrees 8 featureSubsetStrategy log2 impurity gini maxDepth 4
17/02/16 16:51:24 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 8 featureSubsetStrategy log2 impurity gini maxDepth 5
17/02/16 16:51:25 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
1.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 8 featureSubsetStrategy log2 impurity gini maxDepth 6
17/02/16 16:51:26 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 8 featureSubsetStrategy log2 impurity entropy maxDepth 3
17/02/16 16:51:27 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  2.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.75
numTrees 8 featureSubsetStrategy log2 impurity entropy maxDepth 4
17/02/16 16:51:28 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 8 featureSubsetStrategy log2 impurity entropy maxDepth 5
17/02/16 16:51:29 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 8 featureSubsetStrategy log2 impurity entropy maxDepth 6
17/02/16 16:51:30 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.9375
numTrees 8 featureSubsetStrategy onethird impurity gini maxDepth 3
17/02/16 16:51:31 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 8 featureSubsetStrategy onethird impurity gini maxDepth 4
17/02/16 16:51:32 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  1.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  
1.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  
0.75
numTrees 8 featureSubsetStrategy onethird impurity gini maxDepth 5
17/02/16 16:51:33 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.875
numTrees 8 featureSubsetStrategy onethird impurity gini maxDepth 6
17/02/16 16:51:34 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  
0.8125
numTrees 8 featureSubsetStrategy onethird impurity entropy maxDepth 3
17/02/16 16:51:35 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 8 featureSubsetStrategy onethird impurity entropy maxDepth 4
17/02/16 16:51:36 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 8 featureSubsetStrategy onethird impurity entropy maxDepth 5
17/02/16 16:51:37 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  2.0  
0.9375
numTrees 8 featureSubsetStrategy onethird impurity entropy maxDepth 6
17/02/16 16:51:38 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 9 featureSubsetStrategy all impurity gini maxDepth 3
17/02/16 16:51:39 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.4375
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  1.0  1.0  0.0  0.0  
0.5625
numTrees 9 featureSubsetStrategy all impurity gini maxDepth 4
17/02/16 16:51:40 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.875
numTrees 9 featureSubsetStrategy all impurity gini maxDepth 5
17/02/16 16:51:41 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.875
numTrees 9 featureSubsetStrategy all impurity gini maxDepth 6
17/02/16 16:51:42 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 9 featureSubsetStrategy all impurity entropy maxDepth 3
17/02/16 16:51:44 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  1.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.8125
numTrees 9 featureSubsetStrategy all impurity entropy maxDepth 4
17/02/16 16:51:45 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 9 featureSubsetStrategy all impurity entropy maxDepth 5
17/02/16 16:51:46 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 9 featureSubsetStrategy all impurity entropy maxDepth 6
17/02/16 16:51:47 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 9 featureSubsetStrategy sqrt impurity gini maxDepth 3
17/02/16 16:51:49 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.875
numTrees 9 featureSubsetStrategy sqrt impurity gini maxDepth 4
17/02/16 16:51:50 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  1.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  1.0  
0.75
numTrees 9 featureSubsetStrategy sqrt impurity gini maxDepth 5
17/02/16 16:51:50 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 9 featureSubsetStrategy sqrt impurity gini maxDepth 6
17/02/16 16:51:51 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 9 featureSubsetStrategy sqrt impurity entropy maxDepth 3
17/02/16 16:51:52 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  
0.875
numTrees 9 featureSubsetStrategy sqrt impurity entropy maxDepth 4
17/02/16 16:51:52 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 9 featureSubsetStrategy sqrt impurity entropy maxDepth 5
17/02/16 16:51:53 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 9 featureSubsetStrategy sqrt impurity entropy maxDepth 6
17/02/16 16:51:54 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 9 featureSubsetStrategy log2 impurity gini maxDepth 3
17/02/16 16:51:55 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.375
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  
0.0  3.0  0.0  0.0  0.0  
1.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  2.0  
0.625
numTrees 9 featureSubsetStrategy log2 impurity gini maxDepth 4
17/02/16 16:51:55 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.8125
numTrees 9 featureSubsetStrategy log2 impurity gini maxDepth 5
17/02/16 16:51:56 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 9 featureSubsetStrategy log2 impurity gini maxDepth 6
17/02/16 16:51:57 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 9 featureSubsetStrategy log2 impurity entropy maxDepth 3
17/02/16 16:51:57 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  
0.75
numTrees 9 featureSubsetStrategy log2 impurity entropy maxDepth 4
17/02/16 16:51:58 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.875
numTrees 9 featureSubsetStrategy log2 impurity entropy maxDepth 5
17/02/16 16:51:59 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 9 featureSubsetStrategy log2 impurity entropy maxDepth 6
17/02/16 16:52:00 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 9 featureSubsetStrategy onethird impurity gini maxDepth 3
17/02/16 16:52:01 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.3125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  1.0  0.0  1.0  1.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  
0.6875
numTrees 9 featureSubsetStrategy onethird impurity gini maxDepth 4
17/02/16 16:52:01 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.8125
numTrees 9 featureSubsetStrategy onethird impurity gini maxDepth 5
17/02/16 16:52:02 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 9 featureSubsetStrategy onethird impurity gini maxDepth 6
17/02/16 16:52:03 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 9 featureSubsetStrategy onethird impurity entropy maxDepth 3
17/02/16 16:52:04 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 9 featureSubsetStrategy onethird impurity entropy maxDepth 4
17/02/16 16:52:05 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 9 featureSubsetStrategy onethird impurity entropy maxDepth 5
17/02/16 16:52:07 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 9 featureSubsetStrategy onethird impurity entropy maxDepth 6
17/02/16 16:52:08 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 10 featureSubsetStrategy all impurity gini maxDepth 3
17/02/16 16:52:09 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.3125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.6875
numTrees 10 featureSubsetStrategy all impurity gini maxDepth 4
17/02/16 16:52:10 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.875
numTrees 10 featureSubsetStrategy all impurity gini maxDepth 5
17/02/16 16:52:11 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 10 featureSubsetStrategy all impurity gini maxDepth 6
17/02/16 16:52:12 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 10 featureSubsetStrategy all impurity entropy maxDepth 3
17/02/16 16:52:14 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
1.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 10 featureSubsetStrategy all impurity entropy maxDepth 4
17/02/16 16:52:15 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 10 featureSubsetStrategy all impurity entropy maxDepth 5
17/02/16 16:52:16 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 10 featureSubsetStrategy all impurity entropy maxDepth 6
17/02/16 16:52:17 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 10 featureSubsetStrategy sqrt impurity gini maxDepth 3
17/02/16 16:52:19 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.3125
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.6875
numTrees 10 featureSubsetStrategy sqrt impurity gini maxDepth 4
17/02/16 16:52:20 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 10 featureSubsetStrategy sqrt impurity gini maxDepth 5
17/02/16 16:52:20 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 10 featureSubsetStrategy sqrt impurity gini maxDepth 6
17/02/16 16:52:21 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  1.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 10 featureSubsetStrategy sqrt impurity entropy maxDepth 3
17/02/16 16:52:22 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
1.0  0.0  2.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.875
numTrees 10 featureSubsetStrategy sqrt impurity entropy maxDepth 4
17/02/16 16:52:22 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  2.0  
0.9375
numTrees 10 featureSubsetStrategy sqrt impurity entropy maxDepth 5
17/02/16 16:52:23 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 10 featureSubsetStrategy sqrt impurity entropy maxDepth 6
17/02/16 16:52:24 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 10 featureSubsetStrategy log2 impurity gini maxDepth 3
17/02/16 16:52:25 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.1875
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.8125
numTrees 10 featureSubsetStrategy log2 impurity gini maxDepth 4
17/02/16 16:52:25 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 10 featureSubsetStrategy log2 impurity gini maxDepth 5
17/02/16 16:52:26 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 10 featureSubsetStrategy log2 impurity gini maxDepth 6
17/02/16 16:52:27 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 10 featureSubsetStrategy log2 impurity entropy maxDepth 3
17/02/16 16:52:28 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  1.0  0.0  0.0  
1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.75
numTrees 10 featureSubsetStrategy log2 impurity entropy maxDepth 4
17/02/16 16:52:28 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.125
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
1.0  0.0  1.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.875
numTrees 10 featureSubsetStrategy log2 impurity entropy maxDepth 5
17/02/16 16:52:29 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 10 featureSubsetStrategy log2 impurity entropy maxDepth 6
17/02/16 16:52:30 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 10 featureSubsetStrategy onethird impurity gini maxDepth 3
17/02/16 16:52:30 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.25
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  
1.0  1.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.75
numTrees 10 featureSubsetStrategy onethird impurity gini maxDepth 4
17/02/16 16:52:31 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  
0.9375
numTrees 10 featureSubsetStrategy onethird impurity gini maxDepth 5
17/02/16 16:52:32 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 10 featureSubsetStrategy onethird impurity gini maxDepth 6
17/02/16 16:52:33 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 10 featureSubsetStrategy onethird impurity entropy maxDepth 3
17/02/16 16:52:34 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 10 featureSubsetStrategy onethird impurity entropy maxDepth 4
17/02/16 16:52:35 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0625
 |=================== Confusion matrix ==========================
1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
1.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
0.9375
numTrees 10 featureSubsetStrategy onethird impurity entropy maxDepth 5
17/02/16 16:52:36 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
numTrees 10 featureSubsetStrategy onethird impurity entropy maxDepth 6
17/02/16 16:52:37 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
Test Error = 0.0
 |=================== Confusion matrix ==========================
2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  2.0  0.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  2.0  0.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0  
0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  
1.0
Best Err 0.0
Best params (featureSubsetStrategy,log2) (numTrees,4) (maxDepth,6) (impurity,gini)
17/02/16 16:52:38 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 100 to 47 (= number of training instances)
17/02/16 16:52:39 INFO FileOutputCommitter: Saved output of task 'attempt_201702161652_4785_m_000000_18455' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/nbmodel/metadata/_temporary/0/task_201702161652_4785_m_000000
17/02/16 16:52:39 INFO CodecConfig: Compression: GZIP
17/02/16 16:52:39 INFO ParquetOutputFormat: Parquet block size to 134217728
17/02/16 16:52:39 INFO ParquetOutputFormat: Parquet page size to 1048576
17/02/16 16:52:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
17/02/16 16:52:39 INFO ParquetOutputFormat: Dictionary is on
17/02/16 16:52:39 INFO ParquetOutputFormat: Validation is off
17/02/16 16:52:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
17/02/16 16:52:39 INFO CodecConfig: Compression: GZIP
17/02/16 16:52:39 INFO CodecConfig: Compression: GZIP
17/02/16 16:52:39 INFO ParquetOutputFormat: Parquet block size to 134217728
17/02/16 16:52:39 INFO ParquetOutputFormat: Parquet block size to 134217728
17/02/16 16:52:39 INFO ParquetOutputFormat: Parquet page size to 1048576
17/02/16 16:52:39 INFO ParquetOutputFormat: Parquet page size to 1048576
17/02/16 16:52:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
17/02/16 16:52:39 INFO ParquetOutputFormat: Dictionary is on
17/02/16 16:52:39 INFO ParquetOutputFormat: Validation is off
17/02/16 16:52:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
17/02/16 16:52:39 INFO CodecConfig: Compression: GZIP
17/02/16 16:52:39 INFO ParquetOutputFormat: Parquet block size to 134217728
17/02/16 16:52:39 INFO ParquetOutputFormat: Parquet page size to 1048576
17/02/16 16:52:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
17/02/16 16:52:39 INFO ParquetOutputFormat: Dictionary is on
17/02/16 16:52:39 INFO ParquetOutputFormat: Validation is off
17/02/16 16:52:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
17/02/16 16:52:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
17/02/16 16:52:39 INFO ParquetOutputFormat: Dictionary is on
17/02/16 16:52:39 INFO ParquetOutputFormat: Validation is off
17/02/16 16:52:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
17/02/16 16:52:39 INFO CodecPool: Got brand-new compressor [.gz]
17/02/16 16:52:39 INFO CodecPool: Got brand-new compressor [.gz]
17/02/16 16:52:39 INFO CodecPool: Got brand-new compressor [.gz]
17/02/16 16:52:39 INFO CodecPool: Got brand-new compressor [.gz]
17/02/16 16:52:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 67,180
17/02/16 16:52:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 67,180
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 61B for [treeId] INT32: 19 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 4B raw, 1B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 99B for [nodeId] INT32: 19 values, 82B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 78B for [predict, predict] DOUBLE: 19 values, 17B raw, 37B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 56B raw, 7B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 81B for [predict, prob] DOUBLE: 19 values, 20B raw, 40B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 12 entries, 96B raw, 12B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 81B for [impurity] DOUBLE: 19 values, 20B raw, 40B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 12 entries, 96B raw, 12B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 56B for [isLeaf] BOOLEAN: 19 values, 9B raw, 29B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 89B for [split, feature] INT32: 19 values, 47B raw, 56B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 80B for [split, threshold] DOUBLE: 19 values, 19B raw, 39B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 56B raw, 7B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 66B for [split, featureType] INT32: 19 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 4B raw, 1B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 59B for [split, categories, list, element] DOUBLE: 19 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 83B for [leftNodeId] INT32: 19 values, 44B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 83B for [rightNodeId] INT32: 19 values, 44B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 143B for [infoGain] DOUBLE: 19 values, 80B raw, 100B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 61B for [treeId] INT32: 19 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 4B raw, 1B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 99B for [nodeId] INT32: 19 values, 82B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 79B for [predict, predict] DOUBLE: 19 values, 20B raw, 38B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 9 entries, 72B raw, 9B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 81B for [predict, prob] DOUBLE: 19 values, 20B raw, 40B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 11 entries, 88B raw, 11B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 81B for [impurity] DOUBLE: 19 values, 20B raw, 40B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 11 entries, 88B raw, 11B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 56B for [isLeaf] BOOLEAN: 19 values, 9B raw, 29B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 90B for [split, feature] INT32: 19 values, 47B raw, 57B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 80B for [split, threshold] DOUBLE: 19 values, 19B raw, 39B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 56B raw, 7B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 66B for [split, featureType] INT32: 19 values, 13B raw, 33B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 4B raw, 1B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 59B for [split, categories, list, element] DOUBLE: 19 values, 20B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 83B for [leftNodeId] INT32: 19 values, 44B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 83B for [rightNodeId] INT32: 19 values, 44B raw, 50B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 139B for [infoGain] DOUBLE: 19 values, 80B raw, 96B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO FileOutputCommitter: Saved output of task 'attempt_201702161652_4786_m_000003_0' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/nbmodel/data/_temporary/0/task_201702161652_4786_m_000003
17/02/16 16:52:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 67,648
17/02/16 16:52:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 67,864
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 61B for [treeId] INT32: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 4B raw, 1B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 114B for [nodeId] INT32: 29 values, 122B raw, 79B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 61B for [treeId] INT32: 33 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 4B raw, 1B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 120B for [nodeId] INT32: 33 values, 138B raw, 85B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 88B for [predict, predict] DOUBLE: 33 values, 28B raw, 47B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 10 entries, 80B raw, 10B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 88B for [predict, prob] DOUBLE: 33 values, 28B raw, 47B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 13 entries, 104B raw, 13B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 88B for [impurity] DOUBLE: 33 values, 28B raw, 47B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 14 entries, 112B raw, 14B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 58B for [isLeaf] BOOLEAN: 33 values, 11B raw, 31B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 113B for [split, feature] INT32: 33 values, 79B raw, 78B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 84B for [split, threshold] DOUBLE: 33 values, 23B raw, 43B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 8 entries, 64B raw, 8B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 69B for [split, featureType] INT32: 33 values, 17B raw, 36B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 4B raw, 1B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 64B for [split, categories, list, element] DOUBLE: 33 values, 26B raw, 43B comp, 1 pages, encodings: [RLE, PLAIN]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 103B for [leftNodeId] INT32: 33 values, 74B raw, 68B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 105B for [rightNodeId] INT32: 33 values, 74B raw, 70B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 81B for [infoGain] DOUBLE: 33 values, 20B raw, 40B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 14 entries, 112B raw, 14B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 85B for [predict, predict] DOUBLE: 29 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 9 entries, 72B raw, 9B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 85B for [predict, prob] DOUBLE: 29 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 11 entries, 88B raw, 11B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 85B for [impurity] DOUBLE: 29 values, 24B raw, 44B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 15 entries, 120B raw, 15B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 57B for [isLeaf] BOOLEAN: 29 values, 10B raw, 30B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 107B for [split, feature] INT32: 29 values, 69B raw, 72B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 82B for [split, threshold] DOUBLE: 29 values, 21B raw, 41B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 5 entries, 40B raw, 5B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 68B for [split, featureType] INT32: 29 values, 15B raw, 35B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 4B raw, 1B comp}
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 62B for [split, categories, list, element] DOUBLE: 29 values, 23B raw, 41B comp, 1 pages, encodings: [RLE, PLAIN]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 99B for [leftNodeId] INT32: 29 values, 65B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 97B for [rightNodeId] INT32: 29 values, 65B raw, 63B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO ColumnChunkPageWriteStore: written 170B for [infoGain] DOUBLE: 29 values, 121B raw, 127B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17/02/16 16:52:39 INFO FileOutputCommitter: Saved output of task 'attempt_201702161652_4786_m_000001_0' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/nbmodel/data/_temporary/0/task_201702161652_4786_m_000001
17/02/16 16:52:39 INFO FileOutputCommitter: Saved output of task 'attempt_201702161652_4786_m_000002_0' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/nbmodel/data/_temporary/0/task_201702161652_4786_m_000002
17/02/16 16:52:39 INFO FileOutputCommitter: Saved output of task 'attempt_201702161652_4786_m_000000_0' to file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/model/nbmodel/data/_temporary/0/task_201702161652_4786_m_000000
17/02/16 16:52:39 INFO ParquetFileReader: Initiating action with parallelism: 5
Random Forest Model generated
17/02/16 16:52:39 INFO FileInputFormat: Total input paths to process : 50
17/02/16 16:52:39 INFO FileInputFormat: Total input paths to process : 50
17/02/16 16:52:39 INFO CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 186415
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/apple/1.jpg
17/02/16 16:52:42 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:42 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:42 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:42 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:42 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:42 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:42 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:42 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:42 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:42 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:42 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:42 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:42 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:42 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03773585, 0.0, 0.0, 0.0754717, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03773585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03773585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0754717, 0.0, 0.018867925, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.018867925, 0.16981132, 0.0, 0.018867925, 0.056603774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056603774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.018867925, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018867925, 0.0 ]
--Histogram size : 400
17/02/16 16:52:43 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:43 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:43 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:43 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:43 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:43 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:43 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:52:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:52:43 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:43 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:52:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:52:43 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:43 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:52:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:43 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:43 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:43 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 19
Predicting test image : apple as laptop
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/apple/10.jpg
17/02/16 16:52:44 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:44 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:44 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:44 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:44 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:44 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:44 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:44 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 100
17/02/16 16:52:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:44 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:44 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 100
17/02/16 16:52:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:44 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:44 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:44 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 100
17/02/16 16:52:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:44 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:44 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.016666668, 0.0, 0.0, 0.011111111, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.022222223, 0.0055555557, 0.0055555557, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.016666668, 0.0055555557, 0.0055555557, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0055555557, 0.0055555557, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0055555557, 0.011111111, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0055555557, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0055555557, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.016666668, 0.0055555557, 0.0, 0.0, 0.011111111, 0.0055555557, 0.0055555557, 0.0055555557, 0.011111111, 0.016666668, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.011111111, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.027777778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0055555557, 0.0055555557, 0.0055555557, 0.011111111, 0.0055555557, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.011111111, 0.0, 0.0, 0.0055555557, 0.016666668, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0055555557, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.011111111, 0.0055555557, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0055555557, 0.0055555557, 0.011111111, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0055555557, 0.0, 0.0055555557, 0.0055555557, 0.027777778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.011111111, 0.011111111, 0.016666668, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.011111111, 0.011111111, 0.022222223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.016666668, 0.0, 0.0, 0.016666668, 0.0055555557, 0.011111111, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0055555557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055555557, 0.0055555557 ]
--Histogram size : 400
17/02/16 16:52:45 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:45 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:45 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:45 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:45 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:52:45 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:45 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:52:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:45 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:45 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:52:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:52:45 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:45 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:45 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 33
17/02/16 16:52:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:45 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:45 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : apple as mobile
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/apple/2.jpg
17/02/16 16:52:46 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:46 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:46 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:46 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:46 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:46 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:46 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:46 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:46 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:46 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:46 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:46 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:46 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:46 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0091743115, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.027522935, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0091743115, 0.0, 0.0091743115, 0.0, 0.0, 0.027522935, 0.018348623, 0.0091743115, 0.0091743115, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0091743115, 0.0091743115, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018348623, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0091743115, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0091743115, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.018348623, 0.0, 0.0, 0.036697246, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.07339449, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018348623, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.018348623, 0.0, 0.0, 0.0, 0.018348623, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.018348623, 0.018348623, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0091743115, 0.0091743115, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0091743115, 0.0, 0.0091743115, 0.0, 0.0, 0.0091743115, 0.0, 0.0091743115, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.027522935, 0.0, 0.0, 0.0091743115, 0.018348623, 0.0, 0.0, 0.0091743115, 0.0091743115, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0091743115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.018348623, 0.0, 0.018348623, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0091743115, 0.0, 0.018348623, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:52:46 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:46 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:46 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:46 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:46 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:52:47 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:47 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:47 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:52:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:47 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:47 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:47 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:52:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:52:47 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:47 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:47 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:52:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:47 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:47 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:47 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : apple as bag
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/apple/4.jpg
17/02/16 16:52:47 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:47 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:47 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:47 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:47 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:48 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:48 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:48 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:48 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:48 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:48 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:48 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:48 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:48 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:48 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:48 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:48 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0058139535, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.017441861, 0.0, 0.0, 0.05232558, 0.0058139535, 0.017441861, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017441861, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.0, 0.0058139535, 0.0, 0.0, 0.0058139535, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023255814, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.0058139535, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0058139535, 0.017441861, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.011627907, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0058139535, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.0058139535, 0.0058139535, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.0058139535, 0.0058139535, 0.0, 0.0058139535, 0.029069766, 0.0058139535, 0.0058139535, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023255814, 0.0, 0.0, 0.0, 0.0058139535, 0.011627907, 0.0058139535, 0.023255814, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.0058139535, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.029069766, 0.0, 0.023255814, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023255814, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.0058139535, 0.011627907, 0.017441861, 0.0, 0.0, 0.0058139535, 0.0, 0.011627907, 0.0, 0.0058139535, 0.0058139535, 0.0058139535, 0.0, 0.0, 0.017441861, 0.0058139535, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.017441861, 0.0, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.0, 0.0058139535, 0.0, 0.0, 0.011627907, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017441861, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0, 0.0, 0.011627907, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.0058139535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0058139535, 0.011627907, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:52:48 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:48 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:48 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:48 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:48 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:49 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:49 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:49 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:52:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:49 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:49 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:49 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:52:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:52:49 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:49 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:49 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:52:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:52:49 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:49 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:49 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
Predicting test image : apple as laptop
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/apple/9.jpg
17/02/16 16:52:49 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:49 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:49 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:49 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:49 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:50 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:50 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:50 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:50 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:50 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:50 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:50 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:50 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:50 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:50 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:50 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:50 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:50 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:50 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:50 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:50 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06521739, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02173913, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06521739, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.010869565, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032608695, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.010869565, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.032608695, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.010869565, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08695652, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.010869565, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02173913, 0.0, 0.0, 0.0, 0.0, 0.02173913, 0.0, 0.0, 0.0, 0.02173913, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02173913, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.02173913, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.02173913, 0.0, 0.02173913, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032608695, 0.0, 0.02173913, 0.010869565, 0.010869565, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.010869565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010869565, 0.0, 0.02173913, 0.0 ]
--Histogram size : 400
17/02/16 16:52:50 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:50 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:50 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:50 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:50 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:50 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:50 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:52:50 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:50 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:50 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:52:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:52:51 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:51 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:51 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:52:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:51 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:51 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:51 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:52:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:51 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:51 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:51 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : apple as chocolate
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/bag/1.jpg
17/02/16 16:52:51 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:51 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:51 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:51 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:51 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:51 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:51 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:51 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:51 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:51 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:51 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:51 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:51 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:51 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:51 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:51 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:51 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024691358, 0.012345679, 0.024691358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.024691358, 0.0, 0.0, 0.0, 0.0, 0.024691358, 0.0, 0.024691358, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.024691358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024691358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.024691358, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.012345679, 0.0, 0.0, 0.012345679, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.061728396, 0.0, 0.0, 0.024691358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.024691358, 0.0, 0.0, 0.024691358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024691358, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.012345679, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.037037037, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.024691358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.024691358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.012345679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024691358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024691358, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:52:52 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:52 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:52 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:52 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:52 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:52:52 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:52 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:52 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:52:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:52:52 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:52 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:52 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:52:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:52 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:52 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:52 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:52:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:52 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:52 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:52 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : bag as bag
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/bag/3.jpg
17/02/16 16:52:53 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:53 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:53 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:53 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:53 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:53 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:53 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:53 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:53 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:53 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:53 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:53 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:53 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:53 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:53 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:53 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:53 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.015503876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.03875969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015503876, 0.015503876, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.007751938, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.023255814, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.015503876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.015503876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015503876, 0.0, 0.0, 0.0, 0.031007752, 0.023255814, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.015503876, 0.0, 0.007751938, 0.0, 0.0, 0.023255814, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.015503876, 0.03875969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015503876, 0.0, 0.023255814, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.007751938, 0.0, 0.007751938, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023255814, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015503876, 0.015503876, 0.0, 0.0, 0.0, 0.007751938, 0.007751938, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015503876, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.007751938, 0.007751938, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.023255814, 0.007751938, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015503876, 0.007751938, 0.0, 0.0, 0.0, 0.007751938, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.007751938, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.007751938, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.007751938, 0.007751938, 0.007751938, 0.015503876, 0.0, 0.015503876, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015503876, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.007751938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:52:54 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:54 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:54 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:54 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:54 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:52:54 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:54 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:54 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:52:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:52:54 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:54 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:54 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:52:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:54 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:54 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:54 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:52:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:54 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:54 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:54 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : bag as laptop
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/bag/7.jpg
17/02/16 16:52:54 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:55 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:55 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:55 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:55 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:55 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:55 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:55 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:55 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:55 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:55 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:55 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:55 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:55 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:55 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:55 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:55 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0057803467, 0.0, 0.0, 0.0, 0.0057803467, 0.01734104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.040462427, 0.0, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.011560693, 0.0, 0.0, 0.023121387, 0.0, 0.0, 0.0, 0.01734104, 0.0, 0.0, 0.0, 0.011560693, 0.011560693, 0.0, 0.0057803467, 0.0, 0.0, 0.011560693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011560693, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.011560693, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.03468208, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.011560693, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.011560693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011560693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011560693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011560693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01734104, 0.0, 0.0057803467, 0.01734104, 0.01734104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011560693, 0.011560693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028901733, 0.0, 0.0, 0.0057803467, 0.011560693, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0057803467, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.01734104, 0.0, 0.0, 0.0, 0.011560693, 0.0, 0.011560693, 0.0, 0.0, 0.01734104, 0.0, 0.0, 0.0057803467, 0.0057803467, 0.0, 0.0, 0.011560693, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011560693, 0.0057803467, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.011560693, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.023121387, 0.0, 0.011560693, 0.0, 0.0057803467, 0.0, 0.028901733, 0.0, 0.0, 0.023121387, 0.0, 0.0, 0.0, 0.0, 0.023121387, 0.0, 0.0, 0.0, 0.0057803467, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0057803467, 0.0, 0.0057803467, 0.011560693, 0.0, 0.028901733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0057803467, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011560693, 0.0, 0.0057803467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011560693, 0.0, 0.011560693, 0.0, 0.0, 0.0, 0.011560693, 0.0057803467, 0.0, 0.0, 0.0, 0.01734104, 0.0, 0.0, 0.011560693, 0.0, 0.011560693, 0.0 ]
--Histogram size : 400
17/02/16 16:52:55 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:55 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:55 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:55 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:55 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:56 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:56 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:56 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:52:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:56 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:56 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:56 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:52:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:52:56 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:56 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:56 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:52:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:52:56 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:56 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:56 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
Predicting test image : bag as laptop
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/bag/8.jpg
17/02/16 16:52:56 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:56 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:56 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:56 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:56 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:57 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:57 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:57 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:57 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:57 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:57 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 100
17/02/16 16:52:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:57 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:57 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:57 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:57 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:57 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:57 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.004901961, 0.0, 0.004901961, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.009803922, 0.004901961, 0.004901961, 0.009803922, 0.0, 0.0, 0.014705883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014705883, 0.004901961, 0.004901961, 0.0, 0.004901961, 0.004901961, 0.004901961, 0.0, 0.009803922, 0.0, 0.004901961, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.014705883, 0.039215688, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.034313727, 0.0, 0.0, 0.004901961, 0.009803922, 0.0, 0.014705883, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.004901961, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.004901961, 0.004901961, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.029411767, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.014705883, 0.004901961, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.019607844, 0.0, 0.014705883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014705883, 0.0, 0.014705883, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.034313727, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.004901961, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.014705883, 0.004901961, 0.0, 0.0, 0.0, 0.004901961, 0.004901961, 0.024509804, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.009803922, 0.004901961, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.004901961, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.004901961, 0.004901961, 0.004901961, 0.004901961, 0.004901961, 0.014705883, 0.0, 0.0, 0.019607844, 0.004901961, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.009803922, 0.004901961, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.009803922, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.004901961, 0.009803922, 0.004901961, 0.004901961, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.009803922, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.004901961, 0.019607844, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004901961, 0.0, 0.0, 0.004901961, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.029411767, 0.0 ]
--Histogram size : 400
17/02/16 16:52:57 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:57 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:57 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:57 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:57 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:52:58 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:58 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:58 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:52:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:58 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:58 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:58 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:52:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:52:58 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:58 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:58 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:52:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:52:58 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:58 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:58 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : bag as chocolate
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/bag/9.jpg
17/02/16 16:52:58 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:58 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:58 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:58 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:58 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:59 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:59 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:59 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:59 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:59 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:59 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:52:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:59 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:52:59 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:52:59 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:59 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:52:59 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:52:59 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.00862069, 0.00862069, 0.0, 0.00862069, 0.00862069, 0.0, 0.0, 0.0, 0.0, 0.00862069, 0.00862069, 0.00862069, 0.00862069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00862069, 0.00862069, 0.0, 0.025862068, 0.01724138, 0.0, 0.00862069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025862068, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.0, 0.00862069, 0.00862069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.0, 0.01724138, 0.025862068, 0.0, 0.00862069, 0.0, 0.01724138, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.0, 0.01724138, 0.01724138, 0.0, 0.03448276, 0.00862069, 0.00862069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00862069, 0.00862069, 0.00862069, 0.0, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00862069, 0.00862069, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.025862068, 0.00862069, 0.0, 0.0, 0.01724138, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025862068, 0.0, 0.00862069, 0.00862069, 0.0, 0.0, 0.01724138, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025862068, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01724138, 0.04310345, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.01724138, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.01724138, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.00862069, 0.0, 0.00862069, 0.0, 0.00862069, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.00862069, 0.00862069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01724138, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.00862069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.00862069, 0.00862069, 0.00862069, 0.0, 0.01724138, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01724138, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00862069, 0.00862069, 0.01724138, 0.0, 0.0, 0.0, 0.00862069, 0.0, 0.00862069, 0.0, 0.0, 0.00862069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01724138, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:52:59 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:52:59 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:59 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:59 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:52:59 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:00 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:00 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:00 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:00 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:00 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:00 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:00 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:00 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:00 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:00 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:00 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:00 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
Predicting test image : bag as bag
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/books/10.jpg
17/02/16 16:53:00 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:00 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:00 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:00 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:00 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:00 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:00 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:00 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:01 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:01 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:01 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:01 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:01 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:01 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.015625, 0.03125, 0.015625, 0.0, 0.0, 0.0, 0.046875, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.109375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.03125, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046875, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.03125, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0 ]
--Histogram size : 400
17/02/16 16:53:01 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:01 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:01 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:01 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:01 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:01 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:01 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:01 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:01 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:01 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:01 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:01 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:01 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : books as apple
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/books/2.jpg
17/02/16 16:53:02 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:02 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:02 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:02 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:02 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:02 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:02 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:02 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:02 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:02 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:02 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:02 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:02 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:02 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:02 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:02 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:02 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023255814, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.058139533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034883723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023255814, 0.0, 0.011627907, 0.023255814, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023255814, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034883723, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.046511628, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.10465116, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046511628, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034883723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.011627907, 0.0, 0.023255814, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.011627907, 0.0, 0.023255814, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.023255814, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.011627907, 0.0, 0.011627907, 0.0, 0.023255814, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034883723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.023255814, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.011627907, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.011627907, 0.0, 0.0, 0.0, 0.0, 0.011627907, 0.0 ]
--Histogram size : 400
17/02/16 16:53:02 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:03 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:03 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:03 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:03 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:03 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:03 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:03 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:03 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:03 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:03 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:03 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:03 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:03 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:03 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:03 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:03 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : books as chocolate
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/books/5.jpg
17/02/16 16:53:04 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:04 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:04 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:04 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:04 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:04 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:04 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:04 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:04 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:04 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:04 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 100
17/02/16 16:53:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:04 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:04 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 100
17/02/16 16:53:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:04 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:04 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:04 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.011111111, 0.0, 0.011111111, 0.011111111, 0.0, 0.0, 0.0, 0.011111111, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033333335, 0.0, 0.022222223, 0.0, 0.011111111, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.033333335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044444446, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.022222223, 0.0, 0.011111111, 0.0, 0.011111111, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.022222223, 0.0, 0.022222223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033333335, 0.011111111, 0.0, 0.0, 0.0, 0.033333335, 0.0, 0.022222223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.022222223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022222223, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022222223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044444446, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011111111, 0.0, 0.0, 0.011111111, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.033333335, 0.011111111, 0.0, 0.0, 0.0, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:53:05 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:05 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:05 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:05 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:05 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:05 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:05 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:05 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:05 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:05 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:05 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:05 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:05 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:05 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:05 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:05 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:05 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
Predicting test image : books as laptop
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/books/6.jpg
17/02/16 16:53:06 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:06 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:06 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:06 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:06 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:06 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:06 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:06 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:06 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:06 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:06 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:06 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:06 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:06 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:06 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:06 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 100
17/02/16 16:53:06 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.057142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.042857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.042857144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.042857144, 0.0, 0.014285714, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.057142857, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.014285714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.0 ]
--Histogram size : 400
17/02/16 16:53:06 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:07 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:07 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:07 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:07 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:07 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:07 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:07 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:07 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:07 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:07 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:07 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:07 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:07 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:07 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:07 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:07 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : books as trees
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/books/7.jpg
17/02/16 16:53:07 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:07 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:07 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:07 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:07 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:08 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:08 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:08 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:08 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:08 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:08 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:08 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:08 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:08 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:08 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:08 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:08 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.005882353, 0.0, 0.005882353, 0.0, 0.005882353, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.011764706, 0.005882353, 0.0, 0.0, 0.011764706, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.011764706, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.005882353, 0.0, 0.005882353, 0.0, 0.0, 0.011764706, 0.0, 0.0, 0.0, 0.0, 0.07647059, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.023529412, 0.0, 0.011764706, 0.0, 0.005882353, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01764706, 0.011764706, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.011764706, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011764706, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.005882353, 0.005882353, 0.005882353, 0.011764706, 0.011764706, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.005882353, 0.005882353, 0.0, 0.005882353, 0.0, 0.0, 0.005882353, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.011764706, 0.0, 0.005882353, 0.011764706, 0.0, 0.0, 0.005882353, 0.005882353, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.01764706, 0.0, 0.0, 0.011764706, 0.0, 0.005882353, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.011764706, 0.0, 0.01764706, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.005882353, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.011764706, 0.005882353, 0.0, 0.0, 0.011764706, 0.011764706, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.011764706, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.011764706, 0.0, 0.0, 0.0, 0.011764706, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.011764706, 0.0, 0.011764706, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.005882353, 0.0, 0.0, 0.0, 0.005882353, 0.005882353, 0.0, 0.0, 0.005882353, 0.005882353, 0.0, 0.0, 0.0, 0.011764706, 0.005882353, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.011764706, 0.005882353, 0.0, 0.0, 0.029411765, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.005882353, 0.0, 0.005882353, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01764706, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.011764706, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01764706, 0.0, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01764706, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005882353, 0.0, 0.011764706, 0.005882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011764706, 0.0, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:53:08 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:08 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:08 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:08 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:08 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:09 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:09 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:09 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:09 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:09 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:09 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:09 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:09 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:09 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:09 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:09 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:09 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
[Stage 4891:==============>                                         (1 + 3) / 4]Predicting test image : books as apple
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/cake/1.jpg
17/02/16 16:53:10 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:10 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:10 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:10 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:10 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:10 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:10 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:10 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:10 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:10 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:10 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:10 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:10 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:10 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:10 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:10 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:10 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.002016129, 0.0, 0.006048387, 0.002016129, 0.0, 0.004032258, 0.0, 0.002016129, 0.0, 0.0, 0.0, 0.002016129, 0.008064516, 0.0, 0.0, 0.0, 0.0, 0.002016129, 0.004032258, 0.004032258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006048387, 0.0, 0.0, 0.008064516, 0.008064516, 0.006048387, 0.0, 0.004032258, 0.0, 0.002016129, 0.002016129, 0.002016129, 0.0, 0.0, 0.0, 0.0, 0.024193548, 0.002016129, 0.0, 0.0, 0.002016129, 0.002016129, 0.004032258, 0.012096774, 0.006048387, 0.004032258, 0.002016129, 0.01814516, 0.0, 0.002016129, 0.006048387, 0.002016129, 0.004032258, 0.0, 0.002016129, 0.0, 0.006048387, 0.0, 0.010080645, 0.004032258, 0.004032258, 0.004032258, 0.002016129, 0.004032258, 0.004032258, 0.0, 0.0, 0.004032258, 0.0, 0.0, 0.004032258, 0.004032258, 0.004032258, 0.006048387, 0.0, 0.012096774, 0.002016129, 0.002016129, 0.004032258, 0.012096774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004032258, 0.002016129, 0.002016129, 0.0, 0.004032258, 0.0, 0.002016129, 0.0, 0.002016129, 0.0, 0.0, 0.002016129, 0.002016129, 0.002016129, 0.002016129, 0.002016129, 0.0, 0.004032258, 0.004032258, 0.0, 0.004032258, 0.0, 0.002016129, 0.002016129, 0.004032258, 0.0, 0.0, 0.002016129, 0.002016129, 0.002016129, 0.0, 0.0, 0.006048387, 0.006048387, 0.002016129, 0.004032258, 0.004032258, 0.0, 0.004032258, 0.002016129, 0.0, 0.0, 0.004032258, 0.004032258, 0.0, 0.0, 0.002016129, 0.0, 0.002016129, 0.0, 0.004032258, 0.002016129, 0.0, 0.004032258, 0.002016129, 0.002016129, 0.012096774, 0.002016129, 0.0, 0.004032258, 0.002016129, 0.002016129, 0.004032258, 0.006048387, 0.0, 0.002016129, 0.002016129, 0.004032258, 0.004032258, 0.002016129, 0.004032258, 0.006048387, 0.002016129, 0.0, 0.004032258, 0.002016129, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002016129, 0.008064516, 0.002016129, 0.0, 0.0, 0.002016129, 0.004032258, 0.0, 0.006048387, 0.004032258, 0.002016129, 0.0, 0.002016129, 0.002016129, 0.0, 0.002016129, 0.006048387, 0.0, 0.0, 0.0, 0.002016129, 0.0, 0.004032258, 0.0, 0.0, 0.002016129, 0.004032258, 0.0, 0.002016129, 0.002016129, 0.0, 0.002016129, 0.010080645, 0.0, 0.006048387, 0.002016129, 0.002016129, 0.010080645, 0.004032258, 0.0, 0.002016129, 0.0, 0.002016129, 0.008064516, 0.0, 0.0, 0.0, 0.0, 0.002016129, 0.008064516, 0.0, 0.0, 0.002016129, 0.004032258, 0.0, 0.008064516, 0.002016129, 0.002016129, 0.004032258, 0.004032258, 0.006048387, 0.004032258, 0.002016129, 0.002016129, 0.0, 0.0, 0.0, 0.0, 0.002016129, 0.0, 0.002016129, 0.002016129, 0.002016129, 0.0, 0.0, 0.004032258, 0.002016129, 0.0, 0.002016129, 0.002016129, 0.010080645, 0.0, 0.002016129, 0.0, 0.004032258, 0.002016129, 0.0, 0.0, 0.0, 0.0, 0.002016129, 0.002016129, 0.004032258, 0.002016129, 0.002016129, 0.0, 0.0, 0.004032258, 0.0, 0.0, 0.0, 0.012096774, 0.0, 0.0, 0.0, 0.002016129, 0.0, 0.002016129, 0.002016129, 0.0, 0.008064516, 0.002016129, 0.010080645, 0.004032258, 0.0, 0.0, 0.006048387, 0.004032258, 0.006048387, 0.006048387, 0.002016129, 0.010080645, 0.0, 0.006048387, 0.008064516, 0.004032258, 0.0, 0.004032258, 0.0, 0.002016129, 0.004032258, 0.004032258, 0.0, 0.004032258, 0.002016129, 0.002016129, 0.008064516, 0.006048387, 0.0, 0.002016129, 0.002016129, 0.006048387, 0.006048387, 0.008064516, 0.002016129, 0.0, 0.0, 0.002016129, 0.0, 0.002016129, 0.0, 0.002016129, 0.0, 0.0, 0.0, 0.006048387, 0.002016129, 0.004032258, 0.004032258, 0.0, 0.002016129, 0.004032258, 0.0, 0.004032258, 0.0, 0.006048387, 0.0, 0.008064516, 0.002016129, 0.014112903, 0.006048387, 0.0, 0.0, 0.002016129, 0.0, 0.0, 0.002016129, 0.006048387, 0.006048387, 0.0, 0.004032258, 0.006048387, 0.0, 0.004032258, 0.0, 0.002016129, 0.004032258, 0.0, 0.004032258, 0.002016129, 0.0, 0.008064516, 0.002016129, 0.004032258, 0.0, 0.0, 0.002016129, 0.012096774, 0.0, 0.006048387, 0.004032258, 0.002016129, 0.002016129, 0.002016129, 0.0, 0.002016129, 0.006048387, 0.002016129, 0.004032258, 0.0, 0.0, 0.0, 0.0, 0.002016129, 0.004032258, 0.004032258, 0.004032258, 0.0, 0.010080645, 0.0, 0.002016129, 0.002016129, 0.004032258, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:53:11 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:11 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:11 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:11 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:11 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:11 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:11 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:11 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:11 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:11 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:11 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:11 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:11 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:11 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:11 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:11 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:11 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : cake as trees
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/cake/2.jpg
17/02/16 16:53:12 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:12 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:12 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:12 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:12 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:13 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:13 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:13 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:13 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:13 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:13 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:13 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:13 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:13 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:13 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:13 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:13 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.008602151, 0.0, 0.0064516133, 0.0043010754, 0.010752688, 0.0021505377, 0.0021505377, 0.0, 0.0021505377, 0.0, 0.0021505377, 0.0, 0.0, 0.0, 0.0021505377, 0.0021505377, 0.0043010754, 0.0, 0.0, 0.0043010754, 0.0043010754, 0.0, 0.0, 0.0043010754, 0.0021505377, 0.0, 0.0, 0.0021505377, 0.0043010754, 0.0021505377, 0.0021505377, 0.0021505377, 0.0043010754, 0.0021505377, 0.0043010754, 0.0043010754, 0.0, 0.0064516133, 0.0, 0.0, 0.0, 0.0021505377, 0.0021505377, 0.0, 0.0043010754, 0.0021505377, 0.0, 0.0043010754, 0.0, 0.0021505377, 0.0, 0.0, 0.0043010754, 0.0064516133, 0.0021505377, 0.012903227, 0.0, 0.0, 0.010752688, 0.0, 0.0021505377, 0.0021505377, 0.012903227, 0.0, 0.0, 0.008602151, 0.0021505377, 0.008602151, 0.0, 0.0021505377, 0.0064516133, 0.0043010754, 0.0021505377, 0.0, 0.0021505377, 0.0043010754, 0.0, 0.0, 0.0, 0.012903227, 0.0, 0.0021505377, 0.0043010754, 0.008602151, 0.0043010754, 0.0, 0.0, 0.0064516133, 0.0, 0.0043010754, 0.0, 0.0021505377, 0.0, 0.0021505377, 0.0, 0.0021505377, 0.0021505377, 0.0, 0.0043010754, 0.0043010754, 0.0, 0.0043010754, 0.0021505377, 0.0, 0.0, 0.0021505377, 0.0, 0.0021505377, 0.0021505377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0064516133, 0.0021505377, 0.0043010754, 0.0, 0.0, 0.0043010754, 0.0, 0.0043010754, 0.0, 0.0, 0.0021505377, 0.0064516133, 0.0021505377, 0.0, 0.0021505377, 0.0, 0.0021505377, 0.0, 0.0, 0.0043010754, 0.0021505377, 0.0064516133, 0.0, 0.0, 0.0021505377, 0.012903227, 0.0021505377, 0.0, 0.0064516133, 0.0021505377, 0.0, 0.0, 0.0064516133, 0.010752688, 0.0021505377, 0.0, 0.0, 0.0021505377, 0.0021505377, 0.0021505377, 0.0064516133, 0.0043010754, 0.0021505377, 0.0, 0.0021505377, 0.0021505377, 0.0021505377, 0.0043010754, 0.0, 0.0043010754, 0.017204301, 0.0021505377, 0.0, 0.0, 0.0, 0.0064516133, 0.0043010754, 0.0, 0.0021505377, 0.0043010754, 0.0, 0.0043010754, 0.0043010754, 0.0, 0.0043010754, 0.0021505377, 0.0021505377, 0.0021505377, 0.0021505377, 0.0064516133, 0.0, 0.0, 0.0, 0.0021505377, 0.0021505377, 0.0, 0.010752688, 0.0, 0.0021505377, 0.0043010754, 0.0021505377, 0.0021505377, 0.0, 0.0, 0.0, 0.0021505377, 0.0, 0.0021505377, 0.0021505377, 0.0, 0.0, 0.0021505377, 0.0064516133, 0.012903227, 0.0064516133, 0.0043010754, 0.0, 0.0021505377, 0.0, 0.0021505377, 0.0064516133, 0.0043010754, 0.0043010754, 0.0043010754, 0.0, 0.0064516133, 0.0, 0.0, 0.0, 0.017204301, 0.0021505377, 0.0, 0.0021505377, 0.0043010754, 0.0064516133, 0.0043010754, 0.0021505377, 0.0, 0.0021505377, 0.008602151, 0.0, 0.0043010754, 0.0043010754, 0.0043010754, 0.0, 0.0021505377, 0.0, 0.0, 0.0021505377, 0.0, 0.0064516133, 0.0, 0.0021505377, 0.0021505377, 0.0043010754, 0.0021505377, 0.0021505377, 0.010752688, 0.0, 0.0, 0.0043010754, 0.0, 0.0021505377, 0.0, 0.0, 0.0021505377, 0.0043010754, 0.0, 0.008602151, 0.0021505377, 0.0, 0.0, 0.0, 0.0021505377, 0.0021505377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0043010754, 0.0, 0.0, 0.0021505377, 0.0, 0.0064516133, 0.008602151, 0.0021505377, 0.0, 0.0, 0.0, 0.0064516133, 0.0021505377, 0.0, 0.008602151, 0.0021505377, 0.0, 0.0, 0.0021505377, 0.0021505377, 0.0064516133, 0.0064516133, 0.0043010754, 0.0043010754, 0.0021505377, 0.0, 0.0021505377, 0.0043010754, 0.0, 0.008602151, 0.0021505377, 0.0021505377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0021505377, 0.0, 0.0021505377, 0.0, 0.0021505377, 0.0021505377, 0.0, 0.0064516133, 0.0021505377, 0.0021505377, 0.0064516133, 0.0, 0.010752688, 0.010752688, 0.0021505377, 0.0021505377, 0.0021505377, 0.0021505377, 0.0, 0.0021505377, 0.0043010754, 0.0, 0.0021505377, 0.0064516133, 0.0, 0.0, 0.0, 0.0021505377, 0.0, 0.0021505377, 0.0, 0.0021505377, 0.008602151, 0.0, 0.0021505377, 0.0, 0.0, 0.0, 0.0021505377, 0.0021505377, 0.008602151, 0.0, 0.0021505377, 0.0043010754, 0.0021505377, 0.008602151, 0.0, 0.0043010754, 0.0, 0.0, 0.0, 0.0064516133, 0.0, 0.0021505377, 0.0021505377, 0.0021505377, 0.0, 0.0, 0.0043010754, 0.0021505377, 0.008602151, 0.0, 0.0064516133, 0.0021505377, 0.0021505377, 0.0064516133, 0.0021505377, 0.0, 0.0021505377, 0.0021505377, 0.0064516133, 0.0, 0.0064516133, 0.0021505377, 0.008602151, 0.0, 0.0043010754, 0.0, 0.0021505377, 0.0, 0.0, 0.0021505377, 0.0021505377, 0.0, 0.0021505377, 0.0021505377, 0.0021505377, 0.0 ]
--Histogram size : 400
17/02/16 16:53:13 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:13 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:13 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:13 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:13 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:14 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:14 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:14 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:14 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:14 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:14 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:14 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:14 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:14 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:14 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 33
17/02/16 16:53:14 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:14 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 19
                                                                                Predicting test image : cake as chocolate
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/cake/3.jpg
17/02/16 16:53:14 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:14 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:14 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:14 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:14 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:15 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:15 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:15 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:15 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:15 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:15 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:15 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:15 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:15 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:15 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:15 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:15 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.001754386, 0.001754386, 0.001754386, 0.001754386, 0.005263158, 0.0, 0.003508772, 0.0, 0.003508772, 0.001754386, 0.003508772, 0.0, 0.001754386, 0.0, 0.00877193, 0.001754386, 0.0, 0.0, 0.003508772, 0.005263158, 0.003508772, 0.0, 0.0, 0.012280702, 0.003508772, 0.0, 0.003508772, 0.0, 0.001754386, 0.001754386, 0.0, 0.003508772, 0.0, 0.0, 0.001754386, 0.001754386, 0.005263158, 0.007017544, 0.0, 0.0, 0.003508772, 0.003508772, 0.0, 0.00877193, 0.003508772, 0.003508772, 0.001754386, 0.001754386, 0.007017544, 0.001754386, 0.007017544, 0.003508772, 0.0, 0.0, 0.0, 0.005263158, 0.0, 0.003508772, 0.0, 0.001754386, 0.0, 0.007017544, 0.001754386, 0.0, 0.001754386, 0.0, 0.0, 0.001754386, 0.0, 0.003508772, 0.007017544, 0.001754386, 0.003508772, 0.0, 0.001754386, 0.0, 0.003508772, 0.001754386, 0.001754386, 0.0, 0.005263158, 0.0, 0.014035088, 0.003508772, 0.0, 0.0, 0.005263158, 0.0, 0.003508772, 0.0, 0.010526316, 0.0, 0.0, 0.0, 0.001754386, 0.003508772, 0.003508772, 0.0, 0.003508772, 0.0, 0.003508772, 0.003508772, 0.001754386, 0.001754386, 0.007017544, 0.0, 0.012280702, 0.0, 0.0, 0.00877193, 0.0, 0.001754386, 0.014035088, 0.0, 0.001754386, 0.001754386, 0.001754386, 0.001754386, 0.003508772, 0.0, 0.001754386, 0.001754386, 0.0, 0.019298246, 0.001754386, 0.0, 0.012280702, 0.003508772, 0.001754386, 0.0, 0.005263158, 0.0, 0.0, 0.0, 0.001754386, 0.001754386, 0.0, 0.001754386, 0.0, 0.0, 0.010526316, 0.0, 0.0, 0.0, 0.001754386, 0.0, 0.003508772, 0.0, 0.001754386, 0.001754386, 0.005263158, 0.003508772, 0.0, 0.0, 0.001754386, 0.003508772, 0.005263158, 0.001754386, 0.015789473, 0.005263158, 0.003508772, 0.00877193, 0.0, 0.0, 0.0, 0.003508772, 0.005263158, 0.0, 0.0, 0.0, 0.0, 0.00877193, 0.001754386, 0.0, 0.0, 0.0, 0.003508772, 0.005263158, 0.001754386, 0.003508772, 0.001754386, 0.001754386, 0.001754386, 0.0, 0.003508772, 0.0, 0.00877193, 0.0, 0.003508772, 0.003508772, 0.0, 0.0, 0.001754386, 0.001754386, 0.003508772, 0.0, 0.003508772, 0.001754386, 0.0, 0.0, 0.0, 0.0, 0.001754386, 0.001754386, 0.003508772, 0.001754386, 0.0, 0.0, 0.0, 0.003508772, 0.003508772, 0.003508772, 0.0, 0.0, 0.0, 0.003508772, 0.007017544, 0.001754386, 0.0, 0.001754386, 0.0, 0.001754386, 0.001754386, 0.012280702, 0.003508772, 0.0, 0.001754386, 0.007017544, 0.0, 0.0, 0.0, 0.001754386, 0.003508772, 0.0, 0.0, 0.010526316, 0.0, 0.003508772, 0.001754386, 0.0, 0.0, 0.0, 0.003508772, 0.001754386, 0.0, 0.001754386, 0.001754386, 0.001754386, 0.00877193, 0.0, 0.001754386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003508772, 0.0, 0.0, 0.0, 0.001754386, 0.0, 0.0, 0.001754386, 0.0, 0.0, 0.001754386, 0.005263158, 0.001754386, 0.0, 0.0, 0.0, 0.0, 0.001754386, 0.0, 0.003508772, 0.0, 0.001754386, 0.007017544, 0.003508772, 0.0, 0.0, 0.001754386, 0.0, 0.001754386, 0.005263158, 0.0, 0.001754386, 0.007017544, 0.0, 0.0, 0.0, 0.001754386, 0.005263158, 0.001754386, 0.0, 0.005263158, 0.01754386, 0.005263158, 0.001754386, 0.001754386, 0.001754386, 0.001754386, 0.00877193, 0.0, 0.003508772, 0.005263158, 0.001754386, 0.001754386, 0.003508772, 0.0, 0.01754386, 0.0, 0.007017544, 0.003508772, 0.0, 0.001754386, 0.00877193, 0.010526316, 0.001754386, 0.005263158, 0.005263158, 0.0, 0.0, 0.010526316, 0.001754386, 0.003508772, 0.007017544, 0.005263158, 0.001754386, 0.0, 0.001754386, 0.001754386, 0.014035088, 0.003508772, 0.0, 0.0, 0.0, 0.001754386, 0.003508772, 0.0, 0.001754386, 0.0, 0.003508772, 0.0, 0.0, 0.0, 0.0, 0.001754386, 0.00877193, 0.001754386, 0.0, 0.001754386, 0.003508772, 0.0, 0.0, 0.0, 0.003508772, 0.001754386, 0.0, 0.0, 0.001754386, 0.001754386, 0.0, 0.003508772, 0.005263158, 0.003508772, 0.0, 0.005263158, 0.003508772, 0.0, 0.015789473, 0.0, 0.007017544, 0.0, 0.0, 0.001754386, 0.001754386, 0.007017544, 0.001754386, 0.005263158, 0.003508772, 0.0, 0.007017544, 0.0, 0.010526316, 0.0, 0.0, 0.019298246, 0.001754386, 0.005263158, 0.0, 0.0, 0.0, 0.0, 0.005263158, 0.003508772, 0.001754386, 0.00877193 ]
--Histogram size : 400
17/02/16 16:53:15 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:15 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:15 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:15 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:15 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:16 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:16 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:16 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:16 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:16 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:16 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:16 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:16 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:16 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:16 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:16 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:16 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
Predicting test image : cake as chocolate
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/cake/6.jpg
17/02/16 16:53:16 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:16 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:16 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:16 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:16 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:17 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:17 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:17 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:17 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:17 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:17 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:17 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:17 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:17 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:17 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:17 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:17 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0068965517, 0.0068965517, 0.0068965517, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0068965517, 0.0, 0.013793103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0068965517, 0.0, 0.0, 0.0068965517, 0.0, 0.027586207, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0068965517, 0.0068965517, 0.0068965517, 0.0068965517, 0.0068965517, 0.0068965517, 0.0, 0.0068965517, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.013793103, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0068965517, 0.0, 0.0068965517, 0.0068965517, 0.0068965517, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.013793103, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0068965517, 0.0, 0.0, 0.0068965517, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0068965517, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013793103, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03448276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.013793103, 0.0, 0.013793103, 0.0, 0.0, 0.0068965517, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0068965517, 0.0068965517, 0.0, 0.0068965517, 0.0, 0.0, 0.0068965517, 0.0, 0.013793103, 0.0, 0.0, 0.013793103, 0.020689655, 0.0068965517, 0.0068965517, 0.0, 0.03448276, 0.020689655, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.013793103, 0.013793103, 0.0, 0.013793103, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013793103, 0.0, 0.013793103, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0068965517, 0.0068965517, 0.020689655, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013793103, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0068965517, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.013793103, 0.0, 0.0, 0.0, 0.013793103, 0.0, 0.0, 0.0068965517, 0.0, 0.013793103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0068965517, 0.0068965517, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013793103, 0.0, 0.0, 0.0068965517, 0.0, 0.0068965517, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.013793103, 0.0, 0.0, 0.0, 0.0068965517, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:53:17 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:17 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:17 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:17 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:17 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:17 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:17 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:17 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:17 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:17 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:17 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:17 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:17 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:17 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:17 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:17 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:17 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
Predicting test image : cake as apple
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/cake/7.jpg
17/02/16 16:53:18 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:18 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:18 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:18 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:18 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:18 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:18 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:18 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:18 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:18 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:18 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:18 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 100
17/02/16 16:53:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:18 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:18 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:18 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.008426966, 0.0056179776, 0.0028089888, 0.0028089888, 0.0028089888, 0.0, 0.008426966, 0.0, 0.0, 0.0, 0.0056179776, 0.008426966, 0.0, 0.0, 0.0, 0.011235955, 0.0, 0.0, 0.0, 0.0028089888, 0.011235955, 0.0, 0.0028089888, 0.0, 0.0056179776, 0.0028089888, 0.0028089888, 0.0, 0.0, 0.0028089888, 0.0028089888, 0.0056179776, 0.0056179776, 0.0, 0.0, 0.014044944, 0.0, 0.0056179776, 0.0, 0.0, 0.0, 0.0028089888, 0.0028089888, 0.0, 0.0, 0.0056179776, 0.0028089888, 0.0028089888, 0.0028089888, 0.0, 0.0028089888, 0.014044944, 0.0028089888, 0.008426966, 0.0, 0.0056179776, 0.0028089888, 0.0028089888, 0.0, 0.0028089888, 0.0056179776, 0.0, 0.0028089888, 0.0028089888, 0.0028089888, 0.0, 0.0, 0.0, 0.0, 0.008426966, 0.0, 0.011235955, 0.0, 0.008426966, 0.0, 0.008426966, 0.0, 0.0028089888, 0.0028089888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0028089888, 0.0056179776, 0.0, 0.0028089888, 0.0, 0.0056179776, 0.0, 0.0, 0.0, 0.0, 0.0028089888, 0.008426966, 0.0, 0.0, 0.0, 0.0028089888, 0.0, 0.0, 0.0, 0.0, 0.0028089888, 0.0, 0.0028089888, 0.0, 0.008426966, 0.0, 0.0, 0.0028089888, 0.0, 0.0, 0.0028089888, 0.0028089888, 0.0, 0.0, 0.0, 0.0028089888, 0.0, 0.0, 0.0, 0.008426966, 0.011235955, 0.008426966, 0.0, 0.0, 0.0028089888, 0.0028089888, 0.011235955, 0.0028089888, 0.0, 0.0028089888, 0.0028089888, 0.0, 0.0, 0.0, 0.0, 0.0028089888, 0.0028089888, 0.0, 0.0, 0.0028089888, 0.0028089888, 0.008426966, 0.0, 0.0028089888, 0.0028089888, 0.0, 0.0028089888, 0.0, 0.0, 0.0028089888, 0.011235955, 0.0056179776, 0.0056179776, 0.0, 0.0028089888, 0.0, 0.008426966, 0.0, 0.011235955, 0.0028089888, 0.0028089888, 0.0028089888, 0.0, 0.0028089888, 0.0, 0.0, 0.0028089888, 0.0, 0.0028089888, 0.0, 0.0056179776, 0.0, 0.0028089888, 0.0056179776, 0.0028089888, 0.0028089888, 0.0028089888, 0.0, 0.0, 0.0, 0.0, 0.0056179776, 0.0, 0.0, 0.0056179776, 0.0028089888, 0.0, 0.0056179776, 0.0, 0.0, 0.0028089888, 0.0028089888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0028089888, 0.0028089888, 0.0, 0.0056179776, 0.0028089888, 0.016853932, 0.0, 0.0, 0.0, 0.0028089888, 0.0, 0.0056179776, 0.0056179776, 0.0, 0.0, 0.0028089888, 0.0028089888, 0.0028089888, 0.0028089888, 0.0, 0.0, 0.0028089888, 0.0, 0.0, 0.0, 0.0, 0.0056179776, 0.0028089888, 0.0, 0.0, 0.0028089888, 0.0, 0.0028089888, 0.0, 0.0056179776, 0.0, 0.0, 0.0, 0.0028089888, 0.0028089888, 0.0028089888, 0.0, 0.0028089888, 0.0, 0.0, 0.0028089888, 0.0028089888, 0.0, 0.0, 0.0028089888, 0.0, 0.0, 0.016853932, 0.0, 0.008426966, 0.0, 0.0, 0.0028089888, 0.0028089888, 0.0, 0.0056179776, 0.008426966, 0.008426966, 0.0, 0.0028089888, 0.0056179776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0056179776, 0.0028089888, 0.0028089888, 0.0, 0.0, 0.0, 0.011235955, 0.0028089888, 0.0056179776, 0.0028089888, 0.0, 0.0, 0.0056179776, 0.008426966, 0.0, 0.011235955, 0.0, 0.0028089888, 0.0, 0.0028089888, 0.0028089888, 0.0, 0.008426966, 0.0056179776, 0.0028089888, 0.0, 0.028089888, 0.0, 0.0, 0.0, 0.0, 0.0056179776, 0.0028089888, 0.0028089888, 0.0, 0.0, 0.0, 0.0028089888, 0.0, 0.0, 0.0, 0.0028089888, 0.0, 0.0028089888, 0.008426966, 0.0028089888, 0.0, 0.0028089888, 0.0, 0.0, 0.0028089888, 0.0, 0.008426966, 0.0, 0.0056179776, 0.0028089888, 0.016853932, 0.0, 0.0056179776, 0.0, 0.0, 0.0028089888, 0.0, 0.0, 0.0, 0.0, 0.014044944, 0.0028089888, 0.011235955, 0.0028089888, 0.0028089888, 0.011235955, 0.0056179776, 0.0, 0.0, 0.0, 0.0056179776, 0.0, 0.0056179776, 0.0028089888, 0.0056179776, 0.0028089888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0056179776, 0.0056179776, 0.0028089888, 0.0028089888, 0.0, 0.0, 0.0, 0.0028089888, 0.008426966, 0.0028089888, 0.0, 0.008426966, 0.0, 0.0, 0.0, 0.0, 0.0028089888, 0.0, 0.0, 0.0028089888, 0.0028089888, 0.0056179776, 0.0, 0.0, 0.0028089888, 0.0, 0.0, 0.0028089888, 0.011235955, 0.0, 0.0, 0.008426966, 0.008426966, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:53:19 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:19 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:19 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:19 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:19 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:19 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:19 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:19 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:19 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:19 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:19 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:19 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:19 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:19 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:19 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 29
Predicting test image : cake as chocolate
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/chocolate/1.jpg
17/02/16 16:53:19 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:19 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:19 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:19 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:19 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:20 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:20 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:20 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:20 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:20 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:20 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:20 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:20 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:20 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0018656716, 0.0018656716, 0.005597015, 0.0, 0.0, 0.0018656716, 0.0, 0.005597015, 0.0018656716, 0.0018656716, 0.0, 0.0018656716, 0.0, 0.0018656716, 0.005597015, 0.009328358, 0.0018656716, 0.0, 0.0018656716, 0.0, 0.005597015, 0.0037313432, 0.0, 0.0018656716, 0.0, 0.0, 0.0018656716, 0.0037313432, 0.0018656716, 0.0037313432, 0.0037313432, 0.0018656716, 0.0018656716, 0.0018656716, 0.0, 0.005597015, 0.0, 0.005597015, 0.0, 0.0037313432, 0.0, 0.0, 0.0018656716, 0.005597015, 0.0, 0.0, 0.0, 0.0, 0.009328358, 0.0018656716, 0.0037313432, 0.0, 0.0, 0.005597015, 0.005597015, 0.0, 0.0, 0.005597015, 0.0, 0.0018656716, 0.0018656716, 0.0, 0.0037313432, 0.0, 0.0018656716, 0.0037313432, 0.0074626864, 0.0, 0.0, 0.0018656716, 0.0018656716, 0.005597015, 0.0, 0.005597015, 0.0074626864, 0.0, 0.0, 0.0018656716, 0.0018656716, 0.0, 0.0037313432, 0.0, 0.0037313432, 0.0, 0.0, 0.0, 0.0, 0.0074626864, 0.0, 0.0, 0.0, 0.0074626864, 0.0, 0.01119403, 0.0, 0.0, 0.0, 0.0074626864, 0.0, 0.0, 0.0, 0.0037313432, 0.005597015, 0.0, 0.0, 0.0018656716, 0.0, 0.0074626864, 0.0018656716, 0.0037313432, 0.0074626864, 0.0, 0.0, 0.013059701, 0.0018656716, 0.0018656716, 0.0037313432, 0.0037313432, 0.0037313432, 0.0018656716, 0.005597015, 0.0018656716, 0.0, 0.0, 0.0, 0.0018656716, 0.0074626864, 0.009328358, 0.0, 0.0, 0.0018656716, 0.009328358, 0.005597015, 0.0, 0.005597015, 0.0, 0.0, 0.0037313432, 0.0, 0.0, 0.0, 0.0018656716, 0.0018656716, 0.0037313432, 0.0037313432, 0.0037313432, 0.0018656716, 0.0037313432, 0.0, 0.0018656716, 0.0037313432, 0.0, 0.0, 0.0, 0.014925373, 0.0018656716, 0.013059701, 0.0, 0.0, 0.0, 0.0037313432, 0.0, 0.0, 0.0074626864, 0.0, 0.005597015, 0.0037313432, 0.009328358, 0.0037313432, 0.0018656716, 0.0, 0.0018656716, 0.0074626864, 0.0018656716, 0.009328358, 0.005597015, 0.0018656716, 0.005597015, 0.0, 0.0018656716, 0.005597015, 0.0037313432, 0.0018656716, 0.0, 0.0018656716, 0.0, 0.0, 0.005597015, 0.0, 0.0018656716, 0.0037313432, 0.0018656716, 0.0, 0.01119403, 0.0, 0.005597015, 0.0, 0.0, 0.0018656716, 0.0018656716, 0.0037313432, 0.0, 0.0, 0.0018656716, 0.0018656716, 0.0, 0.0074626864, 0.0, 0.020522388, 0.0074626864, 0.0, 0.0, 0.0018656716, 0.0, 0.0, 0.005597015, 0.0037313432, 0.0, 0.0074626864, 0.0, 0.0018656716, 0.0, 0.0074626864, 0.0018656716, 0.005597015, 0.0, 0.0, 0.0018656716, 0.0, 0.0074626864, 0.0018656716, 0.0, 0.0, 0.0, 0.01119403, 0.0, 0.0018656716, 0.0018656716, 0.009328358, 0.0, 0.0018656716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0074626864, 0.0018656716, 0.0018656716, 0.0037313432, 0.009328358, 0.0, 0.0, 0.0018656716, 0.0, 0.0037313432, 0.0, 0.0, 0.0018656716, 0.0037313432, 0.0, 0.0, 0.0037313432, 0.0037313432, 0.0, 0.0037313432, 0.009328358, 0.0037313432, 0.0, 0.0018656716, 0.0, 0.0, 0.0018656716, 0.013059701, 0.0037313432, 0.0, 0.0037313432, 0.0018656716, 0.0018656716, 0.0018656716, 0.0018656716, 0.0037313432, 0.0, 0.0, 0.014925373, 0.0037313432, 0.0, 0.01119403, 0.0018656716, 0.0018656716, 0.0018656716, 0.0037313432, 0.0, 0.0, 0.0, 0.005597015, 0.0018656716, 0.0, 0.005597015, 0.0018656716, 0.009328358, 0.0, 0.0, 0.0074626864, 0.0018656716, 0.0018656716, 0.0, 0.0, 0.0018656716, 0.0018656716, 0.0, 0.0, 0.005597015, 0.0, 0.0018656716, 0.0037313432, 0.0, 0.0018656716, 0.0, 0.0037313432, 0.0018656716, 0.0, 0.0018656716, 0.0037313432, 0.0018656716, 0.0, 0.0037313432, 0.0, 0.005597015, 0.0018656716, 0.0018656716, 0.0, 0.01119403, 0.0, 0.0, 0.0, 0.0, 0.0018656716, 0.0037313432, 0.0018656716, 0.0074626864, 0.0, 0.0037313432, 0.005597015, 0.0018656716, 0.0, 0.0, 0.005597015, 0.005597015, 0.0, 0.0, 0.0, 0.0037313432, 0.0, 0.0018656716, 0.0018656716, 0.0018656716, 0.0018656716, 0.0018656716, 0.0018656716, 0.0, 0.01119403, 0.0037313432, 0.0018656716, 0.0037313432, 0.0018656716, 0.0, 0.009328358, 0.0018656716, 0.0, 0.005597015, 0.0037313432, 0.005597015, 0.0018656716, 0.0, 0.0, 0.009328358, 0.0, 0.0018656716, 0.0037313432, 0.0018656716, 0.0, 0.0, 0.0018656716, 0.01119403, 0.0, 0.005597015, 0.0037313432, 0.0018656716, 0.0, 0.0, 0.0037313432, 0.0, 0.0018656716, 0.0, 0.005597015, 0.0, 0.0018656716 ]
--Histogram size : 400
17/02/16 16:53:20 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:20 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:20 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:20 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:20 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:21 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:21 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:21 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:21 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:21 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:21 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:21 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:21 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:21 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:21 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:21 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:21 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : chocolate as books
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/chocolate/10.jpg
17/02/16 16:53:21 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:21 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:21 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:21 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:21 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:22 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:22 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:22 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:22 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:22 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:22 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 100
17/02/16 16:53:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:22 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:22 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:22 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:22 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:22 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:22 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.010080645, 0.0, 0.008064516, 0.0, 0.004032258, 0.002016129, 0.004032258, 0.002016129, 0.004032258, 0.010080645, 0.004032258, 0.008064516, 0.0, 0.0, 0.004032258, 0.006048387, 0.004032258, 0.004032258, 0.004032258, 0.008064516, 0.006048387, 0.012096774, 0.0, 0.0, 0.0, 0.004032258, 0.004032258, 0.004032258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010080645, 0.0, 0.002016129, 0.014112903, 0.0, 0.002016129, 0.0, 0.004032258, 0.004032258, 0.0, 0.0, 0.006048387, 0.004032258, 0.0, 0.010080645, 0.002016129, 0.004032258, 0.012096774, 0.002016129, 0.0, 0.004032258, 0.002016129, 0.002016129, 0.006048387, 0.0, 0.004032258, 0.002016129, 0.0, 0.004032258, 0.010080645, 0.0, 0.004032258, 0.004032258, 0.0, 0.006048387, 0.008064516, 0.004032258, 0.002016129, 0.002016129, 0.0, 0.002016129, 0.0, 0.0, 0.0, 0.002016129, 0.0, 0.004032258, 0.002016129, 0.002016129, 0.0, 0.0, 0.004032258, 0.004032258, 0.004032258, 0.0, 0.002016129, 0.0, 0.006048387, 0.002016129, 0.002016129, 0.0, 0.0, 0.0, 0.0, 0.002016129, 0.004032258, 0.0, 0.0, 0.002016129, 0.0, 0.004032258, 0.002016129, 0.004032258, 0.006048387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006048387, 0.0, 0.0, 0.002016129, 0.002016129, 0.002016129, 0.006048387, 0.004032258, 0.004032258, 0.006048387, 0.002016129, 0.0, 0.0, 0.0, 0.002016129, 0.002016129, 0.002016129, 0.0, 0.002016129, 0.006048387, 0.0, 0.0, 0.002016129, 0.0, 0.006048387, 0.0, 0.002016129, 0.0, 0.004032258, 0.002016129, 0.004032258, 0.006048387, 0.0, 0.0, 0.002016129, 0.002016129, 0.002016129, 0.0, 0.0, 0.008064516, 0.0, 0.0, 0.004032258, 0.006048387, 0.004032258, 0.004032258, 0.006048387, 0.004032258, 0.0, 0.002016129, 0.006048387, 0.0, 0.0, 0.0, 0.002016129, 0.002016129, 0.002016129, 0.0, 0.002016129, 0.010080645, 0.004032258, 0.0, 0.004032258, 0.0, 0.010080645, 0.002016129, 0.002016129, 0.008064516, 0.0, 0.002016129, 0.004032258, 0.002016129, 0.004032258, 0.0, 0.002016129, 0.0, 0.002016129, 0.010080645, 0.004032258, 0.004032258, 0.004032258, 0.004032258, 0.002016129, 0.004032258, 0.002016129, 0.004032258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002016129, 0.0, 0.002016129, 0.0, 0.002016129, 0.0, 0.002016129, 0.002016129, 0.002016129, 0.0, 0.0, 0.004032258, 0.008064516, 0.0, 0.004032258, 0.004032258, 0.002016129, 0.0, 0.006048387, 0.0, 0.0, 0.004032258, 0.0, 0.004032258, 0.004032258, 0.006048387, 0.0, 0.0, 0.002016129, 0.004032258, 0.0, 0.0, 0.006048387, 0.0, 0.010080645, 0.0, 0.0, 0.0, 0.002016129, 0.004032258, 0.0, 0.002016129, 0.0, 0.0, 0.0, 0.008064516, 0.0, 0.002016129, 0.0, 0.0, 0.004032258, 0.010080645, 0.0, 0.012096774, 0.0, 0.002016129, 0.004032258, 0.0, 0.0, 0.0, 0.006048387, 0.002016129, 0.004032258, 0.0, 0.0, 0.002016129, 0.0, 0.0, 0.006048387, 0.002016129, 0.002016129, 0.002016129, 0.004032258, 0.0, 0.0, 0.0, 0.002016129, 0.0, 0.002016129, 0.004032258, 0.0, 0.0, 0.008064516, 0.004032258, 0.0, 0.0, 0.004032258, 0.0, 0.0, 0.004032258, 0.002016129, 0.004032258, 0.0, 0.002016129, 0.004032258, 0.002016129, 0.010080645, 0.002016129, 0.006048387, 0.006048387, 0.008064516, 0.002016129, 0.002016129, 0.004032258, 0.0, 0.004032258, 0.002016129, 0.008064516, 0.006048387, 0.0, 0.004032258, 0.0, 0.0, 0.004032258, 0.0, 0.002016129, 0.004032258, 0.0, 0.0, 0.0, 0.004032258, 0.0, 0.002016129, 0.004032258, 0.002016129, 0.002016129, 0.0, 0.0, 0.002016129, 0.0, 0.002016129, 0.0, 0.0, 0.0, 0.0, 0.004032258, 0.008064516, 0.006048387, 0.006048387, 0.002016129, 0.002016129, 0.002016129, 0.0, 0.0, 0.0, 0.002016129, 0.002016129, 0.002016129, 0.008064516, 0.0, 0.006048387, 0.0, 0.004032258, 0.0, 0.0, 0.002016129, 0.0, 0.0, 0.002016129, 0.006048387, 0.0, 0.0, 0.006048387, 0.002016129, 0.0, 0.0, 0.004032258, 0.008064516, 0.010080645, 0.002016129, 0.002016129, 0.0, 0.002016129, 0.002016129, 0.004032258, 0.0, 0.004032258, 0.002016129, 0.004032258, 0.0, 0.006048387, 0.002016129, 0.004032258, 0.004032258, 0.0, 0.004032258, 0.0, 0.006048387, 0.002016129, 0.0, 0.0, 0.002016129, 0.004032258, 0.0, 0.002016129, 0.004032258 ]
--Histogram size : 400
17/02/16 16:53:22 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:22 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:22 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:22 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:22 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:23 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:23 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:23 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:23 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:23 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:23 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:23 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:23 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:23 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:23 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:23 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:23 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
Predicting test image : chocolate as trees
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/chocolate/3.jpg
17/02/16 16:53:23 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:23 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:23 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:23 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:23 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:23 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:23 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:23 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:23 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:23 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:23 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:23 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:23 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:23 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 100
17/02/16 16:53:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:23 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:23 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:23 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0017513135, 0.0017513135, 0.0, 0.003502627, 0.0017513135, 0.0017513135, 0.010507881, 0.0052539404, 0.0, 0.0017513135, 0.0017513135, 0.0, 0.0, 0.0, 0.0, 0.003502627, 0.0017513135, 0.0052539404, 0.0, 0.003502627, 0.003502627, 0.0, 0.0017513135, 0.003502627, 0.007005254, 0.014010508, 0.0, 0.0, 0.0, 0.0, 0.003502627, 0.0017513135, 0.0052539404, 0.0017513135, 0.0, 0.0, 0.0, 0.008756567, 0.0, 0.0017513135, 0.0017513135, 0.0052539404, 0.0, 0.0, 0.014010508, 0.003502627, 0.0, 0.0, 0.008756567, 0.0, 0.007005254, 0.007005254, 0.008756567, 0.003502627, 0.0017513135, 0.0, 0.0017513135, 0.003502627, 0.0, 0.0017513135, 0.0052539404, 0.0, 0.0017513135, 0.0, 0.008756567, 0.0, 0.007005254, 0.0017513135, 0.0017513135, 0.0017513135, 0.0017513135, 0.0052539404, 0.0, 0.007005254, 0.003502627, 0.0017513135, 0.0, 0.0017513135, 0.007005254, 0.0, 0.0, 0.003502627, 0.003502627, 0.0, 0.007005254, 0.0, 0.0, 0.010507881, 0.0, 0.0, 0.0, 0.003502627, 0.003502627, 0.0017513135, 0.0017513135, 0.003502627, 0.0017513135, 0.0017513135, 0.0, 0.003502627, 0.0, 0.008756567, 0.0017513135, 0.003502627, 0.003502627, 0.0, 0.0, 0.012259195, 0.0017513135, 0.0, 0.003502627, 0.0017513135, 0.007005254, 0.014010508, 0.0, 0.0, 0.0, 0.0, 0.007005254, 0.0017513135, 0.003502627, 0.0017513135, 0.0, 0.0, 0.0, 0.007005254, 0.003502627, 0.0017513135, 0.0017513135, 0.0, 0.0017513135, 0.0017513135, 0.007005254, 0.0, 0.0017513135, 0.0017513135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008756567, 0.0, 0.0, 0.0017513135, 0.0, 0.0, 0.0017513135, 0.0017513135, 0.0, 0.0, 0.0, 0.003502627, 0.0, 0.017513134, 0.003502627, 0.0, 0.0052539404, 0.014010508, 0.0017513135, 0.0017513135, 0.0, 0.0017513135, 0.0, 0.0017513135, 0.007005254, 0.003502627, 0.007005254, 0.0052539404, 0.003502627, 0.0, 0.003502627, 0.0, 0.008756567, 0.0, 0.0052539404, 0.0017513135, 0.0052539404, 0.0, 0.0017513135, 0.0052539404, 0.008756567, 0.0, 0.003502627, 0.003502627, 0.0, 0.0, 0.0017513135, 0.0, 0.0017513135, 0.003502627, 0.0, 0.0, 0.008756567, 0.0017513135, 0.0017513135, 0.010507881, 0.0017513135, 0.0, 0.0, 0.0017513135, 0.0, 0.0017513135, 0.007005254, 0.0, 0.0, 0.0, 0.0, 0.0017513135, 0.0017513135, 0.0017513135, 0.0017513135, 0.0017513135, 0.0017513135, 0.0, 0.0017513135, 0.0, 0.0017513135, 0.0017513135, 0.0, 0.007005254, 0.003502627, 0.007005254, 0.0017513135, 0.0017513135, 0.0, 0.0, 0.0017513135, 0.0, 0.007005254, 0.0017513135, 0.0, 0.0, 0.0017513135, 0.003502627, 0.003502627, 0.0052539404, 0.0, 0.0, 0.0, 0.0, 0.0017513135, 0.0, 0.0, 0.0, 0.007005254, 0.0, 0.0052539404, 0.0017513135, 0.0, 0.0, 0.0, 0.003502627, 0.0, 0.0, 0.0052539404, 0.0, 0.010507881, 0.010507881, 0.0, 0.0, 0.003502627, 0.0, 0.0, 0.012259195, 0.007005254, 0.0, 0.0, 0.0052539404, 0.0, 0.0, 0.0, 0.0017513135, 0.0, 0.0017513135, 0.0, 0.003502627, 0.0, 0.0, 0.0052539404, 0.008756567, 0.0017513135, 0.0, 0.0017513135, 0.0, 0.0, 0.008756567, 0.0, 0.0, 0.003502627, 0.0, 0.0017513135, 0.0017513135, 0.0017513135, 0.0, 0.0, 0.0, 0.0, 0.003502627, 0.0052539404, 0.0, 0.0, 0.0017513135, 0.0, 0.003502627, 0.003502627, 0.0, 0.0017513135, 0.0, 0.0, 0.007005254, 0.0017513135, 0.0052539404, 0.0, 0.010507881, 0.0017513135, 0.0017513135, 0.0, 0.0, 0.0, 0.0017513135, 0.003502627, 0.008756567, 0.0, 0.0, 0.0, 0.0017513135, 0.0, 0.008756567, 0.007005254, 0.0052539404, 0.0017513135, 0.0, 0.003502627, 0.007005254, 0.0, 0.0, 0.0052539404, 0.0, 0.0052539404, 0.007005254, 0.0, 0.010507881, 0.0017513135, 0.0017513135, 0.008756567, 0.0, 0.0, 0.0052539404, 0.008756567, 0.0, 0.0, 0.0017513135, 0.010507881, 0.0017513135, 0.0, 0.0017513135, 0.0, 0.003502627, 0.0, 0.003502627, 0.0, 0.0017513135, 0.010507881, 0.0, 0.003502627, 0.003502627, 0.0017513135, 0.0, 0.0, 0.0017513135, 0.0, 0.010507881, 0.0017513135, 0.008756567, 0.0, 0.0, 0.0017513135, 0.003502627, 0.0, 0.0017513135, 0.0052539404, 0.010507881, 0.0, 0.0017513135, 0.0017513135, 0.003502627, 0.0, 0.0052539404, 0.008756567, 0.0, 0.0, 0.0, 0.010507881, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0052539404 ]
--Histogram size : 400
17/02/16 16:53:24 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:24 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:24 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:24 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:24 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:24 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:24 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:24 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:24 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:24 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:24 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:24 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:24 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:24 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 19
17/02/16 16:53:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:24 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:24 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:24 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
Predicting test image : chocolate as cake
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/chocolate/7.jpg
17/02/16 16:53:25 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:25 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:25 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:25 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:25 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:25 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:25 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:25 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:25 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:25 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:25 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:25 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:25 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:25 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:25 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:25 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:25 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.007075472, 0.0, 0.0, 0.007075472, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.007075472, 0.004716981, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.0, 0.007075472, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.0023584906, 0.0023584906, 0.004716981, 0.0023584906, 0.0, 0.0, 0.0, 0.0023584906, 0.0, 0.0, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.0, 0.004716981, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.0023584906, 0.0, 0.0, 0.004716981, 0.0, 0.0023584906, 0.0023584906, 0.0023584906, 0.0, 0.004716981, 0.004716981, 0.0, 0.0, 0.011792453, 0.0023584906, 0.0, 0.0023584906, 0.0, 0.0, 0.0, 0.009433962, 0.0023584906, 0.004716981, 0.0, 0.004716981, 0.007075472, 0.0, 0.004716981, 0.0, 0.0023584906, 0.0, 0.0, 0.004716981, 0.004716981, 0.0, 0.0, 0.0023584906, 0.0, 0.009433962, 0.007075472, 0.004716981, 0.0023584906, 0.0, 0.0023584906, 0.011792453, 0.0023584906, 0.0023584906, 0.007075472, 0.0, 0.007075472, 0.009433962, 0.004716981, 0.0, 0.0023584906, 0.004716981, 0.0023584906, 0.004716981, 0.009433962, 0.0, 0.007075472, 0.0023584906, 0.0, 0.0023584906, 0.0, 0.004716981, 0.0, 0.0, 0.0, 0.004716981, 0.009433962, 0.0023584906, 0.0023584906, 0.0023584906, 0.007075472, 0.0023584906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0023584906, 0.004716981, 0.0023584906, 0.0, 0.0, 0.004716981, 0.0023584906, 0.0, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.0, 0.0023584906, 0.009433962, 0.0, 0.0023584906, 0.0, 0.004716981, 0.011792453, 0.0, 0.004716981, 0.0023584906, 0.0, 0.0, 0.004716981, 0.0, 0.004716981, 0.0, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.0023584906, 0.0, 0.007075472, 0.0023584906, 0.0, 0.0, 0.009433962, 0.0023584906, 0.007075472, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0023584906, 0.0023584906, 0.0, 0.0023584906, 0.007075472, 0.0023584906, 0.0023584906, 0.0, 0.004716981, 0.007075472, 0.007075472, 0.004716981, 0.0023584906, 0.0, 0.007075472, 0.0023584906, 0.0, 0.0023584906, 0.004716981, 0.0, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.0, 0.0, 0.007075472, 0.007075472, 0.0, 0.0, 0.0, 0.0, 0.007075472, 0.0023584906, 0.004716981, 0.0023584906, 0.0, 0.0023584906, 0.004716981, 0.0023584906, 0.0, 0.0023584906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007075472, 0.0023584906, 0.0, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.004716981, 0.0, 0.0, 0.0, 0.007075472, 0.0023584906, 0.0, 0.0, 0.0023584906, 0.0023584906, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.011792453, 0.0, 0.007075472, 0.016509434, 0.004716981, 0.0023584906, 0.004716981, 0.0, 0.0023584906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007075472, 0.0, 0.0023584906, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.0023584906, 0.004716981, 0.0023584906, 0.004716981, 0.0, 0.0023584906, 0.0023584906, 0.004716981, 0.0023584906, 0.0, 0.0, 0.0, 0.0023584906, 0.0, 0.004716981, 0.0, 0.0023584906, 0.009433962, 0.004716981, 0.014150944, 0.0, 0.009433962, 0.0, 0.004716981, 0.0023584906, 0.014150944, 0.009433962, 0.004716981, 0.0023584906, 0.0, 0.004716981, 0.004716981, 0.004716981, 0.004716981, 0.0, 0.0023584906, 0.0, 0.0, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.004716981, 0.0, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.007075472, 0.007075472, 0.0023584906, 0.004716981, 0.0, 0.0023584906, 0.0, 0.0023584906, 0.0, 0.0, 0.004716981, 0.004716981, 0.0, 0.0023584906, 0.0, 0.016509434, 0.0, 0.007075472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009433962, 0.0023584906, 0.004716981, 0.0, 0.0, 0.0023584906, 0.004716981, 0.0023584906, 0.0023584906, 0.0, 0.004716981, 0.0023584906, 0.004716981, 0.0023584906, 0.009433962, 0.0, 0.0, 0.0, 0.0023584906, 0.0, 0.018867925, 0.004716981, 0.007075472, 0.0, 0.004716981, 0.0023584906, 0.0, 0.0, 0.004716981, 0.0, 0.0, 0.0023584906, 0.0023584906, 0.0023584906, 0.0023584906, 0.0023584906, 0.0023584906, 0.0, 0.007075472, 0.0, 0.0023584906, 0.0023584906, 0.0, 0.0, 0.004716981, 0.0, 0.004716981, 0.009433962 ]
--Histogram size : 400
17/02/16 16:53:26 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:26 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:26 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:26 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:26 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:26 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:26 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:26 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:26 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:26 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:26 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:26 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:26 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:26 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:26 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:26 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:26 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
Predicting test image : chocolate as trees
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/chocolate/9.jpg
17/02/16 16:53:27 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:27 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:27 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:27 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:27 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:27 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:27 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:27 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:27 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:27 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:27 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:27 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:27 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:27 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:27 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:27 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:27 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0025188916, 0.0, 0.005037783, 0.005037783, 0.0, 0.0025188916, 0.010075566, 0.0025188916, 0.0, 0.0, 0.0025188916, 0.0, 0.0, 0.0, 0.0, 0.0025188916, 0.0, 0.005037783, 0.0025188916, 0.0, 0.005037783, 0.005037783, 0.0, 0.0, 0.005037783, 0.007556675, 0.0, 0.0025188916, 0.005037783, 0.0, 0.007556675, 0.0, 0.0, 0.007556675, 0.0, 0.0, 0.005037783, 0.027707808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0025188916, 0.0025188916, 0.0025188916, 0.0, 0.0, 0.005037783, 0.0025188916, 0.005037783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007556675, 0.0, 0.0, 0.0, 0.0, 0.0025188916, 0.005037783, 0.0, 0.007556675, 0.0, 0.005037783, 0.0025188916, 0.0025188916, 0.0025188916, 0.0025188916, 0.01511335, 0.0025188916, 0.0, 0.0, 0.007556675, 0.0, 0.010075566, 0.0, 0.007556675, 0.0, 0.0025188916, 0.0, 0.0, 0.0, 0.0, 0.005037783, 0.0, 0.0, 0.007556675, 0.0, 0.0025188916, 0.0, 0.005037783, 0.0, 0.0025188916, 0.005037783, 0.0, 0.0, 0.0025188916, 0.0, 0.005037783, 0.0025188916, 0.0, 0.0, 0.0025188916, 0.005037783, 0.0025188916, 0.0025188916, 0.0, 0.0, 0.0, 0.005037783, 0.0302267, 0.0, 0.0, 0.0025188916, 0.0, 0.0025188916, 0.0025188916, 0.0, 0.0025188916, 0.0025188916, 0.0, 0.0, 0.007556675, 0.0025188916, 0.0025188916, 0.007556675, 0.0025188916, 0.0, 0.0, 0.01763224, 0.0, 0.007556675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007556675, 0.022670025, 0.0, 0.0, 0.005037783, 0.0, 0.0025188916, 0.007556675, 0.0, 0.0025188916, 0.0, 0.0, 0.0025188916, 0.0, 0.010075566, 0.0025188916, 0.005037783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0025188916, 0.0025188916, 0.0025188916, 0.005037783, 0.0025188916, 0.007556675, 0.0, 0.0025188916, 0.0, 0.0, 0.0, 0.01763224, 0.0025188916, 0.0025188916, 0.005037783, 0.0, 0.0, 0.0025188916, 0.0025188916, 0.005037783, 0.0, 0.0, 0.005037783, 0.0, 0.0, 0.0025188916, 0.0025188916, 0.0, 0.0025188916, 0.0, 0.0025188916, 0.007556675, 0.0025188916, 0.0025188916, 0.0025188916, 0.0025188916, 0.0, 0.0, 0.0, 0.0, 0.0025188916, 0.01511335, 0.0, 0.0, 0.005037783, 0.0, 0.0025188916, 0.0025188916, 0.0025188916, 0.0025188916, 0.0025188916, 0.0025188916, 0.0, 0.0, 0.007556675, 0.007556675, 0.007556675, 0.0, 0.0, 0.0, 0.0025188916, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0025188916, 0.0025188916, 0.025188915, 0.007556675, 0.0, 0.0025188916, 0.0025188916, 0.0, 0.0025188916, 0.0, 0.0025188916, 0.0, 0.0025188916, 0.0, 0.0, 0.0025188916, 0.010075566, 0.0, 0.0025188916, 0.0, 0.005037783, 0.0, 0.0, 0.0, 0.0025188916, 0.0025188916, 0.005037783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0025188916, 0.0025188916, 0.0, 0.0, 0.005037783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010075566, 0.0, 0.0, 0.0, 0.0, 0.0025188916, 0.0, 0.0, 0.0025188916, 0.0, 0.0, 0.0, 0.0, 0.005037783, 0.0, 0.010075566, 0.0, 0.0025188916, 0.0025188916, 0.0025188916, 0.0, 0.0, 0.0025188916, 0.0, 0.0025188916, 0.0, 0.0025188916, 0.0, 0.0025188916, 0.0, 0.0025188916, 0.0, 0.0, 0.005037783, 0.0, 0.0, 0.007556675, 0.007556675, 0.0, 0.0, 0.005037783, 0.0025188916, 0.0025188916, 0.0025188916, 0.0, 0.0025188916, 0.005037783, 0.0025188916, 0.010075566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0025188916, 0.012594458, 0.01763224, 0.005037783, 0.0, 0.005037783, 0.01511335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010075566, 0.0, 0.007556675, 0.0, 0.0, 0.0025188916, 0.0, 0.0, 0.010075566, 0.0025188916, 0.0, 0.0, 0.005037783, 0.01511335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0025188916, 0.0, 0.0, 0.0, 0.0, 0.0025188916, 0.0025188916, 0.007556675, 0.0, 0.0, 0.005037783, 0.0, 0.0, 0.007556675, 0.0025188916, 0.0025188916, 0.0, 0.0, 0.0025188916, 0.0025188916, 0.0, 0.010075566, 0.010075566, 0.0, 0.0, 0.0, 0.005037783, 0.0025188916, 0.0, 0.0, 0.0025188916, 0.0025188916, 0.0025188916, 0.0, 0.020151133, 0.0, 0.0, 0.0, 0.007556675, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:53:27 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:28 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:28 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:28 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:28 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:28 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:28 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:28 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:28 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:28 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:28 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:28 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:28 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:28 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 19
17/02/16 16:53:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:28 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:28 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:28 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
Predicting test image : chocolate as trees
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/laptop/1.jpg
17/02/16 16:53:28 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:28 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:28 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:28 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:28 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:29 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:29 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:29 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:29 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:29 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:29 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:29 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 100
17/02/16 16:53:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:29 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:29 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:29 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 100
17/02/16 16:53:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:29 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:29 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:29 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:29 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:29 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:29 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.011904762, 0.0, 0.0, 0.011904762, 0.0, 0.023809524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023809524, 0.0, 0.023809524, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.011904762, 0.011904762, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023809524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.023809524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023809524, 0.0, 0.011904762, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.023809524, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.011904762, 0.0, 0.0, 0.011904762, 0.0, 0.011904762, 0.023809524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.011904762, 0.011904762, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023809524, 0.0, 0.0, 0.011904762, 0.011904762, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.023809524, 0.0, 0.0, 0.0, 0.0, 0.023809524, 0.0, 0.0, 0.011904762, 0.023809524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.023809524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.011904762, 0.023809524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.023809524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762, 0.0, 0.0, 0.011904762, 0.0, 0.011904762, 0.0, 0.011904762, 0.011904762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011904762 ]
--Histogram size : 400
17/02/16 16:53:29 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:29 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:29 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:29 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:29 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:30 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:30 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:30 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:30 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:30 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:30 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:30 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:30 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:30 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:30 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:30 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:30 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : laptop as trees
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/laptop/2.jpg
17/02/16 16:53:30 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:30 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:30 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:30 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:30 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:30 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:30 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:30 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:30 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:30 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:30 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:30 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:30 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:30 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:30 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:30 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:30 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.049180325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.049180325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032786883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.049180325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.016393442, 0.0, 0.049180325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032786883, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.049180325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032786883, 0.0, 0.016393442, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.049180325, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032786883, 0.0, 0.0, 0.0, 0.032786883, 0.0, 0.0, 0.0, 0.032786883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.06557377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032786883, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:53:31 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:31 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:31 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:31 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:31 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:31 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:31 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:31 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:31 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:31 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:31 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:31 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:31 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:31 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:31 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:31 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:31 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : laptop as bag
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/laptop/3.jpg
17/02/16 16:53:32 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:32 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:32 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:32 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:32 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:32 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:32 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:32 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 100
17/02/16 16:53:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:32 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:32 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:32 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:32 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:32 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:32 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033898305, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10169491, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.033898305, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.033898305, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.033898305, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033898305, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.033898305, 0.0, 0.0, 0.033898305, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.033898305, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033898305, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033898305, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779661, 0.0, 0.0, 0.033898305, 0.0, 0.033898305, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016949153, 0.0 ]
--Histogram size : 400
17/02/16 16:53:33 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:33 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:33 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:33 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:33 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:33 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:33 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:33 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:33 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:33 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:33 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:33 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:33 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:33 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
Predicting test image : laptop as bag
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/laptop/6.jpg
17/02/16 16:53:33 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:34 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:34 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:34 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:34 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:34 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:34 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:34 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:34 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:34 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:34 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:34 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:34 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:34 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.005524862, 0.005524862, 0.0, 0.0, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.0, 0.005524862, 0.005524862, 0.005524862, 0.005524862, 0.0, 0.0, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.0, 0.005524862, 0.016574586, 0.022099448, 0.0, 0.005524862, 0.0, 0.0, 0.005524862, 0.0, 0.005524862, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011049724, 0.011049724, 0.0, 0.0, 0.0, 0.0, 0.011049724, 0.0, 0.005524862, 0.005524862, 0.0, 0.0, 0.005524862, 0.005524862, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005524862, 0.005524862, 0.0, 0.0, 0.005524862, 0.011049724, 0.0, 0.005524862, 0.0, 0.005524862, 0.0, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.005524862, 0.02762431, 0.011049724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011049724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022099448, 0.0, 0.005524862, 0.011049724, 0.0, 0.0, 0.0, 0.0, 0.005524862, 0.0, 0.005524862, 0.0, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.0, 0.0, 0.005524862, 0.005524862, 0.0, 0.0, 0.0, 0.011049724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005524862, 0.005524862, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011049724, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.0, 0.02762431, 0.005524862, 0.005524862, 0.0, 0.011049724, 0.0, 0.0, 0.005524862, 0.0, 0.011049724, 0.0, 0.0, 0.011049724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016574586, 0.016574586, 0.0, 0.0, 0.0, 0.0, 0.038674034, 0.005524862, 0.0, 0.005524862, 0.005524862, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016574586, 0.016574586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016574586, 0.0, 0.0, 0.0, 0.011049724, 0.005524862, 0.0, 0.005524862, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005524862, 0.0, 0.011049724, 0.016574586, 0.0, 0.011049724, 0.005524862, 0.0, 0.0, 0.0, 0.016574586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005524862, 0.0, 0.005524862, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011049724, 0.0, 0.0, 0.005524862, 0.005524862, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.005524862, 0.0, 0.011049724, 0.0, 0.0, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005524862, 0.005524862, 0.0, 0.0, 0.03314917, 0.0, 0.0, 0.0, 0.005524862, 0.005524862, 0.0, 0.016574586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.06629834, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.016574586, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.0, 0.0, 0.011049724, 0.005524862, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011049724, 0.005524862, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.005524862, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005524862, 0.0, 0.005524862 ]
--Histogram size : 400
17/02/16 16:53:34 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:34 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:34 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:34 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:34 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:35 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:35 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:35 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:35 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:35 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:35 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:35 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:35 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:35 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:35 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:35 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 19
Predicting test image : laptop as mobile
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/laptop/9.jpg
17/02/16 16:53:35 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:35 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:35 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:35 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:35 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:36 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:36 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:36 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:36 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:36 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:36 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:36 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:36 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:36 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:36 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:36 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:36 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.029411767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.009803922, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.009803922, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029411767, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029411767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.039215688, 0.0, 0.0, 0.009803922, 0.009803922, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.019607844, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.009803922, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.009803922, 0.019607844, 0.0, 0.0, 0.009803922, 0.029411767, 0.0, 0.009803922, 0.009803922, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.009803922, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.019607844, 0.0, 0.0, 0.0, 0.029411767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.029411767, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009803922, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:53:36 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:36 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:36 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:36 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:36 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:37 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:37 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:37 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:37 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:37 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:37 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:37 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 19
17/02/16 16:53:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:37 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:37 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:37 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : laptop as watch
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/lock/2.jpg
17/02/16 16:53:37 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:37 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:37 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:37 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:37 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:38 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:38 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:38 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:38 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:38 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:38 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:38 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:38 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:38 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.028571429, 0.0, 0.0, 0.057142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.057142857, 0.0, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.0, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.057142857, 0.0, 0.0, 0.028571429, 0.14285715, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028571429, 0.0, 0.028571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.057142857, 0.0 ]
--Histogram size : 400
17/02/16 16:53:38 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:38 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:38 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:38 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:38 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:39 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:39 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:39 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:39 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:39 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:39 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:39 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:39 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:39 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:39 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 29
Predicting test image : lock as apple
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/lock/3.jpg
17/02/16 16:53:39 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:39 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:39 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:39 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:39 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:40 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:40 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:40 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:40 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:40 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:40 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 100
17/02/16 16:53:40 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 100
17/02/16 16:53:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:40 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:40 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:40 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.078431375, 0.0, 0.0, 0.039215688, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.039215688, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039215688, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.019607844, 0.019607844, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.039215688, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.019607844, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.019607844, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.058823533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.019607844, 0.0, 0.0, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019607844, 0.019607844, 0.0, 0.019607844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:53:40 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:40 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:40 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:40 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:40 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:41 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:41 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:41 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:41 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:41 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:41 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:41 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:41 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:41 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : lock as bag
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/lock/6.jpg
17/02/16 16:53:41 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:41 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:41 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:41 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:41 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:42 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:42 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:42 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:42 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:42 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:42 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:42 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:42 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:42 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10810811, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.054054055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.081081085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.054054055, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027027028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:53:42 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:42 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:42 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:42 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:42 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:43 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:43 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:43 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:43 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:43 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:43 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:43 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:43 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : lock as laptop
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/lock/7.jpg
17/02/16 16:53:43 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:43 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:43 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:43 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:43 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:43 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:43 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:43 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:43 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:43 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:43 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:43 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:43 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:43 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.0, 0.008695652, 0.008695652, 0.034782607, 0.008695652, 0.0, 0.017391304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.008695652, 0.008695652, 0.0, 0.0, 0.008695652, 0.008695652, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.008695652, 0.0, 0.0, 0.008695652, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.008695652, 0.0, 0.017391304, 0.017391304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.026086956, 0.0, 0.0, 0.008695652, 0.008695652, 0.0, 0.0, 0.008695652, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.008695652, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.008695652, 0.0, 0.0, 0.008695652, 0.0, 0.017391304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.008695652, 0.008695652, 0.0, 0.008695652, 0.0, 0.008695652, 0.0, 0.034782607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.008695652, 0.008695652, 0.0, 0.017391304, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.026086956, 0.0, 0.008695652, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.008695652, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017391304, 0.0, 0.0, 0.008695652, 0.0, 0.0, 0.008695652, 0.0, 0.017391304, 0.0, 0.0, 0.0, 0.0, 0.008695652, 0.017391304, 0.0, 0.0, 0.0, 0.008695652, 0.008695652, 0.0, 0.0, 0.0, 0.008695652, 0.0 ]
--Histogram size : 400
17/02/16 16:53:44 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:44 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:44 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:44 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:44 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:44 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:44 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:44 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 19
17/02/16 16:53:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:44 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:44 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:44 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:44 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:44 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:44 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:44 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
Predicting test image : lock as chocolate
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/lock/9.jpg
17/02/16 16:53:45 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:45 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:45 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:45 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:45 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:45 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:45 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:45 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:45 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:45 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:45 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:45 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:45 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:45 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032786883, 0.0, 0.0, 0.0, 0.0, 0.049180325, 0.0, 0.049180325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06557377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032786883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032786883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.016393442, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.016393442, 0.0, 0.016393442, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.016393442, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.081967205, 0.0, 0.0, 0.0, 0.0, 0.032786883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.016393442, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.016393442, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032786883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016393442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:53:46 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:46 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:46 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:46 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:46 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:46 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:46 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:46 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:46 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:46 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:46 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:46 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 29
17/02/16 16:53:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:46 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:46 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:46 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : lock as laptop
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/mobile/10.jpg
17/02/16 16:53:46 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:47 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:47 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:47 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:47 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:47 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:47 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:47 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:47 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:47 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:47 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:47 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:47 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:47 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:47 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:47 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:47 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007905139, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0039525693, 0.0039525693, 0.0, 0.0039525693, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0039525693, 0.011857708, 0.0, 0.007905139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.007905139, 0.0, 0.0039525693, 0.0039525693, 0.0039525693, 0.015810277, 0.007905139, 0.0039525693, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.007905139, 0.0, 0.0039525693, 0.0039525693, 0.0, 0.007905139, 0.007905139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007905139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0039525693, 0.0039525693, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007905139, 0.007905139, 0.0039525693, 0.0, 0.007905139, 0.0, 0.031620555, 0.0, 0.007905139, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0039525693, 0.0039525693, 0.0, 0.0, 0.0039525693, 0.0039525693, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.015810277, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007905139, 0.007905139, 0.011857708, 0.007905139, 0.007905139, 0.007905139, 0.0, 0.0, 0.0, 0.027667984, 0.0039525693, 0.0039525693, 0.0039525693, 0.0039525693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0, 0.0039525693, 0.015810277, 0.0, 0.007905139, 0.019762848, 0.007905139, 0.007905139, 0.0, 0.0, 0.0039525693, 0.0039525693, 0.0, 0.0, 0.0, 0.011857708, 0.0039525693, 0.0, 0.0, 0.0, 0.023715416, 0.0, 0.0, 0.011857708, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.007905139, 0.0039525693, 0.0039525693, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007905139, 0.0, 0.0, 0.007905139, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0039525693, 0.007905139, 0.0039525693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.011857708, 0.0, 0.0039525693, 0.0, 0.007905139, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.011857708, 0.0039525693, 0.007905139, 0.0, 0.0, 0.0, 0.0039525693, 0.007905139, 0.0, 0.0, 0.0039525693, 0.0039525693, 0.0, 0.0, 0.0, 0.0039525693, 0.13438736, 0.0, 0.0039525693, 0.0, 0.0, 0.007905139, 0.0, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.011857708, 0.0, 0.0, 0.011857708, 0.0, 0.0039525693, 0.0, 0.0039525693, 0.0039525693, 0.0, 0.0, 0.015810277, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.011857708, 0.007905139, 0.0, 0.0039525693, 0.0, 0.007905139, 0.0, 0.0, 0.0039525693, 0.011857708, 0.0, 0.0, 0.0039525693, 0.0, 0.0, 0.0, 0.007905139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011857708, 0.0039525693, 0.0, 0.0, 0.0, 0.015810277, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0039525693 ]
--Histogram size : 400
17/02/16 16:53:47 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:47 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:47 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:47 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:47 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:48 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:48 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:48 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:48 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:48 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:48 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:48 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:48 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:48 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:48 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:48 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:48 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
Predicting test image : mobile as chocolate
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/mobile/2.jpg
17/02/16 16:53:48 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:48 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:48 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:48 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:48 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:49 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:49 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:49 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:49 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:49 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:49 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:49 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:49 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:49 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:49 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:49 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:49 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.003021148, 0.0, 0.003021148, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.006042296, 0.0, 0.0, 0.003021148, 0.003021148, 0.0, 0.0, 0.003021148, 0.003021148, 0.0, 0.0, 0.003021148, 0.003021148, 0.003021148, 0.0, 0.0, 0.009063444, 0.0, 0.003021148, 0.006042296, 0.0, 0.006042296, 0.0, 0.0, 0.018126888, 0.003021148, 0.006042296, 0.006042296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009063444, 0.0, 0.0, 0.003021148, 0.0, 0.0, 0.0, 0.003021148, 0.003021148, 0.009063444, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018126888, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006042296, 0.0, 0.0, 0.0, 0.003021148, 0.0, 0.012084592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012084592, 0.009063444, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006042296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.0, 0.012084592, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.027190331, 0.012084592, 0.0, 0.003021148, 0.003021148, 0.006042296, 0.0, 0.0, 0.0, 0.0, 0.009063444, 0.0, 0.0, 0.003021148, 0.0, 0.0, 0.0, 0.018126888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009063444, 0.0, 0.009063444, 0.0, 0.0, 0.0, 0.009063444, 0.0, 0.0, 0.006042296, 0.0, 0.0, 0.003021148, 0.0, 0.012084592, 0.0, 0.0, 0.003021148, 0.003021148, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.003021148, 0.003021148, 0.0, 0.003021148, 0.003021148, 0.0, 0.003021148, 0.0, 0.018126888, 0.0, 0.006042296, 0.0, 0.003021148, 0.0, 0.003021148, 0.0, 0.003021148, 0.003021148, 0.0, 0.0, 0.006042296, 0.012084592, 0.0, 0.0, 0.0, 0.003021148, 0.0, 0.0, 0.0, 0.021148037, 0.003021148, 0.0, 0.0, 0.003021148, 0.0, 0.009063444, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01510574, 0.003021148, 0.006042296, 0.009063444, 0.009063444, 0.0, 0.0, 0.003021148, 0.0, 0.027190331, 0.0, 0.012084592, 0.009063444, 0.003021148, 0.009063444, 0.0, 0.003021148, 0.003021148, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.003021148, 0.0, 0.006042296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.003021148, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024169184, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.0, 0.006042296, 0.003021148, 0.0, 0.0, 0.006042296, 0.0, 0.003021148, 0.006042296, 0.012084592, 0.006042296, 0.0, 0.0, 0.0, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.0, 0.0, 0.0, 0.01510574, 0.003021148, 0.003021148, 0.003021148, 0.003021148, 0.006042296, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012084592, 0.0, 0.0, 0.0, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.0, 0.006042296, 0.009063444, 0.0, 0.003021148, 0.0, 0.0, 0.012084592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009063444, 0.0, 0.0, 0.012084592, 0.0, 0.03021148, 0.0, 0.012084592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006042296, 0.003021148, 0.0, 0.012084592, 0.0, 0.0, 0.0, 0.012084592, 0.006042296, 0.006042296, 0.003021148, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.021148037, 0.006042296, 0.0, 0.006042296, 0.003021148, 0.006042296, 0.0, 0.0, 0.003021148, 0.003021148, 0.0, 0.03021148, 0.0, 0.003021148, 0.003021148, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.0, 0.003021148, 0.0, 0.0, 0.003021148, 0.0, 0.0, 0.0, 0.0, 0.003021148, 0.003021148, 0.0, 0.0, 0.012084592, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:53:49 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:49 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:49 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:49 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:49 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:50 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:50 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:50 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:50 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:50 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:50 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:50 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:50 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:50 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:50 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:50 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:50 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:50 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:50 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:50 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:50 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : mobile as chocolate
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/mobile/4.jpg
17/02/16 16:53:50 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:50 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:50 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:50 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:50 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:51 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:51 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:51 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:51 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:51 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:51 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:51 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:51 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:51 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:51 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:51 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:51 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.0, 0.35, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:53:51 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:51 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:51 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:51 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:51 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:52 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:52 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:52 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:52 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:52 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:52 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:52 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:52 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:52 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 33
17/02/16 16:53:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:52 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:52 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:52 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : mobile as bag
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/mobile/6.jpg
17/02/16 16:53:52 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:52 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:52 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:52 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:52 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:53 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:53 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:53 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:53 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:53 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:53 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:53 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:53 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:53 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:53 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:53 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:53 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.021978023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.054945055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.010989011, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032967035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.021978023, 0.010989011, 0.021978023, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021978023, 0.0, 0.010989011, 0.0, 0.010989011, 0.0, 0.010989011, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.054945055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021978023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021978023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021978023, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021978023, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.010989011, 0.0, 0.0, 0.010989011, 0.0, 0.043956045, 0.0, 0.021978023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021978023, 0.010989011, 0.010989011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.010989011, 0.0, 0.021978023, 0.0, 0.021978023, 0.0, 0.054945055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010989011, 0.0 ]
--Histogram size : 400
17/02/16 16:53:53 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:53 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:53 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:53 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:53 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:53 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:53 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:53 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:53 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:53 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:53 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:53 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:53 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:53 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:53 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:54 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:54 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 29
Predicting test image : mobile as watch
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/mobile/9.jpg
17/02/16 16:53:54 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:54 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:54 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:54 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:54 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:54 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:54 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:54 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:54 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:54 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:54 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:54 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:54 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:54 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:54 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:54 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:54 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.008264462, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.016528925, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016528925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024793386, 0.0, 0.008264462, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.016528925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016528925, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.07438016, 0.0, 0.008264462, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016528925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.008264462, 0.0, 0.0, 0.024793386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.008264462, 0.016528925, 0.0, 0.016528925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.024793386, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.024793386, 0.008264462, 0.0, 0.016528925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.024793386, 0.008264462, 0.0, 0.0, 0.008264462, 0.0, 0.016528925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016528925, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.008264462, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.016528925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.016528925, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.008264462, 0.0, 0.016528925, 0.0, 0.057851236, 0.0, 0.008264462, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.016528925, 0.008264462, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.024793386, 0.0, 0.016528925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008264462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:53:55 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:55 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:55 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:55 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:55 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:55 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:55 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:55 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:55 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:55 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:55 INFO InternalParquetRecordReader: block read in memory in 17 ms. row count = 33
17/02/16 16:53:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:55 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:55 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:55 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 29
17/02/16 16:53:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:55 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:55 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:55 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : mobile as chocolate
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/trees/10.jpg
17/02/16 16:53:56 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:56 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:56 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:56 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:56 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:56 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:56 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:56 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:56 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:56 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 100
17/02/16 16:53:56 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:56 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:56 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:56 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:56 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:56 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:56 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.006681515, 0.006681515, 0.006681515, 0.0, 0.004454343, 0.0, 0.0, 0.004454343, 0.004454343, 0.004454343, 0.0, 0.004454343, 0.0, 0.0, 0.006681515, 0.0, 0.0, 0.0022271716, 0.011135858, 0.006681515, 0.0022271716, 0.006681515, 0.0, 0.0, 0.0, 0.004454343, 0.0, 0.0, 0.0, 0.0022271716, 0.0022271716, 0.0, 0.01336303, 0.0022271716, 0.015590201, 0.022271715, 0.01336303, 0.006681515, 0.004454343, 0.0022271716, 0.0, 0.006681515, 0.0, 0.0022271716, 0.0, 0.0, 0.0022271716, 0.0, 0.0022271716, 0.0, 0.0022271716, 0.0, 0.006681515, 0.0, 0.0, 0.006681515, 0.0022271716, 0.0, 0.0022271716, 0.0, 0.004454343, 0.0, 0.004454343, 0.0, 0.0022271716, 0.004454343, 0.004454343, 0.008908686, 0.004454343, 0.0, 0.0, 0.004454343, 0.004454343, 0.0, 0.0, 0.008908686, 0.0, 0.006681515, 0.0022271716, 0.0022271716, 0.0, 0.0022271716, 0.0022271716, 0.004454343, 0.0, 0.0022271716, 0.0, 0.01336303, 0.0, 0.004454343, 0.0, 0.004454343, 0.0022271716, 0.004454343, 0.0, 0.0, 0.0022271716, 0.0022271716, 0.0, 0.008908686, 0.0, 0.0022271716, 0.0, 0.004454343, 0.0, 0.0, 0.004454343, 0.0022271716, 0.0, 0.0, 0.004454343, 0.0, 0.008908686, 0.0, 0.0, 0.004454343, 0.0022271716, 0.0022271716, 0.0, 0.011135858, 0.004454343, 0.0, 0.011135858, 0.0, 0.0022271716, 0.0022271716, 0.004454343, 0.004454343, 0.0022271716, 0.0, 0.0022271716, 0.004454343, 0.004454343, 0.0, 0.0, 0.004454343, 0.0022271716, 0.0, 0.0022271716, 0.004454343, 0.0, 0.0, 0.0, 0.0, 0.0022271716, 0.0, 0.0, 0.0, 0.0022271716, 0.0, 0.0, 0.0022271716, 0.0022271716, 0.0, 0.006681515, 0.0, 0.0022271716, 0.004454343, 0.0, 0.0022271716, 0.0022271716, 0.004454343, 0.008908686, 0.008908686, 0.0, 0.0, 0.0022271716, 0.0, 0.0022271716, 0.0, 0.0022271716, 0.0, 0.006681515, 0.0022271716, 0.004454343, 0.0, 0.0, 0.0, 0.0022271716, 0.008908686, 0.0022271716, 0.004454343, 0.0022271716, 0.004454343, 0.0022271716, 0.0, 0.0, 0.004454343, 0.0022271716, 0.004454343, 0.006681515, 0.004454343, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006681515, 0.0022271716, 0.0, 0.0, 0.0, 0.0, 0.0022271716, 0.0022271716, 0.0, 0.01336303, 0.0, 0.020044545, 0.0, 0.004454343, 0.0, 0.008908686, 0.0, 0.0022271716, 0.0, 0.0, 0.0022271716, 0.008908686, 0.0, 0.0, 0.0, 0.0, 0.0022271716, 0.0, 0.0022271716, 0.0, 0.004454343, 0.004454343, 0.0022271716, 0.0022271716, 0.0, 0.0, 0.0022271716, 0.004454343, 0.0, 0.004454343, 0.0022271716, 0.0022271716, 0.0022271716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0022271716, 0.0, 0.004454343, 0.0, 0.006681515, 0.0, 0.0, 0.0, 0.0022271716, 0.0, 0.0, 0.0, 0.0022271716, 0.0, 0.004454343, 0.0022271716, 0.0022271716, 0.0, 0.0, 0.0, 0.0022271716, 0.0, 0.004454343, 0.0, 0.0, 0.0, 0.0, 0.0022271716, 0.0, 0.0022271716, 0.004454343, 0.008908686, 0.0, 0.0, 0.004454343, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004454343, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0022271716, 0.004454343, 0.0, 0.0, 0.0022271716, 0.006681515, 0.0022271716, 0.011135858, 0.0022271716, 0.011135858, 0.0, 0.004454343, 0.0022271716, 0.004454343, 0.0022271716, 0.0, 0.004454343, 0.0, 0.011135858, 0.011135858, 0.0, 0.004454343, 0.0, 0.0, 0.004454343, 0.0022271716, 0.006681515, 0.0022271716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0022271716, 0.0, 0.004454343, 0.008908686, 0.0, 0.0022271716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0022271716, 0.0022271716, 0.0022271716, 0.004454343, 0.0, 0.004454343, 0.004454343, 0.0022271716, 0.0, 0.004454343, 0.0, 0.0, 0.0022271716, 0.004454343, 0.0, 0.022271715, 0.0, 0.004454343, 0.0022271716, 0.0, 0.004454343, 0.0, 0.0022271716, 0.0, 0.0022271716, 0.004454343, 0.0022271716, 0.0022271716, 0.0, 0.0022271716, 0.0022271716, 0.004454343, 0.0, 0.011135858, 0.006681515, 0.004454343, 0.004454343, 0.004454343, 0.0, 0.004454343, 0.0, 0.0, 0.0, 0.0, 0.0022271716, 0.0, 0.0022271716, 0.004454343, 0.006681515, 0.0, 0.0022271716, 0.004454343, 0.011135858, 0.0, 0.0022271716, 0.0, 0.0, 0.004454343, 0.004454343, 0.0022271716, 0.0022271716 ]
--Histogram size : 400
17/02/16 16:53:56 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:57 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:57 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:57 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:57 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:57 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:57 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:57 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:57 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:57 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:53:57 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:57 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:57 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:57 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:57 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:57 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:57 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
Predicting test image : trees as books
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/trees/2.jpg
17/02/16 16:53:57 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:57 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:57 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:57 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:57 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:58 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:58 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:58 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:58 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:58 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:58 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:58 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:58 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:58 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:53:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:53:58 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:58 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:58 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.01, 0.006666667, 0.013333334, 0.01, 0.0033333334, 0.0033333334, 0.0033333334, 0.01, 0.0, 0.0, 0.0, 0.0033333334, 0.0033333334, 0.013333334, 0.0033333334, 0.0033333334, 0.0033333334, 0.006666667, 0.01, 0.0033333334, 0.0, 0.0033333334, 0.0, 0.0, 0.0033333334, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0033333334, 0.0, 0.006666667, 0.0, 0.0, 0.0033333334, 0.0, 0.0, 0.016666668, 0.0, 0.0033333334, 0.0, 0.0, 0.006666667, 0.0033333334, 0.006666667, 0.0, 0.006666667, 0.013333334, 0.0, 0.0, 0.006666667, 0.0033333334, 0.0033333334, 0.0033333334, 0.0, 0.01, 0.0033333334, 0.01, 0.0033333334, 0.0, 0.0033333334, 0.0, 0.0, 0.0, 0.0, 0.006666667, 0.0, 0.0, 0.0, 0.0033333334, 0.0, 0.0, 0.0033333334, 0.006666667, 0.0, 0.0, 0.01, 0.0033333334, 0.0, 0.0, 0.0033333334, 0.006666667, 0.0, 0.0033333334, 0.0, 0.0, 0.0033333334, 0.01, 0.0, 0.0, 0.006666667, 0.006666667, 0.0, 0.0, 0.006666667, 0.0, 0.01, 0.0033333334, 0.0, 0.0, 0.0, 0.006666667, 0.0033333334, 0.0, 0.0, 0.0033333334, 0.006666667, 0.0, 0.0, 0.0, 0.006666667, 0.0033333334, 0.0033333334, 0.0, 0.0, 0.01, 0.0033333334, 0.0033333334, 0.0, 0.0, 0.006666667, 0.0, 0.0, 0.0, 0.0, 0.0033333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0033333334, 0.0, 0.0, 0.006666667, 0.0, 0.0, 0.0033333334, 0.0, 0.0, 0.0033333334, 0.0, 0.006666667, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0033333334, 0.0033333334, 0.006666667, 0.0033333334, 0.0, 0.006666667, 0.0033333334, 0.0, 0.006666667, 0.0, 0.0, 0.0033333334, 0.0, 0.0, 0.0, 0.0033333334, 0.0, 0.0033333334, 0.006666667, 0.0033333334, 0.0, 0.0033333334, 0.006666667, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0033333334, 0.006666667, 0.0, 0.0, 0.0, 0.0, 0.006666667, 0.0, 0.0033333334, 0.013333334, 0.0, 0.0, 0.0033333334, 0.0033333334, 0.0033333334, 0.006666667, 0.0033333334, 0.0, 0.006666667, 0.0033333334, 0.0, 0.0033333334, 0.0, 0.0, 0.016666668, 0.01, 0.0, 0.0033333334, 0.0, 0.0, 0.0033333334, 0.0, 0.0033333334, 0.0033333334, 0.006666667, 0.0, 0.0033333334, 0.0, 0.0, 0.0033333334, 0.0, 0.0033333334, 0.0, 0.006666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006666667, 0.0, 0.0, 0.0, 0.0, 0.013333334, 0.0, 0.0, 0.0, 0.0033333334, 0.0, 0.0, 0.0, 0.0033333334, 0.0, 0.006666667, 0.0033333334, 0.006666667, 0.0, 0.0, 0.006666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0033333334, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0033333334, 0.0, 0.0, 0.0, 0.0033333334, 0.0, 0.0, 0.0, 0.0, 0.0033333334, 0.0, 0.0033333334, 0.006666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0033333334, 0.006666667, 0.013333334, 0.0033333334, 0.01, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0033333334, 0.0033333334, 0.0, 0.0033333334, 0.0033333334, 0.0033333334, 0.0, 0.0033333334, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0033333334, 0.0033333334, 0.0, 0.0, 0.0, 0.006666667, 0.0, 0.01, 0.0, 0.0, 0.0033333334, 0.0, 0.0033333334, 0.0033333334, 0.0033333334, 0.01, 0.006666667, 0.0033333334, 0.0, 0.0, 0.0033333334, 0.0, 0.0, 0.0, 0.0033333334, 0.02, 0.0, 0.006666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0033333334, 0.0, 0.006666667, 0.0, 0.0033333334, 0.0, 0.0033333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0033333334, 0.0, 0.006666667, 0.0033333334, 0.006666667, 0.0, 0.0033333334, 0.0, 0.0033333334, 0.0, 0.0, 0.0033333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.013333334, 0.0033333334, 0.0, 0.0, 0.0033333334, 0.0, 0.0, 0.0033333334, 0.0, 0.0033333334 ]
--Histogram size : 400
17/02/16 16:53:58 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:58 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:58 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:58 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:58 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:53:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:59 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:59 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:59 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:59 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:59 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:59 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:53:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:53:59 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:59 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:59 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:53:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:53:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:53:59 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:53:59 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:53:59 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
Predicting test image : trees as trees
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/trees/6.jpg
17/02/16 16:53:59 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:53:59 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:59 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:59 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:53:59 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:00 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:00 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:00 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:00 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:00 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:00 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:00 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:00 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:00 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:00 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:00 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:00 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.003690037, 0.007380074, 0.0, 0.0018450185, 0.009225093, 0.0055350554, 0.0, 0.0055350554, 0.007380074, 0.0018450185, 0.0, 0.0, 0.0, 0.003690037, 0.012915129, 0.0018450185, 0.0, 0.0, 0.0018450185, 0.0055350554, 0.003690037, 0.0, 0.003690037, 0.0, 0.0055350554, 0.003690037, 0.003690037, 0.0, 0.0, 0.0055350554, 0.0, 0.009225093, 0.0, 0.007380074, 0.0, 0.0, 0.0, 0.003690037, 0.0018450185, 0.0, 0.0018450185, 0.0055350554, 0.0, 0.0055350554, 0.007380074, 0.0, 0.003690037, 0.0018450185, 0.007380074, 0.0, 0.0, 0.007380074, 0.0, 0.0, 0.009225093, 0.003690037, 0.0055350554, 0.0055350554, 0.0055350554, 0.0018450185, 0.0018450185, 0.014760148, 0.0018450185, 0.0, 0.0018450185, 0.0018450185, 0.003690037, 0.0, 0.0055350554, 0.0018450185, 0.0018450185, 0.0, 0.0, 0.009225093, 0.0, 0.0, 0.0, 0.003690037, 0.0018450185, 0.003690037, 0.0055350554, 0.0055350554, 0.0, 0.0, 0.0018450185, 0.0018450185, 0.0, 0.0018450185, 0.0018450185, 0.0, 0.014760148, 0.009225093, 0.0, 0.0, 0.0018450185, 0.0, 0.0, 0.003690037, 0.012915129, 0.0, 0.0055350554, 0.003690037, 0.0018450185, 0.003690037, 0.0, 0.0, 0.007380074, 0.0018450185, 0.0, 0.007380074, 0.0, 0.0018450185, 0.0, 0.0, 0.0, 0.0018450185, 0.0, 0.0, 0.0018450185, 0.009225093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003690037, 0.0018450185, 0.0055350554, 0.0, 0.003690037, 0.0018450185, 0.0018450185, 0.0, 0.0, 0.007380074, 0.003690037, 0.0, 0.0018450185, 0.0055350554, 0.0055350554, 0.0018450185, 0.0018450185, 0.0, 0.003690037, 0.0055350554, 0.0018450185, 0.0018450185, 0.0055350554, 0.0055350554, 0.0, 0.003690037, 0.0, 0.0, 0.0055350554, 0.003690037, 0.003690037, 0.0, 0.003690037, 0.0, 0.003690037, 0.0018450185, 0.003690037, 0.0, 0.0, 0.0, 0.0018450185, 0.003690037, 0.007380074, 0.003690037, 0.0, 0.0, 0.0055350554, 0.0, 0.003690037, 0.003690037, 0.003690037, 0.0, 0.0, 0.0, 0.0055350554, 0.0, 0.003690037, 0.007380074, 0.003690037, 0.0018450185, 0.0, 0.0, 0.0018450185, 0.0018450185, 0.0055350554, 0.003690037, 0.0, 0.003690037, 0.003690037, 0.0018450185, 0.0, 0.003690037, 0.009225093, 0.0, 0.0018450185, 0.0, 0.0018450185, 0.003690037, 0.0, 0.0, 0.009225093, 0.003690037, 0.0, 0.0, 0.0018450185, 0.0018450185, 0.0018450185, 0.0018450185, 0.003690037, 0.0018450185, 0.003690037, 0.0, 0.0055350554, 0.0, 0.0, 0.0, 0.003690037, 0.003690037, 0.0055350554, 0.0018450185, 0.0, 0.0, 0.0, 0.0018450185, 0.0, 0.003690037, 0.0018450185, 0.0018450185, 0.0, 0.0, 0.0055350554, 0.0, 0.003690037, 0.0018450185, 0.0, 0.0, 0.0, 0.0055350554, 0.0, 0.0018450185, 0.0, 0.0, 0.0, 0.0, 0.0018450185, 0.0, 0.0, 0.0018450185, 0.0018450185, 0.0, 0.0, 0.0018450185, 0.003690037, 0.0, 0.0055350554, 0.0, 0.0, 0.0, 0.0018450185, 0.0018450185, 0.0, 0.0, 0.003690037, 0.0018450185, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055350554, 0.009225093, 0.0018450185, 0.0, 0.0, 0.011070111, 0.0018450185, 0.0018450185, 0.0055350554, 0.0, 0.003690037, 0.009225093, 0.0018450185, 0.0, 0.0018450185, 0.0, 0.0018450185, 0.0, 0.0018450185, 0.012915129, 0.0018450185, 0.007380074, 0.0, 0.009225093, 0.0, 0.0055350554, 0.0, 0.003690037, 0.003690037, 0.009225093, 0.0, 0.007380074, 0.003690037, 0.0, 0.0, 0.0, 0.0055350554, 0.0018450185, 0.0, 0.0055350554, 0.0, 0.003690037, 0.0018450185, 0.003690037, 0.0018450185, 0.0, 0.0, 0.0, 0.0, 0.0055350554, 0.0, 0.007380074, 0.011070111, 0.0, 0.0, 0.0, 0.0, 0.0055350554, 0.0055350554, 0.0018450185, 0.007380074, 0.0018450185, 0.0, 0.0, 0.0018450185, 0.0018450185, 0.003690037, 0.0055350554, 0.0, 0.012915129, 0.0018450185, 0.0, 0.0, 0.011070111, 0.0, 0.0, 0.0018450185, 0.0018450185, 0.0, 0.0018450185, 0.0018450185, 0.011070111, 0.0, 0.003690037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003690037, 0.0, 0.007380074, 0.0, 0.0018450185, 0.0018450185, 0.003690037, 0.003690037, 0.003690037, 0.0018450185, 0.0055350554, 0.0, 0.0055350554, 0.0, 0.003690037, 0.0018450185, 0.007380074, 0.003690037, 0.0018450185, 0.0, 0.0, 0.0018450185, 0.007380074, 0.003690037, 0.009225093, 0.0, 0.0055350554, 0.0, 0.0, 0.0, 0.003690037, 0.0, 0.0055350554, 0.0, 0.0018450185, 0.0018450185 ]
--Histogram size : 400
17/02/16 16:54:00 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:54:00 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:00 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:00 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:00 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:54:01 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:54:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:54:01 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:01 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:01 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:01 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:54:01 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:54:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:54:01 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:54:01 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:01 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 33
Predicting test image : trees as trees
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/trees/8.jpg
17/02/16 16:54:01 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:54:01 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:01 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:01 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:01 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:01 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:01 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:01 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:01 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:01 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:01 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:01 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:01 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:01 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0070422534, 0.0035211267, 0.0035211267, 0.0035211267, 0.0035211267, 0.014084507, 0.0, 0.0070422534, 0.014084507, 0.0, 0.0035211267, 0.0035211267, 0.0035211267, 0.0, 0.0, 0.014084507, 0.0, 0.0035211267, 0.0, 0.0035211267, 0.0070422534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0070422534, 0.0, 0.0, 0.0070422534, 0.0, 0.0, 0.0070422534, 0.0070422534, 0.0035211267, 0.0, 0.0035211267, 0.0, 0.0, 0.0035211267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01056338, 0.0, 0.0, 0.0035211267, 0.0, 0.0, 0.017605633, 0.0, 0.0035211267, 0.0035211267, 0.0035211267, 0.0070422534, 0.0, 0.017605633, 0.0035211267, 0.0035211267, 0.0, 0.0070422534, 0.0, 0.0, 0.0, 0.0, 0.0035211267, 0.0, 0.0, 0.0035211267, 0.0, 0.0, 0.0, 0.0, 0.014084507, 0.0, 0.014084507, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0035211267, 0.0035211267, 0.0, 0.0035211267, 0.0, 0.0, 0.0, 0.01056338, 0.0, 0.01056338, 0.0070422534, 0.0035211267, 0.0, 0.0, 0.0070422534, 0.0, 0.0, 0.0070422534, 0.0, 0.0, 0.0035211267, 0.014084507, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0035211267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0035211267, 0.0, 0.0035211267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0035211267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0035211267, 0.0, 0.0070422534, 0.0070422534, 0.0035211267, 0.0, 0.0070422534, 0.0035211267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0035211267, 0.0, 0.0035211267, 0.0035211267, 0.0, 0.0, 0.0035211267, 0.0, 0.0035211267, 0.02112676, 0.0, 0.0035211267, 0.0, 0.0035211267, 0.0, 0.0, 0.0035211267, 0.0, 0.0, 0.0035211267, 0.0, 0.0, 0.0, 0.0035211267, 0.0070422534, 0.0070422534, 0.0070422534, 0.0, 0.0070422534, 0.0, 0.0, 0.0, 0.0, 0.0035211267, 0.0035211267, 0.0070422534, 0.0070422534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0070422534, 0.0, 0.0035211267, 0.0035211267, 0.0, 0.0, 0.0, 0.0, 0.014084507, 0.0035211267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0035211267, 0.0035211267, 0.0070422534, 0.0035211267, 0.0, 0.0, 0.0, 0.01056338, 0.0, 0.0035211267, 0.0, 0.0, 0.0035211267, 0.0035211267, 0.0035211267, 0.0, 0.0, 0.0, 0.0035211267, 0.01056338, 0.0, 0.0035211267, 0.0035211267, 0.0, 0.0, 0.0, 0.0, 0.0035211267, 0.0, 0.0035211267, 0.0, 0.0070422534, 0.0, 0.0, 0.0, 0.0035211267, 0.0, 0.0, 0.0, 0.0035211267, 0.0035211267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0035211267, 0.01056338, 0.01056338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0035211267, 0.0, 0.0, 0.0070422534, 0.0035211267, 0.0, 0.0, 0.0070422534, 0.0070422534, 0.0035211267, 0.0, 0.0035211267, 0.0035211267, 0.0, 0.0035211267, 0.0, 0.0, 0.0070422534, 0.0, 0.0035211267, 0.0035211267, 0.0035211267, 0.0070422534, 0.014084507, 0.014084507, 0.0, 0.0035211267, 0.0, 0.0035211267, 0.0035211267, 0.0035211267, 0.0070422534, 0.01056338, 0.0035211267, 0.0, 0.0035211267, 0.0, 0.0, 0.0070422534, 0.0070422534, 0.0, 0.0035211267, 0.0, 0.0, 0.0, 0.0, 0.0035211267, 0.01056338, 0.0035211267, 0.0035211267, 0.0, 0.0, 0.0, 0.0, 0.0035211267, 0.0, 0.0035211267, 0.0035211267, 0.0035211267, 0.01056338, 0.0, 0.0, 0.0070422534, 0.0070422534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0035211267, 0.0, 0.0035211267, 0.028169014, 0.01056338, 0.01056338, 0.0, 0.0, 0.0035211267, 0.0, 0.0035211267, 0.0035211267, 0.0, 0.0035211267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0070422534, 0.0, 0.0, 0.0, 0.0, 0.0035211267, 0.0, 0.0, 0.0, 0.0035211267, 0.0, 0.0, 0.01056338, 0.0070422534, 0.0035211267, 0.0070422534, 0.0, 0.0070422534, 0.0, 0.0, 0.0, 0.0, 0.0035211267, 0.0, 0.0, 0.0070422534, 0.0070422534, 0.0070422534, 0.0035211267, 0.0, 0.01056338, 0.0035211267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0070422534 ]
--Histogram size : 400
17/02/16 16:54:02 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:54:02 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:02 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:02 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:02 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:54:02 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:02 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:02 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:54:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:54:02 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:02 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:02 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:54:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:54:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:54:02 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:02 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:02 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:02 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:02 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:54:02 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
Predicting test image : trees as trees
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/trees/9.jpg
17/02/16 16:54:03 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:54:03 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:03 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:03 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:03 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:03 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:03 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:03 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:03 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:03 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:03 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:03 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:03 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:03 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:03 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:03 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:03 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0023419203, 0.0, 0.009367681, 0.0023419203, 0.0046838406, 0.0, 0.0070257606, 0.009367681, 0.0023419203, 0.0023419203, 0.0046838406, 0.0046838406, 0.0, 0.0023419203, 0.0023419203, 0.0046838406, 0.0, 0.0, 0.0023419203, 0.0046838406, 0.0023419203, 0.0, 0.0, 0.0070257606, 0.0023419203, 0.0, 0.0046838406, 0.0, 0.0, 0.0, 0.0046838406, 0.0, 0.0, 0.0, 0.0023419203, 0.0046838406, 0.0, 0.0046838406, 0.0, 0.0, 0.0, 0.0, 0.0046838406, 0.0023419203, 0.0, 0.0046838406, 0.0, 0.0, 0.0070257606, 0.009367681, 0.0, 0.0023419203, 0.009367681, 0.0023419203, 0.0, 0.009367681, 0.0023419203, 0.0070257606, 0.0023419203, 0.0, 0.0070257606, 0.0, 0.016393442, 0.0023419203, 0.0, 0.0, 0.0023419203, 0.0046838406, 0.0023419203, 0.0023419203, 0.0070257606, 0.0, 0.0023419203, 0.0, 0.0, 0.0, 0.0, 0.0023419203, 0.0070257606, 0.0023419203, 0.0, 0.009367681, 0.0, 0.0070257606, 0.0023419203, 0.0, 0.0046838406, 0.0, 0.0, 0.0, 0.0, 0.0023419203, 0.0046838406, 0.009367681, 0.0, 0.0070257606, 0.0023419203, 0.0023419203, 0.0, 0.0, 0.0, 0.0023419203, 0.0023419203, 0.0023419203, 0.0, 0.0, 0.0023419203, 0.0, 0.0, 0.0, 0.0023419203, 0.0, 0.0, 0.0046838406, 0.0, 0.0023419203, 0.0, 0.009367681, 0.0046838406, 0.0, 0.0023419203, 0.0, 0.0023419203, 0.0, 0.0023419203, 0.0, 0.0, 0.0046838406, 0.0, 0.0, 0.0046838406, 0.0023419203, 0.0023419203, 0.0, 0.0, 0.0023419203, 0.0, 0.0, 0.0, 0.0, 0.0023419203, 0.0, 0.0, 0.0, 0.0023419203, 0.0, 0.0, 0.0023419203, 0.0, 0.0023419203, 0.0, 0.0, 0.011709602, 0.0070257606, 0.0, 0.0, 0.0046838406, 0.0, 0.0023419203, 0.0, 0.0, 0.0046838406, 0.0, 0.0, 0.011709602, 0.0023419203, 0.0, 0.0, 0.011709602, 0.0, 0.009367681, 0.0046838406, 0.0023419203, 0.0023419203, 0.0, 0.0023419203, 0.0046838406, 0.0023419203, 0.0046838406, 0.0046838406, 0.0023419203, 0.0, 0.0, 0.0046838406, 0.011709602, 0.0, 0.0, 0.0, 0.0070257606, 0.0023419203, 0.0, 0.009367681, 0.0, 0.0046838406, 0.0023419203, 0.0023419203, 0.0, 0.0046838406, 0.0046838406, 0.0, 0.0, 0.0046838406, 0.0, 0.011709602, 0.0, 0.0023419203, 0.009367681, 0.0, 0.0023419203, 0.0046838406, 0.0, 0.0, 0.0070257606, 0.0, 0.0023419203, 0.0046838406, 0.0046838406, 0.0, 0.0070257606, 0.0, 0.0023419203, 0.0, 0.0023419203, 0.0023419203, 0.0, 0.0, 0.0, 0.0046838406, 0.0, 0.0046838406, 0.0, 0.0, 0.0, 0.0023419203, 0.0023419203, 0.0, 0.0, 0.0046838406, 0.009367681, 0.0, 0.0, 0.0023419203, 0.0, 0.0, 0.0, 0.011709602, 0.0023419203, 0.011709602, 0.0023419203, 0.0046838406, 0.0, 0.0, 0.0, 0.0, 0.0046838406, 0.009367681, 0.0023419203, 0.009367681, 0.0046838406, 0.0046838406, 0.0046838406, 0.0, 0.0, 0.0, 0.0023419203, 0.0070257606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0023419203, 0.0, 0.0046838406, 0.0023419203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0046838406, 0.0046838406, 0.0023419203, 0.0, 0.0, 0.0023419203, 0.0046838406, 0.0, 0.0, 0.0, 0.011709602, 0.0023419203, 0.0, 0.009367681, 0.0, 0.016393442, 0.0, 0.009367681, 0.0, 0.0070257606, 0.0023419203, 0.0, 0.0023419203, 0.0070257606, 0.0023419203, 0.0, 0.0070257606, 0.0, 0.0, 0.011709602, 0.0, 0.0, 0.0, 0.0023419203, 0.0, 0.0, 0.0023419203, 0.0023419203, 0.0, 0.0023419203, 0.0, 0.0, 0.0, 0.0023419203, 0.0, 0.0023419203, 0.0, 0.0, 0.0023419203, 0.0, 0.0, 0.0, 0.0046838406, 0.011709602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0046838406, 0.0, 0.0, 0.0, 0.011709602, 0.0023419203, 0.0023419203, 0.0, 0.018735362, 0.009367681, 0.0, 0.0023419203, 0.0070257606, 0.0070257606, 0.0046838406, 0.0023419203, 0.0023419203, 0.0, 0.0, 0.0046838406, 0.0, 0.0, 0.0, 0.0023419203, 0.0, 0.0070257606, 0.0070257606, 0.0023419203, 0.0046838406, 0.0, 0.0023419203, 0.0, 0.011709602, 0.0023419203, 0.0046838406, 0.0, 0.009367681, 0.0046838406, 0.0046838406, 0.0, 0.0023419203, 0.0046838406, 0.0023419203, 0.0023419203, 0.0, 0.0, 0.0, 0.0, 0.0023419203, 0.009367681, 0.009367681, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0070257606 ]
--Histogram size : 400
17/02/16 16:54:04 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:54:04 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:04 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:04 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:04 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:54:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:54:04 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:04 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:04 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:04 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:04 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:54:04 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:54:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:54:04 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:04 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:04 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:54:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:54:04 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:04 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:04 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
Predicting test image : trees as bag
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/watch/1.jpg
17/02/16 16:54:05 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:54:05 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:05 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:05 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:05 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:05 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:05 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:05 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:05 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:05 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:05 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:05 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:05 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:05 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:05 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:05 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:05 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0048076925, 0.0048076925, 0.0, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0048076925, 0.0048076925, 0.0, 0.014423078, 0.009615385, 0.0048076925, 0.0, 0.0, 0.009615385, 0.009615385, 0.0, 0.0, 0.0048076925, 0.0048076925, 0.0, 0.014423078, 0.0048076925, 0.0, 0.0048076925, 0.0, 0.0, 0.01923077, 0.0, 0.0, 0.0, 0.009615385, 0.0, 0.0, 0.0, 0.0048076925, 0.0, 0.0048076925, 0.0, 0.0, 0.0048076925, 0.0048076925, 0.0, 0.0, 0.0, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01923077, 0.0, 0.0048076925, 0.0048076925, 0.0, 0.0, 0.0048076925, 0.0, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0048076925, 0.0048076925, 0.0, 0.014423078, 0.0, 0.0, 0.0048076925, 0.028846156, 0.0048076925, 0.0, 0.0, 0.0, 0.0048076925, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.0, 0.0048076925, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.0, 0.0048076925, 0.0, 0.009615385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0048076925, 0.0048076925, 0.0, 0.014423078, 0.0, 0.0, 0.009615385, 0.0, 0.0, 0.014423078, 0.0048076925, 0.0048076925, 0.0, 0.009615385, 0.0, 0.0, 0.0, 0.0048076925, 0.009615385, 0.0, 0.0048076925, 0.0, 0.009615385, 0.0048076925, 0.0, 0.0, 0.0, 0.009615385, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.009615385, 0.009615385, 0.0, 0.0, 0.0, 0.0, 0.0048076925, 0.0, 0.0048076925, 0.0, 0.0, 0.009615385, 0.0048076925, 0.0048076925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009615385, 0.0, 0.01923077, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.0, 0.0048076925, 0.0048076925, 0.0, 0.0048076925, 0.0, 0.0, 0.01923077, 0.0, 0.0, 0.009615385, 0.0048076925, 0.009615385, 0.0, 0.0048076925, 0.0048076925, 0.0, 0.0048076925, 0.0, 0.0, 0.0048076925, 0.0048076925, 0.009615385, 0.0, 0.0048076925, 0.0048076925, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014423078, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0048076925, 0.0, 0.0048076925, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.0048076925, 0.0048076925, 0.0048076925, 0.0048076925, 0.01923077, 0.0, 0.0, 0.0, 0.009615385, 0.009615385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0048076925, 0.009615385, 0.0, 0.0048076925, 0.0, 0.0048076925, 0.009615385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0048076925, 0.0048076925, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.0, 0.014423078, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.009615385, 0.0, 0.0, 0.014423078, 0.0048076925, 0.0, 0.0, 0.0048076925, 0.0048076925, 0.0048076925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01923077, 0.0, 0.0048076925, 0.0, 0.0, 0.0048076925, 0.0, 0.0, 0.009615385, 0.0, 0.009615385, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.0, 0.024038462, 0.01923077, 0.0, 0.0048076925, 0.01923077, 0.0, 0.01923077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0048076925, 0.0, 0.0, 0.0, 0.01923077, 0.0, 0.0, 0.0048076925, 0.014423078, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:54:06 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:54:06 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:06 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:06 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:06 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:54:06 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:06 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:06 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:54:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:54:06 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:06 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:06 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:54:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:54:06 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:06 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:06 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:54:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:54:06 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:06 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:06 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : watch as trees
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/watch/2.jpg
17/02/16 16:54:06 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:54:06 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:06 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:06 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:06 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:07 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:07 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:07 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:07 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:07 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:07 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 100
17/02/16 16:54:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:07 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:07 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:07 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:07 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:07 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:07 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.009090909, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.027272727, 0.0, 0.0, 0.0, 0.018181818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027272727, 0.009090909, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027272727, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018181818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.090909086, 0.0, 0.018181818, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036363635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.036363635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018181818, 0.018181818, 0.0, 0.009090909, 0.009090909, 0.0, 0.018181818, 0.0, 0.0, 0.0, 0.009090909, 0.009090909, 0.009090909, 0.0, 0.0, 0.018181818, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.018181818, 0.0, 0.06363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018181818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018181818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.036363635, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036363635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027272727, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.018181818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018181818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.0, 0.0, 0.0, 0.0, 0.009090909, 0.009090909, 0.0, 0.0, 0.0, 0.0 ]
--Histogram size : 400
17/02/16 16:54:07 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:54:07 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:07 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:07 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:07 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:54:08 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:08 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:08 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:54:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:54:08 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:08 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:08 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:54:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:54:08 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:08 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:08 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:54:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:54:08 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:08 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:08 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : watch as chocolate
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/watch/6.jpg
17/02/16 16:54:08 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:54:08 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:08 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:08 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:08 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:09 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:09 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 100
17/02/16 16:54:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:09 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:09 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:09 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 100
17/02/16 16:54:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:09 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:09 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:09 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:09 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:09 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:09 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0014388489, 0.0, 0.0014388489, 0.0014388489, 0.0057553956, 0.0, 0.0014388489, 0.0057553956, 0.004316547, 0.0014388489, 0.0014388489, 0.0, 0.004316547, 0.0028776978, 0.0014388489, 0.011510791, 0.0, 0.0028776978, 0.0014388489, 0.0, 0.004316547, 0.0014388489, 0.0028776978, 0.004316547, 0.0, 0.0028776978, 0.0028776978, 0.0028776978, 0.0028776978, 0.0028776978, 0.0028776978, 0.0014388489, 0.0, 0.0, 0.0014388489, 0.004316547, 0.0014388489, 0.0014388489, 0.0014388489, 0.0, 0.0028776978, 0.0028776978, 0.0014388489, 0.0057553956, 0.0071942443, 0.0014388489, 0.0028776978, 0.004316547, 0.0014388489, 0.0028776978, 0.0014388489, 0.0028776978, 0.0014388489, 0.0014388489, 0.014388489, 0.0014388489, 0.0, 0.0028776978, 0.0, 0.0, 0.0014388489, 0.0028776978, 0.0014388489, 0.0028776978, 0.008633094, 0.0014388489, 0.0057553956, 0.0014388489, 0.0071942443, 0.0, 0.0, 0.004316547, 0.0014388489, 0.0, 0.0071942443, 0.0, 0.0028776978, 0.0, 0.0071942443, 0.0, 0.004316547, 0.0, 0.0028776978, 0.0014388489, 0.0028776978, 0.0014388489, 0.004316547, 0.0014388489, 0.004316547, 0.0, 0.0014388489, 0.0, 0.0, 0.0014388489, 0.0014388489, 0.0, 0.0, 0.0028776978, 0.0, 0.0, 0.008633094, 0.0, 0.0014388489, 0.0028776978, 0.0, 0.0028776978, 0.0014388489, 0.0, 0.0, 0.0, 0.008633094, 0.0028776978, 0.0028776978, 0.0014388489, 0.0071942443, 0.0028776978, 0.0, 0.0014388489, 0.0, 0.0, 0.0014388489, 0.0057553956, 0.0, 0.0028776978, 0.0028776978, 0.0014388489, 0.0, 0.0071942443, 0.0, 0.0014388489, 0.0014388489, 0.0, 0.0014388489, 0.0, 0.0014388489, 0.0028776978, 0.0, 0.0, 0.0028776978, 0.0014388489, 0.0028776978, 0.0028776978, 0.0014388489, 0.0, 0.0028776978, 0.0014388489, 0.0057553956, 0.0028776978, 0.0, 0.0, 0.0, 0.0, 0.008633094, 0.0, 0.0014388489, 0.0, 0.0, 0.0, 0.0014388489, 0.0028776978, 0.0028776978, 0.004316547, 0.0057553956, 0.004316547, 0.0014388489, 0.0, 0.0, 0.0028776978, 0.0014388489, 0.004316547, 0.0028776978, 0.0028776978, 0.0, 0.0028776978, 0.0028776978, 0.0014388489, 0.0, 0.004316547, 0.0028776978, 0.0, 0.0014388489, 0.0057553956, 0.0071942443, 0.0028776978, 0.0014388489, 0.0, 0.0, 0.020143885, 0.004316547, 0.0028776978, 0.0028776978, 0.0028776978, 0.0014388489, 0.0, 0.0014388489, 0.0, 0.0028776978, 0.0, 0.0, 0.004316547, 0.0071942443, 0.0057553956, 0.0028776978, 0.0028776978, 0.0014388489, 0.0028776978, 0.0057553956, 0.0, 0.0, 0.0014388489, 0.0057553956, 0.0014388489, 0.0028776978, 0.0028776978, 0.0014388489, 0.0028776978, 0.0028776978, 0.0014388489, 0.0, 0.0, 0.0028776978, 0.0, 0.0028776978, 0.0014388489, 0.0028776978, 0.0014388489, 0.0014388489, 0.0071942443, 0.0028776978, 0.0057553956, 0.0071942443, 0.0, 0.0028776978, 0.0028776978, 0.0028776978, 0.0028776978, 0.0, 0.0014388489, 0.0014388489, 0.0014388489, 0.0, 0.008633094, 0.0057553956, 0.0, 0.004316547, 0.004316547, 0.0, 0.0014388489, 0.0014388489, 0.008633094, 0.0014388489, 0.0057553956, 0.0057553956, 0.008633094, 0.0, 0.004316547, 0.0071942443, 0.0014388489, 0.0, 0.0014388489, 0.0028776978, 0.0, 0.0028776978, 0.0, 0.0028776978, 0.0014388489, 0.0014388489, 0.0, 0.0, 0.0, 0.0028776978, 0.004316547, 0.0, 0.0014388489, 0.0014388489, 0.0, 0.0, 0.0, 0.0014388489, 0.004316547, 0.0014388489, 0.0014388489, 0.0057553956, 0.0, 0.0071942443, 0.0, 0.0028776978, 0.0071942443, 0.0, 0.0057553956, 0.0, 0.0014388489, 0.0057553956, 0.0014388489, 0.0071942443, 0.0071942443, 0.0, 0.004316547, 0.0028776978, 0.0, 0.0014388489, 0.0, 0.0028776978, 0.0014388489, 0.0014388489, 0.0, 0.0014388489, 0.0028776978, 0.0, 0.0057553956, 0.004316547, 0.008633094, 0.0, 0.0, 0.0, 0.004316547, 0.0014388489, 0.0, 0.0014388489, 0.0014388489, 0.0, 0.0071942443, 0.0, 0.0028776978, 0.0, 0.0, 0.004316547, 0.0028776978, 0.0014388489, 0.004316547, 0.0014388489, 0.0, 0.0014388489, 0.0, 0.0057553956, 0.0, 0.0028776978, 0.0, 0.0071942443, 0.0028776978, 0.0028776978, 0.0, 0.0014388489, 0.015827337, 0.0028776978, 0.011510791, 0.0, 0.0, 0.0014388489, 0.0014388489, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0014388489, 0.0, 0.0057553956, 0.004316547, 0.0028776978, 0.008633094, 0.010071943, 0.0, 0.0028776978, 0.0, 0.0028776978, 0.0, 0.004316547, 0.0028776978, 0.004316547, 0.0014388489, 0.017266188, 0.0014388489, 0.0, 0.0014388489, 0.010071943, 0.0028776978, 0.0, 0.0057553956, 0.0028776978, 0.0, 0.0014388489, 0.004316547, 0.0028776978, 0.0, 0.0057553956, 0.0014388489, 0.004316547, 0.0, 0.004316547, 0.0014388489, 0.0028776978, 0.011510791, 0.01294964, 0.0, 0.0, 0.0014388489, 0.0028776978, 0.0057553956 ]
--Histogram size : 400
17/02/16 16:54:09 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:54:09 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:09 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:09 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:09 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:54:10 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:10 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:10 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 19
17/02/16 16:54:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:54:10 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:54:10 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:10 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:10 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:54:10 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:10 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:54:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:54:10 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:10 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:10 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 19
Predicting test image : watch as trees
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/watch/7.jpg
17/02/16 16:54:10 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:54:10 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:10 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:10 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:10 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:11 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:11 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:11 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 100
17/02/16 16:54:11 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:11 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:11 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:11 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 100
17/02/16 16:54:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:11 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:11 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:11 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0034482758, 0.0, 0.0068965517, 0.0068965517, 0.0034482758, 0.0034482758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0034482758, 0.0068965517, 0.0068965517, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020689655, 0.0034482758, 0.0, 0.0034482758, 0.0068965517, 0.0, 0.0, 0.0, 0.010344828, 0.0068965517, 0.0034482758, 0.013793103, 0.0034482758, 0.0, 0.0034482758, 0.0, 0.0, 0.013793103, 0.0034482758, 0.0, 0.020689655, 0.0, 0.0, 0.0034482758, 0.0068965517, 0.0, 0.0, 0.0034482758, 0.0034482758, 0.0034482758, 0.0, 0.0034482758, 0.0, 0.0, 0.0068965517, 0.0, 0.0034482758, 0.0034482758, 0.0, 0.0, 0.0034482758, 0.0034482758, 0.0034482758, 0.0034482758, 0.0, 0.0034482758, 0.0034482758, 0.0, 0.0034482758, 0.0068965517, 0.0034482758, 0.0034482758, 0.0, 0.0068965517, 0.0, 0.010344828, 0.0, 0.0034482758, 0.0, 0.010344828, 0.0, 0.0034482758, 0.0, 0.0, 0.0034482758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0034482758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0034482758, 0.0, 0.0, 0.0, 0.0034482758, 0.0, 0.0, 0.0034482758, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0034482758, 0.0034482758, 0.0, 0.0, 0.0, 0.0034482758, 0.0, 0.0068965517, 0.0, 0.0034482758, 0.0, 0.0, 0.0, 0.0, 0.0034482758, 0.0, 0.0, 0.0, 0.0034482758, 0.0, 0.0, 0.0034482758, 0.0, 0.0034482758, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0068965517, 0.0034482758, 0.0, 0.0, 0.0068965517, 0.0034482758, 0.0, 0.0034482758, 0.0068965517, 0.0, 0.0, 0.0, 0.0034482758, 0.0034482758, 0.0068965517, 0.0, 0.0, 0.0068965517, 0.0, 0.0034482758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0034482758, 0.0, 0.0, 0.0, 0.0034482758, 0.0, 0.0, 0.013793103, 0.0034482758, 0.0034482758, 0.0034482758, 0.0034482758, 0.0, 0.0, 0.0, 0.0034482758, 0.0034482758, 0.0, 0.0, 0.0, 0.0068965517, 0.010344828, 0.0, 0.0034482758, 0.0068965517, 0.0034482758, 0.0034482758, 0.010344828, 0.0068965517, 0.0034482758, 0.0034482758, 0.0068965517, 0.0, 0.013793103, 0.0, 0.0034482758, 0.0, 0.0034482758, 0.0, 0.0, 0.0, 0.0, 0.0034482758, 0.0034482758, 0.0068965517, 0.0, 0.0, 0.0034482758, 0.0, 0.0034482758, 0.0034482758, 0.0, 0.0068965517, 0.020689655, 0.0034482758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0034482758, 0.0, 0.0034482758, 0.0034482758, 0.0034482758, 0.0034482758, 0.0, 0.0068965517, 0.0, 0.0, 0.0068965517, 0.0, 0.010344828, 0.0068965517, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0034482758, 0.0034482758, 0.0, 0.0, 0.0068965517, 0.0034482758, 0.0, 0.0, 0.0, 0.0, 0.0034482758, 0.0, 0.0034482758, 0.0, 0.0, 0.0, 0.0, 0.0034482758, 0.0034482758, 0.0, 0.0034482758, 0.0, 0.010344828, 0.010344828, 0.0, 0.0068965517, 0.0, 0.0, 0.0034482758, 0.0, 0.0, 0.0, 0.0034482758, 0.0, 0.0068965517, 0.0068965517, 0.0068965517, 0.0, 0.0, 0.0068965517, 0.0034482758, 0.0, 0.0034482758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0068965517, 0.0034482758, 0.010344828, 0.0, 0.0034482758, 0.0, 0.0, 0.0, 0.013793103, 0.0, 0.0, 0.0034482758, 0.0, 0.0034482758, 0.0, 0.0068965517, 0.0068965517, 0.0034482758, 0.0, 0.0, 0.0, 0.0034482758, 0.0034482758, 0.0034482758, 0.0, 0.0, 0.0, 0.0034482758, 0.0, 0.0068965517, 0.0, 0.0, 0.0068965517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010344828, 0.0, 0.0034482758, 0.0, 0.0034482758, 0.0, 0.0034482758, 0.0, 0.01724138, 0.0, 0.013793103, 0.0034482758, 0.0034482758, 0.0068965517, 0.0, 0.0068965517, 0.0, 0.0, 0.010344828, 0.0, 0.010344828, 0.0, 0.0, 0.0, 0.0068965517, 0.0034482758, 0.0, 0.0, 0.0034482758, 0.0, 0.0034482758, 0.0034482758, 0.0, 0.0034482758, 0.0, 0.0, 0.010344828, 0.0068965517, 0.0, 0.0, 0.0, 0.010344828, 0.0034482758, 0.0, 0.0034482758, 0.0, 0.0034482758, 0.0 ]
--Histogram size : 400
17/02/16 16:54:11 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:54:11 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:11 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:11 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:11 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:54:12 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:54:12 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:12 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:54:12 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:54:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:54:12 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:12 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:12 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 19
17/02/16 16:54:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:54:12 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:12 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
Predicting test image : watch as mobile
file:/C:/Users/saijy/Desktop/BigData/Sample/image_classification_Windows/data/test2/watch/8.jpg
17/02/16 16:54:12 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:54:12 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:12 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:12 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:12 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:13 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:13 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:13 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:13 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:13 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:13 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:13 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:13 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:13 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
17/02/16 16:54:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
17/02/16 16:54:13 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:13 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:13 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 100
400 5
Histogram size : (400, 1)
Histogram : [ 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.02793296, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.016759776, 0.005586592, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.005586592, 0.005586592, 0.0, 0.005586592, 0.011173184, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.005586592, 0.0, 0.005586592, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.022346368, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.005586592, 0.0, 0.022346368, 0.0, 0.011173184, 0.005586592, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016759776, 0.0, 0.005586592, 0.016759776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.011173184, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022346368, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.016759776, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.011173184, 0.0, 0.005586592, 0.0, 0.005586592, 0.005586592, 0.02793296, 0.005586592, 0.0, 0.0, 0.005586592, 0.005586592, 0.005586592, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.011173184, 0.0, 0.005586592, 0.0, 0.005586592, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.011173184, 0.044692736, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.011173184, 0.0, 0.0, 0.005586592, 0.016759776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.005586592, 0.005586592, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.011173184, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.005586592, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02793296, 0.0, 0.005586592, 0.0, 0.022346368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592, 0.005586592, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.005586592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022346368, 0.0, 0.011173184, 0.0, 0.016759776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011173184, 0.0, 0.0, 0.0, 0.0, 0.016759776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005586592 ]
--Histogram size : 400
17/02/16 16:54:13 INFO FileInputFormat: Total input paths to process : 1
17/02/16 16:54:13 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:13 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:13 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:13 INFO ParquetFileReader: Initiating action with parallelism: 5
17/02/16 16:54:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:54:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 33 records.
17/02/16 16:54:14 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:14 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:14 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:14 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:14 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 33
17/02/16 16:54:14 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
17/02/16 16:54:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
17/02/16 16:54:14 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:14 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:14 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
17/02/16 16:54:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
17/02/16 16:54:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 19 records.
17/02/16 16:54:14 INFO InternalParquetRecordReader: at row 0. reading next block
17/02/16 16:54:14 INFO CodecPool: Got brand-new decompressor [.gz]
17/02/16 16:54:14 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 19
Predicting test image : watch as laptop
(5.0,9)
(7.0,9)
(8.0,9)
(4.0,9)
(8.0,9)
(1.0,8)
(8.0,8)
(8.0,8)
(8.0,8)
(2.0,8)
(4.0,7)
(9.0,7)
(1.0,7)
(4.0,7)
(4.0,7)
(5.0,6)
(4.0,6)
(5.0,6)
(1.0,6)
(0.0,6)
(9.0,5)
(7.0,5)
(1.0,5)
(1.0,5)
(8.0,5)
(8.0,4)
(8.0,4)
(3.0,4)
(8.0,4)
(2.0,4)
(4.0,3)
(0.0,3)
(4.0,3)
(4.0,3)
(8.0,3)
(0.0,2)
(8.0,2)
(5.0,2)
(4.0,2)
(0.0,2)
(1.0,1)
(4.0,1)
(5.0,1)
(5.0,1)
(1.0,1)
(4.0,0)
(5.0,0)
(1.0,0)
(7.0,0)
(5.0,0)
0.1
 |=================== Confusion matrix ==========================
0.0  1.0  0.0  0.0  1.0  2.0  0.0  1.0  0.0  0.0  
0.0  2.0  0.0  0.0  1.0  2.0  0.0  0.0  0.0  0.0  
2.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  0.0  
1.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  1.0  0.0  
0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  3.0  0.0  
0.0  2.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  
1.0  1.0  0.0  0.0  1.0  2.0  0.0  0.0  0.0  0.0  
0.0  1.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  1.0  
0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  3.0  0.0  
0.0  0.0  0.0  0.0  1.0  1.0  0.0  1.0  2.0  0.0  
0.1
17/02/16 16:54:15 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/02/16 16:54:15 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/02/16 16:54:15 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.

Process finished with exit code 0
